<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Linear Models</title>
  <meta name="description" content="Course notes of LIMO" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Linear Models" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course notes of LIMO" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Linear Models" />
  
  <meta name="twitter:description" content="Course notes of LIMO" />
  

<meta name="author" content="Olivier Thas" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path=""><a href="#Ch_Introduction"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path=""><a href="#how-to-work-through-the-course-notes"><i class="fa fa-check"></i><b>1.1</b> How to work through the course notes</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#exercise"><i class="fa fa-check"></i>Exercise</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path=""><a href="#two-versions-of-this-course-oc-and-dl"><i class="fa fa-check"></i><b>1.2</b> Two versions of this course: OC and DL</a></li>
<li class="chapter" data-level="1.3" data-path=""><a href="#communication"><i class="fa fa-check"></i><b>1.3</b> Communication</a></li>
<li class="chapter" data-level="1.4" data-path=""><a href="#software"><i class="fa fa-check"></i><b>1.4</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path=""><a href="#Ch_SimpleLinearRegression"><i class="fa fa-check"></i><b>2</b> Simple Linear Regression Analysis</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height-data"><i class="fa fa-check"></i>Example (Galton's height data)</a></li>
<li class="chapter" data-level="2.1" data-path=""><a href="#S:RegSimStudy"><i class="fa fa-check"></i><b>2.1</b> Interpretation via simulations</a></li>
<li class="chapter" data-level="2.2" data-path=""><a href="#S:LSE1"><i class="fa fa-check"></i><b>2.2</b> Least squares estimators</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height-data-1"><i class="fa fa-check"></i>Example (Galton's height data)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-blood-pressure"><i class="fa fa-check"></i>Exercise: blood pressure</a></li>
<li class="chapter" data-level="2.3" data-path=""><a href="#S:PropLSE"><i class="fa fa-check"></i><b>2.3</b> Properties of the Least Squares Estimator</a><ul>
<li class="chapter" data-level="2.3.1" data-path=""><a href="#mean-and-variance-of-the-lse"><i class="fa fa-check"></i><b>2.3.1</b> Mean and variance of the LSE</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-simulation-study"><i class="fa fa-check"></i>Exercise: simulation study</a><ul>
<li class="chapter" data-level="2.3.2" data-path=""><a href="#best-linear-unbiased-estimator-blue"><i class="fa fa-check"></i><b>2.3.2</b> Best Linear Unbiased Estimator (BLUE)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-simulation-study-1"><i class="fa fa-check"></i>Exercise: Simulation study</a><ul>
<li class="chapter" data-level="2.3.3" data-path=""><a href="#sampling-distribution-of-the-lse"><i class="fa fa-check"></i><b>2.3.3</b> Sampling distribution of the LSE</a></li>
<li class="chapter" data-level="2.3.4" data-path=""><a href="#maximum-likelihood-estimator-of-hatmbbeta"><i class="fa fa-check"></i><b>2.3.4</b> Maximum likelihood estimator of <span class="math inline">\(\hat{\mb\beta}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path=""><a href="#an-estimator-of-sigma2"><i class="fa fa-check"></i><b>2.4</b> An Estimator of <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-simulation-study-2"><i class="fa fa-check"></i>Exercise: Simulation study</a></li>
<li class="chapter" data-level="2.5" data-path=""><a href="#sampling-distributions-of-the-standardised-and-the-studentised-lse"><i class="fa fa-check"></i><b>2.5</b> Sampling Distributions of the Standardised and the Studentised LSE</a></li>
<li class="chapter" data-level="2.6" data-path=""><a href="#S:BIReg1"><i class="fa fa-check"></i><b>2.6</b> Confidence Intervals</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height-data-2"><i class="fa fa-check"></i>Example (Galton's height data)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-blood-pressure-1"><i class="fa fa-check"></i>Exercise: Blood Pressure</a></li>
<li class="chapter" data-level="2.7" data-path=""><a href="#S_RegTests"><i class="fa fa-check"></i><b>2.7</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height-data-3"><i class="fa fa-check"></i>Example (Galton's height data)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-muscle-mass"><i class="fa fa-check"></i>Exercise: Muscle mass</a></li>
<li class="chapter" data-level="2.8" data-path=""><a href="#S_AssessAssumptions"><i class="fa fa-check"></i><b>2.8</b> Assessment of the Model Assumptions</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height-data-4"><i class="fa fa-check"></i>Example (Galton's height data)</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#normality-of-the-error-term"><i class="fa fa-check"></i>Normality of the error term</a></li>
<li class="chapter" data-level="" data-path=""><a href="#homoskedasticity"><i class="fa fa-check"></i>Homoskedasticity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-muscle-mass-1"><i class="fa fa-check"></i>Exercise: Muscle mass</a></li>
<li class="chapter" data-level="2.9" data-path=""><a href="#binary-dummy-regressors"><i class="fa fa-check"></i><b>2.9</b> Binary Dummy Regressors</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example"><i class="fa fa-check"></i>Example</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-smoking"><i class="fa fa-check"></i>Exercise: Smoking</a></li>
<li class="chapter" data-level="2.10" data-path=""><a href="#association-versus-causation"><i class="fa fa-check"></i><b>2.10</b> Association versus Causation</a><ul>
<li class="chapter" data-level="2.10.1" data-path=""><a href="#introduction"><i class="fa fa-check"></i><b>2.10.1</b> Introduction</a></li>
<li class="chapter" data-level="2.10.2" data-path=""><a href="#causal-inference-and-counterfactuals"><i class="fa fa-check"></i><b>2.10.2</b> Causal inference and counterfactuals</a></li>
<li class="chapter" data-level="2.10.3" data-path=""><a href="#randomised-studies"><i class="fa fa-check"></i><b>2.10.3</b> Randomised studies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path=""><a href="#reporting"><i class="fa fa-check"></i><b>3</b> Reporting</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path=""><a href="#app:VecDiff"><i class="fa fa-check"></i><b>A</b> Vector Differentiation</a></li>
<li class="chapter" data-level="B" data-path=""><a href="#app:LinTrans"><i class="fa fa-check"></i><b>B</b> Linear Transformations of MVN</a></li>
<li class="chapter" data-level="C" data-path=""><a href="#app:Slutsky"><i class="fa fa-check"></i><b>C</b> Slutsky's Theorem</a></li>
<li class="chapter" data-level="D" data-path=""><a href="#types-of-statistical-models"><i class="fa fa-check"></i><b>D</b> Types of Statistical Models</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Linear Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Linear Models</h1>
<p class="author"><em>Olivier Thas</em></p>
<p class="date"><em>2021-2021</em></p>
</div>
<!--- For HTML Only --->
<p><span class="math inline">\(\newcommand{\prob}[1]{\text{P}\left\{#1\right\}}\)</span> <span class="math inline">\(\newcommand{\mb}[1]{\boldsymbol{#1}}\)</span> <span class="math inline">\(\newcommand{\E}[1]{\mbox{E}\left\{#1\right\}}\)</span> <span class="math inline">\(\newcommand{\Ef}[2]{\mbox{E}_{#1}\left\{#2\right\}}\)</span> <span class="math inline">\(\newcommand{\cov}[1]{\mbox{Cov}\left\{#1\right\}}\)</span> <span class="math inline">\(\newcommand{\cor}[1]{\mbox{Cor}\left\{#1\right\}}\)</span> <span class="math inline">\(\newcommand{\covf}[2]{\mbox{Cov}_{#1}\left\{#2\right\}}\)</span> <span class="math inline">\(\newcommand{\var}[1]{\mbox{Var}\left\{#1\right\}}\)</span> <span class="math inline">\(\newcommand{\varf}[2]{\mbox{Var}_{#1}\left\{#2\right\}}\)</span> <span class="math inline">\(\newcommand{\ind}[0]{\perp \!\!\! \perp}\)</span> <span class="math inline">\(\newcommand{\eps}[0]{\varepsilon}\)</span> <span class="math inline">\(\newcommand{\SSE}[0]{\text{SSE}}\)</span> <span class="math inline">\(\newcommand{\iid}[0]{\text{ i.i.d. }}\)</span> <span class="math inline">\(\newcommand{\convDistr}[0]{\stackrel{d}{\longrightarrow}}\)</span> <span class="math inline">\(\newcommand{\convProb}[0]{\stackrel{p}{\longrightarrow}}\)</span> <span class="math inline">\(\newcommand{\QED}[0]{\null\nobreak\hfill\ensuremath{\blacksquare}}\)</span> <span class="math inline">\(\newcommand{\MSE}[0]{\text{MSE}}\)</span> <span class="math inline">\(\newcommand{\probf}[2]{\text{P}_{#1}\left\{#2\right\}}\)</span> <span class="math inline">\(\newcommand{\HSim}[0]{\stackrel{H_0}{\sim}}\)</span></p>

<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>

<div id="Ch_Introduction" class="section level1">
<h1><span class="header-section-number">Hoofdstuk 1</span> Introduction</h1>
<p>This course (<em>Linear Models</em>) is a classical treatment of linear models, which are among the most popular and simple models for statistical data analysis and prediction. The models describe the relationship between the mean of an outcome variable <span class="math inline">\(Y\)</span> and one or more regressors <span class="math inline">\(x\)</span> via a function that is linear in the parameters. The model thus focusses on the conditional expectation <span class="math inline">\(\E{Y \mid x}\)</span>. Here is a brief overview of the content of the course:</p>
<ul>
<li><p>simple linear regression model: here the models are limited to a single regressor. In this chapter most of the concepts and theory will be presented so that in later chapters we can rely on many of the theoretical results of this chapter. Throughout the whole chapter, the methods will be illustrated by means of example datasets, and the meaning of the model and the statistical inference procedures will be demonstrated with Monte Carlo simulation studies. In particular, the following topics will be discussed:</p>
<ul>
<li><p>the model and its interpretation</p></li>
<li><p>parameter estimation (ordinary least squares and maximum likelihood) and the properties of the estimators</p></li>
<li><p>sampling distributions of the estimators. They form the basis of statistical inference.</p></li>
<li><p>confidence intervals of the parameters</p></li>
<li><p>hypothesis tests related to the parameters</p></li>
<li><p>assessment of the model assumptions</p></li>
<li><p>the use of binary dummy regressors to mimic the two-sample problem (two-sample t-test)</p></li>
<li><p>association versus causation. We will give a brief introduction to causal inference.</p></li>
<li><p>linear regression models for prediction purposes</p></li>
</ul></li>
<li><p>the multiple linear regression model: the simple linear regression model is extended to include more than one regressor. For many topics (e.g. parameter estimation, confidence intervals and hypopthesis tests) we will be able to refer to the previous chapter, which will make this chapter less theoretical. Quite some focus will be on the interpretation of the parameters and specific issues that are irrelevant for the simple linear regression model. These topics will be discussed:</p>
<ul>
<li><p>the additive model and its interpretation</p></li>
<li><p>parameter estimation, confidence intervals and hypothesis testing (with a lot of references to the previous chapter)</p></li>
<li><p>interaction effects and the non-additive model</p></li>
<li><p>sum of squares and the ANOVA table</p></li>
<li><p>multicolinearity</p></li>
<li><p>assessment of the model assumptions</p></li>
<li><p>prediction modelling and model selection</p></li>
</ul></li>
<li><p>design-related topics and causal inference. In this chapter we discuss in some more detail the importance of the study design and some further concepts in causal inference. In particular:</p>
<ul>
<li><p>blocking and stratification</p></li>
<li><p>estimability</p></li>
<li><p>randomisation</p></li>
<li><p>confouders</p></li>
<li><p>causality, causal graphs, collapsibility</p></li>
</ul></li>
<li><p>Analysis of variance (ANOVA). The approach taken is this course, is to embed anova models in regression models. So in this chapter you first learn about the conventional formulation of anova models, and subsequently how these models can be rewritten as linear regression models so that the theory and methods from the previous chapters become available. The following topics will be treated:</p>
<ul>
<li><p>cell means and factor effects models and their reformulation as linear regression models</p></li>
<li><p>sum of squares and the ANOVA table</p></li>
<li><p>one way, two way and multiple way models</p></li>
<li><p>contrasts and multiple comparisons of means</p></li>
</ul></li>
<li><p>This course is concluded with a chapter on reporting: how to write a good statistical report. This can be seen as a stand alone chapter and can be read at any time.</p></li>
</ul>
<div id="how-to-work-through-the-course-notes" class="section level2">
<h2><span class="header-section-number">1.1</span> How to work through the course notes</h2>
<p>These course notes are written with R Bookdown in R Studio, and rendered to an html file that is made available on GitHub. If you wish, you can convert the html to a pdf file. The best way to do this, is via the <em>print</em> function of your browser. Before doing so, you can best first collapse the sidebar (that contains the table of content).</p>
<p>This course is work in progress. It is a new course and <strong>the course notes are still work in progress</strong>. So every week there will be an update. If you access the course notes via the link to GitHub you will see the updates automatically.</p>
<p>Many <strong>data analyses examples</strong> are worked out in detail in the course notes, including the R code. When you access the course notes via GitHub, you can easily copy the R code and paste it to your local R software. Here is an example of a chunck of R code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a&lt;-<span class="dv">1</span>
b&lt;-<span class="dv">2</span>
a<span class="op">+</span>b</code></pre></div>
<pre><code>## [1] 3</code></pre>
<p>If you now move your mouse towards the upper right corner of the box that contains the R code, then you will see an icon appearing in that corner:</p>
<p><img src="figures/copy.png" width="5%" style="display: block; margin: auto;" /> If you click on that icon, the content of the box will be copied to your clipboard so that you can paste it wherever you want (e.g. in a local R file).</p>
<p>I actually advise that you also <strong>work with your own local R Markdown or Notebook file</strong> to which you copy some of the R code of the course notes so that you can <em>play</em> with the data and the R code yourself. All datafiles are also available on Blackboard.</p>
<p>Throughout the course there are several exercises. The introduction to the excercise, as well as the data set and some instructions are given, but you do not directly get to see the solution. However, there is a option to expand the html so that the solution becomes visible. This should invite you to first try to make the exercise yourself (in a local R Markdown or Notebook file) before looking at the solution in the course notes. Here is an illustration:</p>
<div id="exercise" class="section level3 unnumbered">
<h3>Exercise</h3>
<p>Consider the following data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dataset&lt;-<span class="kw">data.frame</span>(<span class="dt">y=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>),
                    <span class="dt">x=</span><span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>))</code></pre></div>
<p>Make a scatter plot of <span class="math inline">\(y\)</span> versus <span class="math inline">\(x\)</span>.</p>
<p><details> <summary markdown="span">Try to make this exercise yourself. If you are ready you can expand this page and look at a solution</summary></p>
<p>Here is a solution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(dataset<span class="op">$</span>y,dataset<span class="op">$</span>x,
     <span class="dt">xlab=</span><span class="st">&quot;x&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;y&quot;</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-6-1.png" width="672" /> </details></p>
</div>
</div>
<div id="two-versions-of-this-course-oc-and-dl" class="section level2">
<h2><span class="header-section-number">1.2</span> Two versions of this course: OC and DL</h2>
<p>As for all courses in our Master of Statistics and Data Science program, each course has two versions: the <strong>on-campus (OC)</strong> and the <strong>distance learning (DL)</strong> versions.</p>
<p>The course notes are the same for both groups of students. Each group, however, has another Blackboard course site at which you can find all files and information. I keep these two Blackboard sites separate because the contact modes and hours are different:</p>
<ul>
<li><p>the OC students have live lectures (either in class, on campus, if permitted in corona times, or online) and self study assignments followed by online Q&amp;A sessions. Their lectures and Q&amp;A sessions are scheduled in their official course schedule.</p></li>
<li><p>the DL students will get access to web lectures and they can watch them at their own pace. On Blackboard I will suggest a time schedule. There are also online Q&amp;A sessions which give the students the opportunity to ask questions to the lecturer. The dates of the Q&amp;A sessions are in the Blackboard Calendar.</p></li>
</ul>
<p>The project assignment is the same for the DL and OC students.</p>
</div>
<div id="communication" class="section level2">
<h2><span class="header-section-number">1.3</span> Communication</h2>
<p>On Blackboard there is a <strong>discussion forum</strong> that can be used for asking questions.</p>
<p>Questions can also be asked during the <strong>Q&amp;A sessions</strong>, and the OC students can of course also ask questions in class (either physically when we are on-campus, or in the virtual class room when we are online).</p>
<p>You are also welcome to send emails to the lecturer, but the discussion forum is preferred so that other students can also learn from the answers to the questions.</p>
</div>
<div id="software" class="section level2">
<h2><span class="header-section-number">1.4</span> Software</h2>
<p>In the course notes all data analyses are demonstrated with the R software, but on Blackboard also documentation on the SAS software will be provided. You will also have to learn how to perform regression analysis and ANOVA with the SAS software.</p>

</div>
</div>
<div id="Ch_SimpleLinearRegression" class="section level1">
<h1><span class="header-section-number">Hoofdstuk 2</span> Simple Linear Regression Analysis</h1>
<div id="example-galtons-height-data" class="section level2 unnumbered">
<h2>Example (Galton's height data)</h2>
<p>As a first example, we consider the original dataset from Francis Galton, who invented <em>regression</em> by looking at this dataset. We obtained the data from <a href="https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/T0HSJ1/LKC7PJ&amp;version=1.1" class="uri">https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/T0HSJ1/LKC7PJ&amp;version=1.1</a> Here is the reference to the orginal paper of Francis Galton from 1886.</p>
<p>Galton, F. (1886). Regression Towards Mediocrity in Hereditary Stature. Journal of the Anthropological Institute, 15, 246-263</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Galton&lt;-<span class="kw">read.csv</span>(<span class="st">&quot;Data/Galton.tab&quot;</span>,<span class="dt">sep=</span><span class="st">&quot;</span><span class="ch">\t</span><span class="st">&quot;</span>)
<span class="kw">head</span>(Galton)</code></pre></div>
<pre><code>##   family father mother gender height kids male female
## 1      1   78.5   67.0      M   73.2    4    1      0
## 2      1   78.5   67.0      F   69.2    4    0      1
## 3      1   78.5   67.0      F   69.0    4    0      1
## 4      1   78.5   67.0      F   69.0    4    0      1
## 5      2   75.5   66.5      M   73.5    4    1      0
## 6      2   75.5   66.5      M   72.5    4    1      0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glimpse</span>(Galton)</code></pre></div>
<pre><code>## Rows: 898
## Columns: 8
## $ family &lt;fct&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5,…
## $ father &lt;dbl&gt; 78.5, 78.5, 78.5, 78.5, 75.5, 75.5, 75.5, 75.5, 75.0, 75.0, 75…
## $ mother &lt;dbl&gt; 67.0, 67.0, 67.0, 67.0, 66.5, 66.5, 66.5, 66.5, 64.0, 64.0, 64…
## $ gender &lt;fct&gt; M, F, F, F, M, M, F, F, M, F, M, M, F, F, F, M, M, M, F, F, F,…
## $ height &lt;dbl&gt; 73.2, 69.2, 69.0, 69.0, 73.5, 72.5, 65.5, 65.5, 71.0, 68.0, 70…
## $ kids   &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,…
## $ male   &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,…
## $ female &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,…</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">skim</span>(Galton)</code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-7">Table 2.1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">Galton</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">898</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">8</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">factor</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">6</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: factor</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="left">ordered</th>
<th align="right">n_unique</th>
<th align="left">top_counts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">family</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">197</td>
<td align="left">185: 15, 166: 11, 66: 11, 130: 10</td>
</tr>
<tr class="even">
<td align="left">gender</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">2</td>
<td align="left">M: 465, F: 433</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">father</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">69.23</td>
<td align="right">2.47</td>
<td align="right">62</td>
<td align="right">68</td>
<td align="right">69.0</td>
<td align="right">71.0</td>
<td align="right">78.5</td>
<td align="left">▁▅▇▂▁</td>
</tr>
<tr class="even">
<td align="left">mother</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">64.08</td>
<td align="right">2.31</td>
<td align="right">58</td>
<td align="right">63</td>
<td align="right">64.0</td>
<td align="right">65.5</td>
<td align="right">70.5</td>
<td align="left">▂▅▇▃▁</td>
</tr>
<tr class="odd">
<td align="left">height</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">66.76</td>
<td align="right">3.58</td>
<td align="right">56</td>
<td align="right">64</td>
<td align="right">66.5</td>
<td align="right">69.7</td>
<td align="right">79.0</td>
<td align="left">▁▇▇▅▁</td>
</tr>
<tr class="even">
<td align="left">kids</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">6.14</td>
<td align="right">2.69</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">6.0</td>
<td align="right">8.0</td>
<td align="right">15.0</td>
<td align="left">▃▇▆▂▁</td>
</tr>
<tr class="odd">
<td align="left">male</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.52</td>
<td align="right">0.50</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1.0</td>
<td align="right">1.0</td>
<td align="right">1.0</td>
<td align="left">▇▁▁▁▇</td>
</tr>
<tr class="even">
<td align="left">female</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.48</td>
<td align="right">0.50</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0.0</td>
<td align="right">1.0</td>
<td align="right">1.0</td>
<td align="left">▇▁▁▁▇</td>
</tr>
</tbody>
</table>
<p>The dataset contains data on heights of parents and their adult children; there can be more than one child per family. For our purpose we will select fathers and one son (we will select the first son that appears in the family).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Galton.sons&lt;-Galton <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(gender<span class="op">==</span><span class="st">&quot;M&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(family) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span>ungroup <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">father.cm=</span>father<span class="op">*</span><span class="fl">2.54</span>,
         <span class="dt">son.cm=</span>height<span class="op">*</span><span class="fl">2.54</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(father.cm, son.cm)
<span class="kw">glimpse</span>(Galton.sons)</code></pre></div>
<pre><code>## Rows: 173
## Columns: 2
## $ father.cm &lt;dbl&gt; 199.390, 175.260, 175.260, 175.260, 175.260, 176.530, 175.2…
## $ son.cm    &lt;dbl&gt; 185.928, 180.848, 190.500, 177.800, 185.420, 179.070, 180.3…</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(Galton.sons,
       <span class="kw">aes</span>(<span class="dt">x=</span>father.cm, <span class="dt">y=</span>son.cm)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">color=</span><span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;length of father (cm)&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;length of son (cm)&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>), <span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>))</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>This scatter plot suggests a possitive correlation between the heights of the father and the son.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(Galton.sons<span class="op">$</span>father.cm, Galton.sons<span class="op">$</span>son.cm)</code></pre></div>
<pre><code>## [1] 0.5022938</code></pre>
<p>In this course, however, we are not only interested in the correlation, but we want to quantify how the expected (or average) height of sons varies with the age of the father.</p>
<p>In the context of regression analysis we will use the following terminology:</p>
<ul>
<li><p>height of son is the <strong>outcome</strong> or <strong>response</strong> (or <strong>response variable</strong>) or <strong>dependent variable</strong>. This will be denoted by <span class="math inline">\(Y\)</span>.</p></li>
<li><p>height of father is the <strong>regressor</strong>, <strong>covariate</strong> or <strong>independent variable</strong>. This will be denoted by <span class="math inline">\(x\)</span>.</p></li>
</ul>
<p>One way of looking at the problem, is to consider the outcomes that correspond to fathers of a given height, as a population. In this way, for example, the heights of sons of fathers of height <span class="math inline">\(172.72\)</span>cm can be considered as a random sample of outcomes from the population of heights of sons of fathers of height <span class="math inline">\(172.72\)</span>cm. We can apply this reasoning for all heights of fathers included in the dataset, and in a more abstract way we can think of samples of outcomes for each value of the regressor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Galton.sons <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(father.cm<span class="op">==</span><span class="fl">172.72</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>father.cm,<span class="dt">y=</span>son.cm)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_boxplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">position =</span> <span class="kw">position_jitter</span>(<span class="fl">0.2</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;length of son (cm)&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>), <span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title.x=</span><span class="kw">element_blank</span>(),<span class="dt">axis.text.x=</span><span class="kw">element_blank</span>())</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>We will use the following notation:</p>
<ul>
<li><p><span class="math inline">\(n\)</span>: total number of observations (subjects, elements) in the dataset.</p></li>
<li><p><span class="math inline">\(x_i\)</span>: the value of the regressor of observation <span class="math inline">\(i=1,\ldots, n\)</span></p></li>
<li><p><span class="math inline">\(Y_i\)</span>: the outcome of observation <span class="math inline">\(i=1,\ldots, n\)</span>.</p></li>
</ul>
<p>Since every <span class="math inline">\(x_i\)</span> can identify another population, we say that <span class="math inline">\(Y_i\)</span> is a random outcome from the population with (cumulative) distribution function (CDF) <span class="math display">\[
  F_i(y)=F(y;x_i,\mb\beta),
\]</span> i.e. the distribution is determined by <span class="math inline">\(x_i\)</span> and possibly by a parameter vector <span class="math inline">\(\mb\beta\)</span>. We also assume that all <span class="math inline">\(n\)</span> outcomes are mutually independent. Note that we do not say that the <span class="math inline">\(n\)</span> outcomes are i.i.d. (<em>identically and independently distributed</em>), because not all <span class="math inline">\(F_i\)</span> coincide.</p>
<p>Regression analysis is a method that allows to study the effect of a regressor on the <strong>mean outcome</strong>. We therefore also introduce a notation for the mean of the distribution <span class="math inline">\(F_i\)</span>, <span class="math display">\[
  \mu_i = \E{Y_i} = \Ef{F_i}{Y_i} = \int_{-\infty}^{+\infty} y dF(y;x_i,\mb\beta) = \E{Y_i \mid x_i}.
\]</span> This notation stresses that <span class="math inline">\(\mu_i\)</span> is the mean of <span class="math inline">\(Y_i\)</span> and that this mean depends on the value of the regressor <span class="math inline">\(x_i\)</span> because the distribution of <span class="math inline">\(Y_i\)</span> depends on <span class="math inline">\(x_i\)</span>. Therefore, <span class="math inline">\(\mu_i=\E{Y_i \mid x_i}\)</span> is the <strong>conditional mean</strong> of the outcome, given <span class="math inline">\(x_i\)</span>. To stress that the conditional mean is a function of the regressor and possibly of a parameter <span class="math inline">\(\mb\beta\)</span> we write <span class="math display">\[
  m(x;\mb\beta) = \E{Y \mid x}.
\]</span></p>
<p>In the two previous paragraphs we actually gave a generic description of a <strong>statistical model</strong>. We repeat here the description, with in slightly more general fashion.</p>
<ul>
<li><p>For a given <span class="math inline">\(x_i\)</span>, we provide the conditional distribution of the outcome, <span class="math display">\[
   Y_i \mid x_i \sim F_i(\cdot;x_i,\mb\beta,\mb\nu)
\]</span> in which <span class="math inline">\(\mb\beta\)</span> and <span class="math inline">\(\mb\nu\)</span> are two parameter vectors.</p></li>
<li><p>The conditional mean of this distribution is described as <span class="math display">\[
   \E{Y_i\mid x_i}=m(x_i;\mb\beta).
\]</span></p></li>
<li><p>The <span class="math inline">\(n\)</span> outcomes <span class="math inline">\(Y_i\)</span> are mutually independent.</p></li>
</ul>
<p>Consider the follow special cases:</p>
<ul>
<li><p>Suppose there are no parameters <span class="math inline">\(\mb\beta\)</span> and <span class="math inline">\(\mb\nu\)</span> and the functions <span class="math inline">\(m\)</span> and <span class="math inline">\(F_i\)</span> are not known. Then the model imposes no restriction on the conditional distribution <span class="math inline">\(Y_i \mid x_i\)</span>. This is a <strong>nonparametric model</strong>.</p></li>
<li><p>Suppose that the function <span class="math inline">\(m\)</span> is known, up to the parameter vector <span class="math inline">\(\mb\beta\)</span>, but the CDFs <span class="math inline">\(F_i\)</span> have no further restrictions (it does of course satisfy the restriction <span class="math inline">\(\E{Y_i\mid x_i}=m(x_i;\mb\beta)\)</span>). Then the statistical model only imposes restrictions on the conditional mean, but leaves other aspects of the conditional distribution unspecified. This is a <strong>semiparametric model</strong>.</p></li>
<li><p>Suppose that the function <span class="math inline">\(m\)</span> is known, up to the parameter vector <span class="math inline">\(\mb\beta\)</span>, and that the CDFs <span class="math inline">\(F_i\)</span> are also known, up to the parameter vector <span class="math inline">\(\mb\nu\)</span>. In this case, the <span class="math inline">\(n\)</span> CDFs <span class="math inline">\(F_i\)</span> are often equal to one another, i.e. <span class="math inline">\(F_i=F\)</span>. Of course, as for the semiparametric model, the CDF <span class="math inline">\(F\)</span> must be compatible with the restriction <span class="math inline">\(\E{Y_i\mid x_i}=m(x_i;\mb\beta)\)</span>. This model is known as a <strong>parametric model</strong>. It basically specifies the full conditional distribution up to finite dimensional parameter vectors <span class="math inline">\(\mb\beta\)</span> and <span class="math inline">\(\mb\nu\)</span>.</p></li>
</ul>
<p>With respect to the parametric models: if the interest is in the model <span class="math inline">\(\E{Y_i\mid x_i}=m(x_i;\mb\beta)\)</span>, i.e. the focus is on parameter <span class="math inline">\(\mb\beta\)</span>, then we call the parameter <span class="math inline">\(\mb\nu\)</span> a <strong>nuisance parameter</strong>.</p>
<p>In later sections we will sometimes refer to semiparametric and parametric models, and then their meaning may become clear.</p>
<p>The general regression model is illustrated in Figure <a href="#fig:RegModel">2.1</a> (left). The focus of a regression analysis is the estimation of the function <span class="math inline">\(m(x;\mb\beta)\)</span>. If the function <span class="math inline">\(m\)</span> is known, then this reduces to the estimation of the parameter <span class="math inline">\(\mb\beta\)</span> using the sample observations. Based on the estimates, regression analysis also aims to formulate conclusions on the relation between the regressor and the conditional mean of the outcome. Sometimes the estimated regression model may also be used for predicting an outcome for a given value of the regressor. These topics will all be discussed later in this course.</p>
<div class="figure" style="text-align: center"><span id="fig:RegModel"></span>
<img src="DASM2_files/figure-html/RegModel-1.png" alt="Illustration of the regression model. The black line represents the function $m$. The red points are observed outcomes, sampled from the conditional distribution of $Y$ given $x=160$. The red line shows the shape of the density function of this conditional distribution. The blue points are observed outcomes, sampled from the conditional distribution of $Y$ given $x=200$. The blue line shows the shape of the density function of this conditional distribution. Left: $m$ is a non-linear function and the conditional distributions have different shapes. Middle: $m$ is a non-linear function and the conditional distribution functions have equal shapes. Right: $m$ is a linear function and the conditional distribution functions have equal shapes (normal distributions)." width="672" />
<p class="caption">
Figure 2.1: Illustration of the regression model. The black line represents the function <span class="math inline">\(m\)</span>. The red points are observed outcomes, sampled from the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x=160\)</span>. The red line shows the shape of the density function of this conditional distribution. The blue points are observed outcomes, sampled from the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x=200\)</span>. The blue line shows the shape of the density function of this conditional distribution. Left: <span class="math inline">\(m\)</span> is a non-linear function and the conditional distributions have different shapes. Middle: <span class="math inline">\(m\)</span> is a non-linear function and the conditional distribution functions have equal shapes. Right: <span class="math inline">\(m\)</span> is a linear function and the conditional distribution functions have equal shapes (normal distributions).
</p>
</div>
We now simplify the model by assuming that the conditional distribution only depends on the regressor via its conditional mean. In other words: for every value of the regressor, the shape of the conditional outcome distribution is the same, and only the location (mean) may depend on the regressor. This is illustrated in Figure <a href="#fig:RegModel">2.1</a> (middle). This extra assumption allows us to write the statistical model as
<span class="math display" id="eq:Mod1">\[\begin{equation}
  Y_i = m(x_i;\mb\beta) + \eps_i
  \tag{2.1}
\end{equation}\]</span>
<p>with <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb\nu)\)</span>, with <span class="math inline">\(F_\eps(\cdot;\mb\nu)\)</span> the CDF of <span class="math inline">\(\eps_i\)</span>, <span class="math inline">\(\mb\nu\)</span> a parameter vector and <span class="math display">\[ 
  \E{\eps_i}=\E{\eps_i\mid x_i}=0.
\]</span> For the latter assumption, we find a useful property: <span class="math display">\[
  \E{Y\mid x}=m(x;\mb\beta)
\]</span> i.e. it allows to interpret <span class="math inline">\(m(x;\mb\beta)\)</span> as the conditional mean. The stochastic variable <span class="math inline">\(\eps_i\)</span> is often referred to as the <strong>error term</strong>. The model thus suggests that the outcome can be decomposed into two components: a <strong>systematic component</strong>, <span class="math inline">\(m(x_i;\mb\beta)\)</span>, and a <strong>stochastic component</strong> (or random component, or error term) <span class="math inline">\(\eps_i\)</span>. The latter gives the deviation between the outcome <span class="math inline">\(Y_i\)</span> and the systematic component <span class="math inline">\(m(x_i;\mb\beta)\)</span>.</p>
<p>It is important, however, to note that this decomposition often does not agree with how the outcomes are generated or realised: the variability of the outcomes <span class="math inline">\(Y_i\)</span> about the conditional mean <span class="math inline">\(m(x_i;\mb\beta)\)</span> is often inherently present in the population (e.g. biological variability). In this sense, <span class="math inline">\(\eps_i\)</span> may not be looked at as an <em>error</em> on the measurement. In other cases, however, part of the variability in <span class="math inline">\(Y_i\)</span> may be due to imprecise measurements and then <span class="math inline">\(\eps_i\)</span> can (at least partly) be considered as a random error term.</p>
<p>Note that the assumption <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb{\nu})\)</span> implies that all error terms have the same distribution and hence, for a fixed <span class="math inline">\(\sigma^2\geq 0\)</span>, <span class="math display">\[
  \var{\eps_i} = \var{\eps_i \mid x_i} = \sigma^2
\]</span> for all <span class="math inline">\(i=1,2,\ldots, n\)</span>. This restriction on the variance is referred to as the assumption of <strong>homoskedasticiteit</strong> or <strong>constant variance</strong>. The variance <span class="math inline">\(\sigma^2\)</span> is called the <strong>residual variance</strong>.</p>
<p>In this course we only discuss <strong>linear regression analysis</strong>, for which the function <span class="math inline">\(m(x;\mb\beta)\)</span> is restricted to linear functions of <span class="math inline">\(\mb\beta\)</span>: <span class="math display">\[
  m(x;\mb\beta) = \beta_0 + \beta_1x,
\]</span> with <span class="math inline">\(\mb{\beta}^t=(\beta_0, \beta_1)\)</span>.</p>
This equation represents a linear line which is referred to as the <strong>regression line</strong>. We write model <a href="#eq:Mod1">(2.1)</a> now as
<span class="math display" id="eq:Mod3">\[\begin{equation}
  Y_i = \beta_0 + \beta_1x_i + \eps_i
  \tag{2.2}
\end{equation}\]</span>
<p>with <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb\nu)\)</span> and <span class="math inline">\(\E{\eps_i}=\E{\eps_i\mid x_i}=0\)</span>. This is called the <strong>simple linear regression model</strong>. See also Figure <a href="#fig:RegModel">2.1</a> (right).</p>
<p>The interpretation of the parameter <span class="math inline">\(\beta_1\)</span> follows from the identity <span class="math display">\[
  \E{Y\mid x+1} - \E{Y\mid x} = \left(\beta_0+\beta_1(x+1)\right)-\left(\beta_0+\beta_1 x\right) = \beta_1 .
\]</span> The parameter <span class="math inline">\(\beta_1\)</span> is thus the average increase in the outcome when the regressor increases with one unit. This parameter is also the <strong>slope</strong> of the regression line. The parameter is often referred to as the <strong>regression coefficient</strong>.</p>
<p>The interpretation of the parameter <span class="math inline">\(\beta_0\)</span> follows from the identity <span class="math display">\[
  \E{Y\mid x=0} = \beta_0+\beta_1 \times 0 = \beta_0 .
\]</span> The parameter <span class="math inline">\(\beta_0\)</span> is thus the average outcome when the regressor takes value zero. It is the <strong>intercept</strong> of the regression line.</p>
<p>Sometimes the situation <span class="math inline">\(x=0\)</span> does not have a physical meaning or it falls outside of the <strong>scope</strong> of the model (i.e. the range of <span class="math inline">\(x\)</span>-values that forms the focus of the data analysis). For this reason, the regression model is sometimes formulated as <span class="math display">\[
  Y_i = \beta_0 + \beta_1(x_i-\bar{x}) + \eps_i
\]</span> with <span class="math inline">\(\bar{x}\)</span> the sample mean of the regressor observations in the dataset, <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb\nu)\)</span> and <span class="math inline">\(\E{\eps_i}=\E{\eps_i\mid x_i}=0\)</span>. The interpretation of <span class="math inline">\(\beta_0\)</span> now becomes <span class="math inline">\(\beta_0=\E{Y\mid x=\bar{x}}\)</span>. Since the sample mean <span class="math inline">\(\bar{x}\)</span> is often withing the scope of the model, the parameter <span class="math inline">\(\beta_0\)</span> now has a real meaning. The interpretation of <span class="math inline">\(\beta_1\)</span> remains unchanged.</p>
<p>In the Galton example, the scope is approximately <span class="math inline">\([150 \text{cm},200 \text{cm}]\)</span>.</p>
<p>In this chapter we discuss methods for the estimation of the parameters in the linear regression model <a href="#eq:Mod3">(2.2)</a>. We will find the sampling distribution of the parameter estimators. This will form the basis for hypothesis tests and for confidence intervals.</p>
</div>
<div id="S:RegSimStudy" class="section level2">
<h2><span class="header-section-number">2.1</span> Interpretation via simulations</h2>

<p>Regression model <a href="#eq:Mod3">(2.2)</a> thus gives an interpretation to the <span class="math inline">\(\beta\)</span> parameters via the conditional expectation of the outcomes. In this section we provide an interpretation via the principle of <strong>repeated sampling</strong>, which can be easily demonstrated with Monte Carlo simulations. Since this is the first simulation in this course, we shall go through it step by step.</p>
<p>We start with model <a href="#eq:Mod3">(2.2)</a>: <span class="math display">\[
  Y_i = \beta_0 + \beta_1x_i + \eps_i
\]</span> with <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb\nu)\)</span> and <span class="math inline">\(\E{\eps_i}=\E{\eps_i\mid x_i}=0\)</span>. We set <span class="math inline">\(F_\eps\)</span> to the normal distribution with mean zero and variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>We start with the simulation of a sample of 5 outcomes (son's heights) at <span class="math inline">\(\mb{x}^t=(165, 170, 175, 180, 185)\)</span> (father's heights).</p>
<p>Before we can start the simulations, we need to set the parameters to specific values. We choose: <span class="math display">\[
  \beta_0=90 \;\;\; \beta_1=0.5 \;\;\; \sigma=25.
\]</span> Thus, we simulate as if we known the truth (i.e. the population). In this context, we refer to this model as the <strong>data generating model</strong>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">724245</span>)
x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span>
x</code></pre></div>
<pre><code>## [1] 165 170 175 180 185</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">eps&lt;-<span class="kw">rnorm</span>(<span class="dv">5</span>,<span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 error terms</span>
eps</code></pre></div>
<pre><code>## [1]  2.6552611 -4.3398292  2.1952809  6.1667742 -0.9250292</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span>eps <span class="co"># random sample van uitkomsten</span>
y   </code></pre></div>
<pre><code>## [1] 175.1553 170.6602 179.6953 186.1668 181.5750</code></pre>
<p>The following R code gives Figure <a href="#fig:RegSim1">2.2</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(x,y,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>)
<span class="kw">abline</span>(<span class="kw">c</span>(<span class="dv">90</span>,<span class="fl">0.5</span>),<span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:RegSim1"></span>
<img src="DASM2_files/figure-html/RegSim1-1.png" alt="Scatter plot of 1 simulated sample from the (data generating) regression model. The red line is the linear regression model with the true parameter values." width="672" />
<p class="caption">
Figure 2.2: Scatter plot of 1 simulated sample from the (data generating) regression model. The red line is the linear regression model with the true parameter values.
</p>
</div>
<p>Next we repeat this procedure (experiment) multiple times. Each time other outcomes will be generated. The following R code generates <span class="math inline">\(N=100\)</span> repeated experiments, each with <span class="math inline">\(n=5\)</span> outcomes as described earlier. The resulst are visualised in Figure <a href="#fig:RegSim2">2.3</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">254111</span>)
N&lt;-N100 <span class="co"># number of repeated experiments</span>
x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span>
y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">5</span>,<span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span>
<span class="kw">plot</span>(x,y,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">160</span>,<span class="dv">195</span>))
<span class="kw">abline</span>(<span class="kw">c</span>(<span class="dv">90</span>,<span class="fl">0.5</span>),<span class="dt">col=</span><span class="dv">2</span>)

Data&lt;-<span class="kw">data.frame</span>(<span class="dt">experiment=</span><span class="dv">1</span>,<span class="dt">x=</span>x,<span class="dt">y=</span>y)

<span class="cf">for</span>(experiment <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>N) {
    y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">5</span>,<span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span>
    <span class="kw">points</span>(x,y,<span class="dt">col=</span>experiment)
    Data&lt;-<span class="kw">rbind</span>(Data,<span class="kw">cbind</span>(experiment,x,y))
}</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:RegSim2"></span>
<img src="DASM2_files/figure-html/RegSim2-1.png" alt="Scatter plot of $N=100$ simulated samples (experiments) from a regression model. Each color corresponds to a repeated experiment. The red line is the linear regression line with the true parameter values." width="672" />
<p class="caption">
Figure 2.3: Scatter plot of <span class="math inline">\(N=100\)</span> simulated samples (experiments) from a regression model. Each color corresponds to a repeated experiment. The red line is the linear regression line with the true parameter values.
</p>
</div>
<p>Now we look at the histograms of the repeated samples; each histogram corresponds to another value of <span class="math inline">\(x\)</span>. See Figure <a href="#fig:RegSim3">2.4</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {
  y&lt;-Data<span class="op">$</span>y[Data<span class="op">$</span>x<span class="op">==</span>x[i]]
  <span class="kw">hist</span>(y,<span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&quot;x=&quot;</span>,x[i]),<span class="dt">xlab=</span><span class="st">&quot;y&quot;</span>)
  <span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x[i],<span class="dt">col=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>)
  <span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">mean</span>(y),<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>)
}
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:RegSim3"></span>
<img src="DASM2_files/figure-html/RegSim3-1.png" alt="Histogram of the $N=100$ repeated experiments. Each histogram corresponds to another value of $x$. The red vertical lines show the average outcomes according to the true regression model and the blue dashed lines are the averages of the repeated outcomes at the corresponding temperatures." width="672" />
<p class="caption">
Figure 2.4: Histogram of the <span class="math inline">\(N=100\)</span> repeated experiments. Each histogram corresponds to another value of <span class="math inline">\(x\)</span>. The red vertical lines show the average outcomes according to the true regression model and the blue dashed lines are the averages of the repeated outcomes at the corresponding temperatures.
</p>
</div>
<p>Every histogram in Figure <a href="#fig:RegSim3">2.4</a> approximately shows a normal distribution. This is expected, because we have sampled from a normal distribution for each value of <span class="math inline">\(x\)</span>. In particular, for a given value of <span class="math inline">\(x\)</span>, we have sampled the outcomes <span class="math inline">\(Y\)</span> from the distribution <span class="math inline">\(N(90+0.5x,25)\)</span>.</p>
<p>Figure <a href="#fig:RegSim3">2.4</a> also shows the sample means of the N=100 repeated outcomes for each given <span class="math inline">\(x\)</span>. If <span class="math inline">\(N\)</span> is very large, then these sample means are approximately equal to the corresponding expected values <span class="math inline">\(\E{Y\mid x}=\beta_0+\beta_1x\)</span>. Also these expected values (with the given parameter values) are depicted in the graph; these are the points on the true regression line. Both lines are very close to one another. If <span class="math inline">\(N\)</span> were much larger then 100, we would expect that the lines would be even closer to one another. The next R code gives the numerical values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Results&lt;-<span class="kw">data.frame</span>(<span class="dt">x=</span><span class="ot">NA</span>,<span class="dt">EmpMean=</span><span class="ot">NA</span>,<span class="dt">ExpectedValue=</span><span class="ot">NA</span>,
                    <span class="dt">EmpVariance=</span><span class="ot">NA</span>,<span class="dt">Variance=</span><span class="ot">NA</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {
  y&lt;-Data<span class="op">$</span>y[Data<span class="op">$</span>x<span class="op">==</span>x[i]]
  Results[i,]&lt;-<span class="kw">c</span>(x[i],<span class="kw">round</span>(<span class="kw">mean</span>(y),<span class="dv">2</span>),<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x[i],
                    <span class="kw">round</span>(<span class="kw">var</span>(y),<span class="dv">2</span>),<span class="dv">25</span>)
}
Results</code></pre></div>
<pre><code>##     x EmpMean ExpectedValue EmpVariance Variance
## 1 165  172.63         172.5       22.46       25
## 2 170  174.69         175.0       27.60       25
## 3 175  177.92         177.5       23.17       25
## 4 180  181.08         180.0       24.82       25
## 5 185  182.94         182.5       25.68       25</code></pre>
<p>This R code also shows the sample variances of the simulated outcomes for the 5 values of <span class="math inline">\(x\)</span>. The true variance according the data generating model equals 25. Also here we see a good agreement.</p>
</div>
<div id="S:LSE1" class="section level2">
<h2><span class="header-section-number">2.2</span> Least squares estimators</h2>
<p>Consider again model <a href="#eq:Mod3">(2.2)</a>, <span class="math display">\[
  Y_i = \beta_0 + \beta_1x_i + \eps_i
\]</span> with <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb\nu)\)</span> and <span class="math inline">\(\E{\eps_i}=\E{\eps_i\mid x_i}=0\)</span>.</p>
<p>The <strong>least squares</strong> estimation method can be applied without knowledge of the exact shape of the distribution of the error term. In this sense, the model is an example of <strong>semiparametric statistical model</strong>: the conditional mean is parameterised (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>) and posesses a restriction on the conditional distributions, but the other moments of the conditional outcome distribution (or error term distribution) remain unspecified.</p>
<p>In this section we aim at estimating the regression parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, which will be denoted by <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>. The estimated regression line should come <em>as close as possible</em> to the observed outcomes in the sample. We will need a measure for the distance between the estimated regression line and the observed sample data. For given estimates <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>, the estimated regression line is given by <span class="math display">\[
  \hat{m}(x)=m(x;\hat{\mb\beta})= \hat\beta_0+\hat\beta_1 x.
\]</span> This line is sometimes referred to as the <strong>fitted</strong> regression line. For given <span class="math inline">\(x\)</span>, this gives an estimate of the conditional mean of the outcome <span class="math inline">\(Y\)</span>. For the <span class="math inline">\(n\)</span> sample observations, the points on the estimated regression line are denoted by <span class="math display">\[
  \hat{Y}_i = \hat{m}(x_i)=m(x_i;\hat{\mb\beta})=\hat\beta_0+\hat\beta_1 x_i \;\;\; i=1,\ldots, n.
\]</span> The <span class="math inline">\(\hat{Y}_i\)</span>s are often called the <strong>predictions</strong> of the outcomes, but in many cases this is a misleading terminology because the points on the estimated regression line should in the first place not be considered as predictions, but rather as estimates of the conditional mean. On the other hand, if no extra information is available, then <span class="math inline">\(\hat{m}(x)\)</span> is a good prediction for an outcome at regression value <span class="math inline">\(x\)</span> (see further). In this course we will stick to the conventional terminology (predictions), but in the first place we consider them as estimates of the conditional means.</p>
<p>Good parameter estimates <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> should make the predictions <span class="math inline">\(\hat{Y}_i\)</span> come as close as possible to the observed outcomes <span class="math inline">\(Y_i\)</span>. This can be quantified by the <strong>least squares criterion</strong></p>
<span class="math display" id="eq:SSEReg1">\[\begin{equation}
  \SSE(\hat{\mb\beta}) = \sum_{i=1}^n \left(Y_i - \hat{Y}_i\right)^2 = \sum_{i=1}^n \left(Y_i - m(x_i;\hat{\mb\beta})\right)^2.
  \tag{2.3}
\end{equation}\]</span>
<p>SSE is the abbreviation of <strong>sum of squares of the error</strong>, which is also referred to as the <strong>residual sum of squares</strong> or the <strong>sum of squared errors</strong>. This brings us to the following definition.</p>

<div class="definition">
<span id="def:SSE1" class="definition"><strong>Definition 2.1  (Least squares parameter estimator)  </strong></span>The least squares parameter estimator of <span class="math inline">\(\mb\beta\)</span> is given by <span class="math display">\[
  \hat{\mb\beta} = \text{ArgMin}_{\mb\beta} \SSE(\mb\beta).
\]</span>
</div>

<p>We use the abbreviation LSE for the <strong>least squares estimator</strong>.</p>
<p>Figure <a href="#fig:RegLSE">2.5</a> shows three datasets with fitted regression lines and with the indication of <span class="math inline">\(Y_i - \hat{Y}_i\)</span>. The deviation <span class="math inline">\(Y_i - \hat{Y}_i\)</span> is called the <strong>residual</strong>, which is often written as <span class="math display">\[
  e_i = Y_i - \hat{Y}_i \;\text{ or }\; e_i(\hat{\mb{\beta}}) =Y_i -m(x_i;\hat{\mb{\beta}}).
\]</span> With this notation we write <span class="math display">\[
  \SSE=\sum_{i=1}^n e_i^2 \;\text{ or }\; \SSE(\hat{\mb{\beta}})=\sum_{i=1}^n e_i^2(\hat{\mb{\beta}}).
\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:RegLSE"></span>
<img src="DASM2_files/figure-html/RegLSE-1.png" alt="Illustration of the estimation of the regession line. The black line shows the function $m$; this function is in practice not known and need to be estimated based on five sample observations (red points). The estimated regression line is depicted as the red dashed line. The vertical lines connect the observed outcomes $y_i$ with the predictions. The lengths of these vertical lines form the basis for SSE. Each of the three graphs start with another (randomly selected) sample. Left: estimates of $\beta_0$ and $\beta_1$ are $51.9$ and $0.71$ with SSE$=52.75$. Middle: $133.3$ and $0.26$ with SSE$=35.94$. Right: $122.3$ and $0.31$ with SSE$=3.18$." width="672" />
<p class="caption">
Figure 2.5: Illustration of the estimation of the regession line. The black line shows the function <span class="math inline">\(m\)</span>; this function is in practice not known and need to be estimated based on five sample observations (red points). The estimated regression line is depicted as the red dashed line. The vertical lines connect the observed outcomes <span class="math inline">\(y_i\)</span> with the predictions. The lengths of these vertical lines form the basis for SSE. Each of the three graphs start with another (randomly selected) sample. Left: estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are <span class="math inline">\(51.9\)</span> and <span class="math inline">\(0.71\)</span> with SSE<span class="math inline">\(=52.75\)</span>. Middle: <span class="math inline">\(133.3\)</span> and <span class="math inline">\(0.26\)</span> with SSE<span class="math inline">\(=35.94\)</span>. Right: <span class="math inline">\(122.3\)</span> and <span class="math inline">\(0.31\)</span> with SSE<span class="math inline">\(=3.18\)</span>.
</p>
</div>
<p>Before we continue, we introduce the <strong>matrix notation</strong> for the model. We introduce the following notation:</p>
<ul>
<li><p>parameter vector <span class="math inline">\(\mb\beta^t=(\beta_0, \beta_1)\)</span> and estimate <span class="math inline">\(\hat{\mb\beta}^t=(\hat\beta_0, \hat\beta_1)\)</span></p></li>
<li><p>outcome vector <span class="math inline">\(\mb{Y}^t=(Y_1,\ldots, Y_n)\)</span></p></li>
<li><p><strong>design matrix</strong> (<span class="math inline">\(n\times 2\)</span> matrix) <span class="math display">\[
 \mb{X}=\begin{pmatrix}
   1 &amp; x_1 \\
   1 &amp; x_2 \\
   \vdots &amp; \vdots \\
   1 &amp; x_n
 \end{pmatrix}.
 \]</span> The <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mb{X}\)</span> is represented by <span class="math inline">\(\mb{x}_i^t=(1,x_i)\)</span>.</p></li>
<li><p>error vector <span class="math inline">\(\mb{\eps}^t=(\eps_1,\ldots, \eps_n)\)</span>.</p></li>
</ul>
<p>With the vector and matrix notation we rewrite model <a href="#eq:Mod3">(2.2)</a> as <span class="math display">\[
  Y_i = \mb{x}_i^t\mb\beta+\eps_i
\]</span> or as <span class="math display">\[
  \mb{Y} = \mb{X}\mb\beta + \mb{\eps}
\]</span> with <span class="math inline">\(\eps_i \iid\)</span> and <span class="math inline">\(\E{\eps_i}=\E{\eps_i \mid x_i}=0\)</span> and <span class="math inline">\(\var{\eps_i}=\sigma^2\)</span>.</p>
<p>With this notation we write SSE from <a href="#eq:SSEReg1">(2.3)</a> as <span class="math display">\[
  \SSE(\hat{\mb\beta})=\Vert \mb{Y} - \mb{X}\hat{\mb\beta}\Vert^2
\]</span> and the LSE from Definition <a href="#def:SSE1">2.1</a> becomes <span class="math display">\[
  \hat{\mb\beta} = \text{ArgMin}_{\mb\beta} \SSE(\mb\beta) = \text{ArgMin}_{\mb\beta} \Vert \mb{Y} - \mb{X}\mb\beta\Vert^2.
\]</span></p>
<p>The solution of the minimisation problem that results in the LSE is given next and formulated as a theorem.</p>

<div class="theorem">
<span id="thm:LSEReg1" class="theorem"><strong>Theorem 2.1  (LSE for simple linear regression)  </strong></span> Assume that model <a href="#eq:Mod3">(2.2)</a> is correct is and that the <span class="math inline">\(n\times 2\)</span> design matrix <span class="math inline">\(\mb{X}\)</span> has rank <span class="math inline">\(2\)</span> heeft. Then, the LSE of <span class="math inline">\(\mb\beta\)</span> is given by <span class="math display">\[
  \hat{\mb\beta} = (\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}  
\]</span> and this solution is unique.
</div>

<p><strong>Proof</strong></p>
<p>First we show that <span class="math inline">\(\hat{\mb\beta} = (\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}\)</span>.</p>
<p>We find the LSE by means of vector differentiation (see Appendix <a href="#app:VecDiff">A</a>).</p>
First we write
<span class="math display">\[\begin{eqnarray*}
\|\mb{Y} - \mb{X}\mb\beta \|^2 
   &amp;=&amp; (\mb{Y} - \mb{X} \mb\beta)^t (\mb{Y}-\mb{X}\mb\beta)\\
   &amp;=&amp; \mb{Y}^t\mb{Y} - \mb\beta^t \mb{X}^t \mb{Y} - \mb{Y}^t \mb{X} \mb\beta + \mb\beta^t \mb{X}^t \mb{X} \mb\beta .
\end{eqnarray*}\]</span>
<p>Applying vector differentiation, we find <span class="math display">\[
  \frac{d}{d\mb\beta} \|\mb{Y} - \mb{X} \mb\beta\|^2 = -2\mb{X}^t\mb{Y} + 2\mb{X}^t\mb{X} \mb\beta .
\]</span></p>
<p>The LSE of <span class="math inline">\(\mb\beta\)</span> satisfies</p>
<span class="math display">\[\begin{eqnarray*}
  \frac{d}{d\mb\beta} \|\mb{Y}- \mb{X} \mb\beta\|^2 
   &amp;=&amp; 0\\
   &amp;\Updownarrow&amp; \\
  \mb{X}^t \mb{X} \mb\beta 
   &amp;=&amp; \mb{X}^t \mb{Y}
\end{eqnarray*}\]</span>
<p>The solution (<span class="math inline">\(\mb{X}\)</span> has full rank and hence <span class="math inline">\(\mb{X}^t\mb{X}\)</span> is invertible) is thus given by <span class="math display">\[
   \hat{\mb\beta} = (\mb{X}^t\mb{X})^{-1} \mb{X}^t \mb{Y}.
\]</span> For demonstrating that this estimate minimises the least squares criterion, we must show that the matrix of partial derivatives of second order is positive definite. <span class="math display">\[
    \frac{d^2}{d\mb\beta d\mb\beta^t}\|\mb{Y}- \mb{X} \mb\beta\|^2 = \frac{d}{d\mb\beta} (-2\mb{X}^t\mb{Y} + 2\mb{X}^t\mb{X} \mb\beta) = 2\mb{X}^t\mb{X}.
  \]</span> The <span class="math inline">\(2 \times 2\)</span> matrix <span class="math inline">\(\mb{X}^t\mb{X}\)</span> is positive definite because <span class="math inline">\(\mb{X}\)</span> is of full rank.</p>
<p>Finally we have to prove that we have a unique solution.</p>
<p>Suppose that there are two different solutions (say <span class="math inline">\(\hat{\mb\beta}_1\)</span> and <span class="math inline">\(\hat{\mb\beta}_2\)</span> such that <span class="math inline">\(\hat{\mb\beta}_1 \neq \hat{\mb\beta}_2\)</span>); then it holds that</p>
<span class="math display">\[\begin{eqnarray*}
  \mb{X}^t\mb{Y} 
     &amp;=&amp; \mb{X}^t \mb{X} \hat{\mb\beta}_1\\
     &amp;=&amp; \mb{X}^t \mb{X} \hat{\mb\beta}_2.
\end{eqnarray*}\]</span>
<p>Hence, <span class="math inline">\(\mb{X}^t\mb{X}(\hat{\mb\beta}_1 - \hat{\mb\beta}_2)=0\)</span>. Because <span class="math inline">\(\mb{X}^t\mb{X}\)</span> is of full rank, the unique solution of the system of equation given by <span class="math inline">\(\mb{X}^t\mb{X}\mb{v} = 0\)</span> is provided by the null solution <span class="math inline">\(\mb{v} = \mb{0}\)</span>. Therefore the following equality must hold true: <span class="math inline">\(\hat{\mb\beta}_1 -\hat{\mb\beta}_2 = \mb{0}\)</span>. Hence, the supposition <span class="math inline">\(\hat{\mb\beta}_1 \neq \hat{\mb\beta}_2\)</span> gives a contradiction and hence <span class="math inline">\(\hat{\mb\beta}_1=\hat{\mb\beta}_2\)</span> must be true, i.e. there is only one unique solution. </p>
<hr />
<p>If we work out the matrix formulation for <span class="math inline">\(\hat{\mb\beta}=(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}\)</span> we find</p>
<span class="math display">\[\begin{eqnarray*}
  \hat\beta_0 
    &amp;=&amp; \bar{Y} - \hat\beta_1 \bar{x}\\
  \hat\beta_1
    &amp;=&amp; \frac{\sum_{i=1}^n (Y_i-\bar{Y})(x_i-\bar{x})}{\sum_{i=1}^n (x_i-\bar{x})^2}.
 \end{eqnarray*}\]</span>
<p>This solution could also be found directly by computing the partial derivatives of SSE w.r.t. the two parameters, setting these derivatives to zero and solving the system of equations for the two parameters (later we will see that our solution via vector differentiation is more general and also applies to multiple linear regression).</p>
<span class="math display">\[\begin{eqnarray*}
  \frac{\partial}{\partial \beta_0} \SSE(\mb\beta)
    &amp;=&amp; 0\\
  \frac{\partial}{\partial \beta_1} \SSE(\mb\beta)
    &amp;=&amp; 0.
 \end{eqnarray*}\]</span>
<p>This gives</p>
<span class="math display" id="eq:LSEEE1" id="eq:LSEEE0">\[\begin{eqnarray}
  \frac{\partial}{\partial \beta_0} \SSE(\mb\beta)
    &amp;=&amp; -2\sum_{i=1}^n (Y_i - \beta_0-\beta_1 x_i) =0 
    \tag{2.4} \\
  \frac{\partial}{\partial \beta_1} \SSE(\mb\beta)
    &amp;=&amp; -2\sum_{i=1}^n x_i(Y_i - \beta_0-\beta_1 x_i)=0.
    \tag{2.5}
 \end{eqnarray}\]</span>
<p>Equations <a href="#eq:LSEEE0">(2.4)</a> and <a href="#eq:LSEEE1">(2.5)</a> are called the <strong>estimating equations</strong>. In the context of LSE for linear regression models they are also known as the <strong>normal equations</strong>.</p>
</div>
<div id="example-galtons-height-data-1" class="section level2 unnumbered">
<h2>Example (Galton's height data)</h2>
<p>We estimate the parameters of the regression line for Galton's data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(son.cm<span class="op">~</span>father.cm,<span class="dt">data=</span>Galton.sons)
m</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = son.cm ~ father.cm, data = Galton.sons)
## 
## Coefficients:
## (Intercept)    father.cm  
##     89.8182       0.5077</code></pre>
<p>The estimated (or fitted) regression line is thus given by <span class="math display">\[
  \hat{m}(x) = 89.8 + 0.51 x.
\]</span></p>
<p>Figure <a href="#fig:GaltonFittedReg">2.6</a> shows the scatter plot and the fitted regression line. Sometimes the fitted regression line is represented as <span class="math display">\[
  \hat{y}_i = 89.8 + 0.51 x_i .
\]</span> Interpretation: if the height of the fathers increases with 1cm, then the average height of their sons is estimated to increase with <span class="math inline">\(0.5\)</span>cm.</p>
<p>The intercept, however, has no direct physical interpretation because there are no fathers of height 0cm. This issue can be resolved by first centering the regressor. This is illustrated next.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Galton.sons&lt;-Galton.sons <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">father.cm.centered=</span>father.cm<span class="op">-</span><span class="kw">mean</span>(father.cm))
<span class="kw">mean</span>(Galton.sons<span class="op">$</span>father.cm)</code></pre></div>
<pre><code>## [1] 175.4905</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(Galton.sons<span class="op">$</span>father.cm.centered)</code></pre></div>
<pre><code>## [1] 6.242341e-15</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m2&lt;-<span class="kw">lm</span>(son.cm<span class="op">~</span>father.cm.centered,<span class="dt">data=</span>Galton.sons)
m2</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = son.cm ~ father.cm.centered, data = Galton.sons)
## 
## Coefficients:
##        (Intercept)  father.cm.centered  
##           178.9070              0.5077</code></pre>
<p>First note that the estimate of the slope remains unchanged after centering the regressor. The intercept is now estimanted by <span class="math inline">\(\hat\beta_0=178.9\)</span>. Hence, when the centered regressor equals 0, i.e. when fathers have average height (175.5cm), their sons have an estimated average height of <span class="math inline">\(178.9\)</span>cm.</p>
<div class="figure" style="text-align: center"><span id="fig:GaltonFittedReg"></span>
<img src="DASM2_files/figure-html/GaltonFittedReg-1.png" alt="Scatter plot of the Galton data and the fitted regression line (black). The red line is the diagonal line." width="672" />
<p class="caption">
Figure 2.6: Scatter plot of the Galton data and the fitted regression line (black). The red line is the diagonal line.
</p>
</div>
<p><strong>An historical note</strong>: Figure <a href="#fig:GaltonFittedReg">2.6</a> shows the fitted regression line, but also the diagonal line. If the fitted regression line would coincide with the diagonal, then this would indicate that the average height of sons equals the height of their fathers. However, this is obviously not the case for Galton's dataset. From the graph we see that the smaller fathers have sons that are on average taller than them. On the other hand, the taller fathers have sons that are on average smaller than them. In 1886 Galton also observed this phenomenon, which he called <em>regression towards mediocrity</em>. In his 1886 paper he develop the basis of modern regression analysis (without the statistical inference); the term <em>regression</em> comes from his paper on the analysis of heights of parents and children. Nowadays, <em>regression towards mediocrity</em> is known as <em>regression to the mean</em>.</p>
<p>Finally, note that the estimates computed for this example, do not give any appreciation of the (im)precision with which they were estimated. To what extent can we thrust these estimates? To answer that question, we need the sampling distribution of the estimators. This will be discussed later in this course.</p>
</div>
<div id="exercise-blood-pressure" class="section level2 unnumbered">
<h2>Exercise: blood pressure</h2>
<p>In a small dose-finding study of a blood pressure reducing drug, 40 high blood pressure patients (systolic blood pressure at least 150 mmHg) were randomised over 4 concentrations of the active compound (<em>arginine</em>) in the drug: 0, 2, 5 and 10 mg per day. The outcome is the systolic blood pressure reduction after 2 months, measured in mmHg. The data can be read as shown in the next chunck of R code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;Data/BloodPressure.RData&quot;</span>)</code></pre></div>
<p>Fit a linear regression to the data and interpret the regression coefficient.</p>
<p><details> <summary markdown="span">Try to make this exercise yourself. If you are ready you can expand this page and look at a solution</summary></p>
<p>First we explore the dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(BloodPressure,
       <span class="kw">aes</span>(<span class="dt">x=</span>dose, <span class="dt">y=</span>bp.reduction)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">color=</span><span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;dose (mg / day&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;systolic blood pressure reduction (mmHg)&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>), <span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>))</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Next we fit the linear regression model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.bloodpressure&lt;-<span class="kw">lm</span>(bp.reduction<span class="op">~</span>dose, <span class="dt">data=</span>BloodPressure)
m.bloodpressure</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = bp.reduction ~ dose, data = BloodPressure)
## 
## Coefficients:
## (Intercept)         dose  
##     0.03304      1.78634</code></pre>
<p>From the output we read <span class="math inline">\(\hat\beta_1=\)</span> 1.7863436. Hence, we conclude that we estimate that on average the systolic blood pressure reducses with 1.8mmHg over a period of two months, with an increase of the daily dose of 1mg.</p>
<p>The next graph shows the estimated regression line.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(BloodPressure,
       <span class="kw">aes</span>(<span class="dt">x=</span>dose, <span class="dt">y=</span>bp.reduction)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">color=</span><span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;dose (mg / day&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;systolic blood pressure reduction (mmHg)&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>), <span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept=</span>m.bloodpressure<span class="op">$</span>coefficients[<span class="dv">1</span>],
              <span class="dt">slope=</span>m.bloodpressure<span class="op">$</span>coefficients[<span class="dv">2</span>]) </code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p></details></p>
</div>
<div id="S:PropLSE" class="section level2">
<h2><span class="header-section-number">2.3</span> Properties of the Least Squares Estimator</h2>

<div id="mean-and-variance-of-the-lse" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Mean and variance of the LSE</h3>

<p>The next theorem gives two important properties of the LSE. </p>

<div class="theorem">
<p><span id="thm:LSEMeanVar" class="theorem"><strong>Theorem 2.2  (Mean and variance of the LSE)  </strong></span>Assume that model <a href="#eq:Mod3">(2.2)</a> is correct and that rank(<span class="math inline">\(\mb{X}\)</span>)=<span class="math inline">\(2\)</span> (<span class="math inline">\(2\leq n\)</span>). Then the following holds</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\E{\hat{\mb\beta}}=\mb\beta\)</span> (the LSE is an <strong>unbiased</strong> estimator of <span class="math inline">\(\mb\beta\)</span>)</p></li>
<li><span class="math inline">\(\var{\hat{\mb\beta}}= (\mb{X}^t\mb{X})^{-1}\sigma^2\)</span>.
</div>
</li>
</ol>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Part 1.</p>
<p>The unbiasedness of <span class="math inline">\(\hat{\mb\beta}\)</span> follows from <span class="math display">\[
   \E{\hat{\mb\beta}} = \E{(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}}=(\mb{X}^t\mb{X})^{-1}\mb{X}^t\E{\mb{Y}}
   =(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{X}\mb\beta=\mb\beta.
 \]</span></p>
<p>Part 2.</p>
<p>For the covariance matrix of <span class="math inline">\(\hat{\mb\beta}\)</span> we will need <span class="math inline">\(\var{\mb{Y}}\)</span>. On the diagonal of this matrix we find <span class="math inline">\(\var{Y_i}=\var{\eps_i}=\sigma^2\)</span> and on the off-diagonal positions we need the covariances <span class="math inline">\(\cov{Y_i,Y_j}\)</span> (<span class="math inline">\(i\neq j\)</span>). All these covariances are equal to zero because the independence between outcomes is assumed. Hence, the covariance matrix of <span class="math inline">\(\hat{\mb\beta}\)</span> becomes</p>
<span class="math display">\[\begin{eqnarray*}
 \var{\hat{\mb\beta}}
  &amp;=&amp; \var{(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}} \\
  &amp;=&amp; (\mb{X}^t\mb{X})^{-1}\mb{X}^t \var{\mb{Y}} \left[(\mb{X}^t\mb{X})^{-1}\mb{X}^t\right]^t \\
  &amp;=&amp; (\mb{X}^t\mb{X})^{-1}\mb{X}^t \sigma^2 \mb{I}_n \mb{X}(\mb{X}^t\mb{X})^{-1} \\
  &amp;=&amp; (\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{X} (\mb{X}^t\mb{X})^{-1} \sigma^2\\
  &amp;=&amp; (\mb{X}^t\mb{X})^{-1} \sigma^2.
\end{eqnarray*}\]</span>
</div>

We also give the explicit form of <span class="math inline">\(\var{\hat{\mb\beta}}= (\mb{X}^t\mb{X})^{-1} \sigma^2\)</span> (after working out the matrix multiplication and inversion):
<span class="math display" id="eq:SigmaBetaLSE">\[\begin{equation}
   \var{\hat{\mb\beta}} = \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \begin{pmatrix}
   \frac{1}{n}\sum_{i=1}^n x_i^2   &amp; - \bar{x} \\
   -\bar{x}                                      &amp; 1 \end{pmatrix}.
   \tag{2.6}
\end{equation}\]</span>
<p>To give you a good understanding of these two properties, we extend the simulation study of Section <a href="#S:RegSimStudy">2.1</a>. For every repeated experiment, we compute the LSE of the two <span class="math inline">\(\beta\)</span>-parameters. We repeat the experiment <span class="math inline">\(N=\)</span> 100 times and then we compute the mean and the variance of the <span class="math inline">\(N=\)</span> 100 parameter estimates. Recall that we set the (true) parameter values to <span class="math inline">\(\beta_0=90\)</span>, <span class="math inline">\(\beta_1=0.5\)</span> and <span class="math inline">\(\sigma=5\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">75286</span>)
N&lt;-N100 <span class="co"># number of repeated experiments</span>
x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span>
betaHat&lt;-<span class="kw">data.frame</span>(<span class="dt">beta0Hat=</span><span class="ot">NA</span>,<span class="dt">beta1Hat=</span><span class="ot">NA</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">5</span>,<span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span>
    m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
    betaHat[i,]&lt;-<span class="kw">coef</span>(m)
}

<span class="kw">colMeans</span>(betaHat)</code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 88.8751318  0.5051252</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(betaHat)</code></pre></div>
<pre><code>##          beta0Hat    beta1Hat
## beta0Hat 2209.750 -12.6339951
## beta1Hat  -12.634   0.0723973</code></pre>
<p>The averages of the <span class="math inline">\(N=\)</span> 100 estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are indeed close to the true values. If <span class="math inline">\(N\)</span> were larger, we expect the estimates to be even closer to the true values.</p>
<p>Theorem <a href="#thm:LSEMeanVar">2.2</a> tells us that the variance of <span class="math inline">\(\hat{\mb\beta}\)</span> equals <span class="math inline">\((\mb{X}^t\mb{X})^{-1}\sigma^2\)</span>. The design matrix <span class="math inline">\(\mb{X}\)</span> for our simulation experiment is constructed in the following chunck of R code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X&lt;-<span class="kw">matrix</span>(<span class="dt">nrow=</span><span class="dv">5</span>,<span class="dt">ncol=</span><span class="dv">2</span>)
X[,<span class="dv">1</span>]=<span class="dv">1</span>
X[,<span class="dv">2</span>]=x
X</code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]    1  165
## [2,]    1  170
## [3,]    1  175
## [4,]    1  180
## [5,]    1  185</code></pre>
<p>With this matrix and with <span class="math inline">\(\sigma^2=25\)</span> we find the covariance matrix of <span class="math inline">\(\hat{\mb\beta}\)</span>. See the next chunck of R code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X)<span class="op">*</span><span class="dv">25</span></code></pre></div>
<pre><code>##        [,1]  [,2]
## [1,] 3067.5 -17.5
## [2,]  -17.5   0.1</code></pre>
<p>The results of our simulation study give only a rough approximation to this true covariance matrix. A better approximation is obtained with a larger number of repeated experiments (<span class="math inline">\(N\)</span>). We illustrate this with <span class="math inline">\(N=\)</span> 10^{4}.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">6247467</span>)
N&lt;-N10000 <span class="co"># number of repeated experiments</span>
x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span>
betaHat5&lt;-<span class="kw">data.frame</span>(<span class="dt">beta0Hat=</span><span class="ot">NA</span>,<span class="dt">beta1Hat=</span><span class="ot">NA</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">5</span>,<span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span>
    m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
    betaHat5[i,]&lt;-<span class="kw">coef</span>(m)
}

<span class="kw">colMeans</span>(betaHat)</code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 88.8751318  0.5051252</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">EmpVar5&lt;-<span class="kw">var</span>(betaHat5)
Var5&lt;-<span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X)<span class="op">*</span><span class="dv">25</span>
EmpVar5</code></pre></div>
<pre><code>##            beta0Hat    beta1Hat
## beta0Hat 3080.48493 -17.5710114
## beta1Hat  -17.57101   0.1003883</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Var5</code></pre></div>
<pre><code>##        [,1]  [,2]
## [1,] 3067.5 -17.5
## [2,]  -17.5   0.1</code></pre>
<p>To get a better understanding of the concept of <span class="math inline">\(\var{\hat{\mb\beta}}\)</span>, we repeat the simulation study but with more observations for each repeated experiment. Before this was <span class="math inline">\(n=5\)</span>. We increase this to <span class="math inline">\(n=50\)</span>. We keep the 5 father's heights, but for each height we have now 10 observations (as if we have 10 fathers of the same height, and each of these fathers has a son).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">6247467</span>)
N&lt;-N10000 <span class="co"># number of repeated experiments</span>
x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span>
x&lt;-<span class="kw">rep</span>(x,<span class="dv">10</span>) <span class="co"># the five father&#39;s heights are replicated 10 times</span>
betaHat50&lt;-<span class="kw">data.frame</span>(<span class="dt">beta0Hat=</span><span class="ot">NA</span>,<span class="dt">beta1Hat=</span><span class="ot">NA</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">50</span>,<span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span>
    m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
    betaHat50[i,]&lt;-<span class="kw">coef</span>(m)
}

<span class="kw">colMeans</span>(betaHat50)</code></pre></div>
<pre><code>##  beta0Hat  beta1Hat 
## 90.178681  0.498924</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X&lt;-<span class="kw">matrix</span>(<span class="dt">nrow=</span><span class="dv">50</span>,<span class="dt">ncol=</span><span class="dv">2</span>)
X[,<span class="dv">1</span>]=<span class="dv">1</span>
X[,<span class="dv">2</span>]=x

EmpVar50&lt;-<span class="kw">var</span>(betaHat50)
Var50&lt;-<span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X)<span class="op">*</span><span class="dv">25</span>
EmpVar50</code></pre></div>
<pre><code>##            beta0Hat    beta1Hat
## beta0Hat 311.718416 -1.77841831
## beta1Hat  -1.778418  0.01016245</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Var50</code></pre></div>
<pre><code>##        [,1]  [,2]
## [1,] 306.75 -1.75
## [2,]  -1.75  0.01</code></pre>
<p>Again we see that the means and variances of the simulated estimates agree with the true means and variances as we found from theory. The results also show that the variances of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are smaller for <span class="math inline">\(n=50\)</span> than for <span class="math inline">\(n=5\)</span>. The former is a factor 10 smaller than the latter. This factor agrees with the factor with which we increased the sample size (from <span class="math inline">\(n=5\)</span> to <span class="math inline">\(n=50\)</span>). (Try to prove this yourself.)</p>
<p>The difference between the <span class="math inline">\(n=5\)</span> and the <span class="math inline">\(n=50\)</span> scenario is demonstrated in Figure <a href="#fig:SimReg3">2.7</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:SimReg3"></span>
<img src="DASM2_files/figure-html/SimReg3-1.png" alt="histograms of $N=10000$ repeated estimates of  $\beta_1$ for $n=5$ and $n=50$. These form approximations for the sampling distributions of the estimators." width="672" />
<p class="caption">
Figure 2.7: histograms of <span class="math inline">\(N=10000\)</span> repeated estimates of <span class="math inline">\(\beta_1\)</span> for <span class="math inline">\(n=5\)</span> and <span class="math inline">\(n=50\)</span>. These form approximations for the sampling distributions of the estimators.
</p>
</div>
<p>The figure shows the histograms of the <span class="math inline">\(N=\)</span> 10^{4} estimates of <span class="math inline">\(\beta_1\)</span> for sample sizes of <span class="math inline">\(n=5\)</span> and <span class="math inline">\(n=50\)</span>. This illustrates once more that the estimator <span class="math inline">\(\hat{\mb\beta}_1\)</span> is a stochastic (or random) variable. This is easy to understand: <span class="math inline">\(\hat\beta_1= \frac{\sum_{i=1}^n (Y_i-\bar{Y})(x_i-\bar{x})}{\sum_{i=1}^n (x_i-\bar{x})^2}\)</span> is a (linear) function of <span class="math inline">\(Y_1,\ldots, Y_1\)</span>, which are randomly sampled outcomes. Hence, <span class="math inline">\(\hat\beta_1\)</span> is also a random variable and it can this be described by a distribution. This distribution is referred to as the <strong>sampling distribution</strong>. It will be discussed in some more detail later in this course. In this section we only looked at the mean and the variance of the estimator.</p>
<p>Figure <a href="#fig:SimReg3">2.7</a> demonstrates that the variance of the sampling distribution of <span class="math inline">\(\hat\beta_1\)</span> descreases with increasing sample size <span class="math inline">\(n\)</span>. The variance, which is defined as <span class="math display">\[
  \var{\hat\beta_1} = \E{(\hat\beta_1-\E{\hat\beta_1})^2},
\]</span> quantifies how the estimates, over the repeated experiments, vary about the true parameter value <span class="math inline">\(\beta_1\)</span> (note that <span class="math inline">\(\beta_1=\E{\hat\beta_1}\)</span>; unbiased estimator). Thus, the smaller the variance, the more frequent (over repeated experiments) the estimate <span class="math inline">\(\hat\beta_1\)</span> is close to the true value <span class="math inline">\(\beta_1\)</span>. In other words (cfr. definition of variance): for a large sample size <span class="math inline">\(n\)</span> we expect the estimates <span class="math inline">\(\hat\beta_1\)</span> an average closer to the true value <span class="math inline">\(\beta_1\)</span> than for a small sample size <span class="math inline">\(n\)</span>.</p>
<p>If the goal of the study is to estimate the parameter with a great <strong>precision</strong>, then the variance of the estimator must be small. We therefore say that the variance of an estimator is to be considered as a measure of the <strong>imprecision</strong> of the estimator.</p>
</div>
</div>
<div id="exercise-simulation-study" class="section level2 unnumbered">
<h2>Exercise: simulation study</h2>
<p>Repeat the previous simulation study, but now with other values of the regressor and other numbers of replicates for each regressor value. In particular, consider the setting with only two different values of the regressor (<span class="math inline">\(x=165\)</span> and <span class="math inline">\(x=185\)</span>) and at each of these values, consider 25 replicates. This makes a total of <span class="math inline">\(n=50\)</span> observations, just like in the simulation study.</p>
<p>Check whether the LSEs of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are unbiased and compute the empirical variance of the estimators from the simulation study. How do these variances compare to the variances from the previous simulation study? Can you give an explanation?</p>
<p><details> <summary markdown="span">Try to first make the exercise yourself. You can expand this page to see a solution. </summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">6247467</span>)
N&lt;-N10000 <span class="co"># number of repeated experiments</span>
x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">185</span>) <span class="co"># two regressor values</span>
x&lt;-<span class="kw">rep</span>(x,<span class="dv">25</span>) <span class="co"># the two regressor values are replicated 25 times</span>
betaHat50&lt;-<span class="kw">data.frame</span>(<span class="dt">beta0Hat=</span><span class="ot">NA</span>,<span class="dt">beta1Hat=</span><span class="ot">NA</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">50</span>,<span class="dt">sd=</span><span class="dv">5</span>) 
    m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
    betaHat50[i,]&lt;-<span class="kw">coef</span>(m)
}

<span class="kw">colMeans</span>(betaHat50)</code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 89.8835261  0.5006106</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(betaHat50)</code></pre></div>
<pre><code>##             beta0Hat     beta1Hat
## beta0Hat 152.1525837 -0.865844097
## beta1Hat  -0.8658441  0.004943341</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(betaHat50<span class="op">$</span>beta1Hat,<span class="dt">main=</span><span class="st">&quot;n=50&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;beta1-hat&quot;</span>,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">abline</span>(<span class="dt">v=</span><span class="fl">0.5</span>,<span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>The simulation study demonstrates once more that the LSE of the estimators are unbiased. When looking at the empirical variances of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> we see that they are now equal to 152.1525837 and 0.0049433, respecitvely. These are smaller than the variances we obtained from the previous simulation study in which we considered 5 equally spaced values of the regressor. The minimum and maximum values of the regressor are still 165 and 185 and the total sample size is also still equal to <span class="math inline">\(n=50\)</span>. The differences in the variances can be explained by expression <a href="#eq:SigmaBetaLSE">(2.6)</a> for <span class="math inline">\(\var{\hat{\mb\beta}}\)</span>.</p>
<p>Let us first look at <span class="math inline">\(\var{\hat\beta_1}\)</span>, which is given by <span class="math display">\[
  \frac{\sigma^2}{\sum_{i=1}^n (x_i-\bar{x})^2}.
\]</span> Recall that <span class="math inline">\(n=50\)</span> and <span class="math inline">\(\sigma^2=25\)</span> are the same here as in the previous simulation study. Also <span class="math inline">\(\bar{x}=175\)</span> is the same as before (less relevant). It is easy to confirm that the denominator <span class="math inline">\(\sum_{i=1}^n (x_i-\bar{x})^2\)</span> is larger here (5000) than for the previous simulation study (2500). For <span class="math inline">\(\var{\hat\beta_0}\)</span> you may also check that the variance for the setting in this exercise is smaller than for the previous setting.</p>
<p>More generally it can be shown that for a given <span class="math inline">\(n\)</span>, a given <span class="math inline">\(\sigma^2\)</span> and a given interval for the regressor, i.e. <span class="math inline">\(x_i \in [x_\text{min}, x_\text{max}]\)</span>, the variances of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are the smallest when half of the regressors are set to <span class="math inline">\(x_\text{min}\)</span> and the other half is set to <span class="math inline">\(x_\text{max}\)</span>.</p>
<p></details></p>
<div id="best-linear-unbiased-estimator-blue" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Best Linear Unbiased Estimator (BLUE)</h3>
<p>In the two previous sections we have investigated the variance of the LSE. We have stressed that the variance is a measure for the imprecission of an estimator. So we like to have an estimator that has a small imprecission or variance. In this section we shall demonstrate that within a certain class of estimators, and for regression model <a href="#eq:Mod3">(2.2)</a>, the LSE has the smallest variance and hence from this perspective it is the best estimator.</p>
<p>Let's describe the class of estimators of <span class="math inline">\(\mb\beta\)</span> that can be written as <span class="math inline">\(\mb{AY}\)</span>, with <span class="math inline">\(\mb{A}\)</span> a <span class="math inline">\(p\times n\)</span> matrix that may depend on <span class="math inline">\(\mb{X}\)</span>, but that may not depend on stochastic variables (such as e.g. <span class="math inline">\(\mb{Y}\)</span>). We write <span class="math inline">\(\hat{\mb\beta}^*=\mb{AY}\)</span>. The class of <strong>linear unbiased estimators</strong> is then given by estimators of the form <span class="math inline">\(\hat{\mb\beta}^*=\mb{AY}\)</span>, for which it holds that <span class="math inline">\(\E{\hat{\mb\beta}^*}=\mb\beta\)</span> (i.e. the estimator is unbiased for <span class="math inline">\(\mb\beta\)</span>).</p>
<p>The LSE <span class="math inline">\(\hat{\mb{\beta}}\)</span> is an example of a linear unbiased estimator of <span class="math inline">\(\mb\beta\)</span>. It has the form <span class="math inline">\(\mb{AY}\)</span> with <span class="math inline">\(\mb{A}=(\mb{X}^t\mb{X})^{-1}\mb{X}^t\)</span>.</p>
<p>As an example, consider the LSE of <span class="math inline">\(\beta_1\)</span>, which can be written as <span class="math display">\[
  \hat\beta_1= \frac{\sum_{i=1}^n (Y_i-\bar{Y})(x_i-\bar{x})}{\sum_{i=1}^n (x_i-\bar{x})^2}.
\]</span> Also here you see that the estimator is a linear combination of the <span class="math inline">\(n\)</span> outcomes <span class="math inline">\(Y_i\)</span>. We consider now an alternative linear estimator: <span class="math display">\[
  \hat\beta_1^*= \frac{\sum_{i=1}^n w_i(Y_i-\bar{Y})(x_i-\bar{x})}{\sum_{i=1}^n (x_i-\bar{x})^2},
\]</span> where <span class="math inline">\(w_1,\ldots, w_n\)</span> are non-negative constants for which holds th at <span class="math inline">\(\sum_{i=1}^n w_i=1\)</span> (i.e. <span class="math inline">\(w_i\)</span> are weigths). For <span class="math inline">\(\hat\beta_1^*\)</span> it still holds that it is an unbiased estimator. The variance, however, is different.</p>
<p>Within the class of linear unbiased estimators, the estimator with the smallest variancs is the <strong>best linear unbiased estimator</strong> (BLUE). The next theorem is still more general. </p>

<div class="theorem">
<span id="thm:BLUE" class="theorem"><strong>Theorem 2.3  (The LSE is the BLUE)  </strong></span>Assume that model <a href="#eq:Mod3">(2.2)</a> is correct, and consider the LSE <span class="math inline">\(\hat{\mb\beta}\)</span>. Then it holds for all <span class="math inline">\(\hat{\mb\beta}^*=\mb{AY}\)</span> for which <span class="math inline">\(\E{\hat{\mb\beta}^*}=\mb\beta\)</span>, that <span class="math display">\[
   \var{\mb{c}^t\hat{\mb\beta}} \leq \var{\mb{c}^t\hat{\mb\beta}^*} \;\;\;\text{ voor alle } \mb{c}\in\mathbb{R}^p.
\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> The estimator <span class="math inline">\(\hat{\mb\beta}^*\)</span> can always be written as <span class="math display">\[
   \hat{\mb\beta}^* = \left[(\mb{X}^t\mb{X})^{-1}\mb{X}^t+\mb{D}\right]\mb{Y}
\]</span> with <span class="math inline">\(\mb{D}=\mb{A}-(\mb{X}^t\mb{X})^{-1}\mb{X}^t\)</span>. Thus, <span class="math display">\[
  \mb{c}^t\hat{\mb\beta}^* = \mb{c}^t\left[(\mb{X}^t\mb{X})^{-1}\mb{X}^t+\mb{D}\right]\mb{Y} = \mb{c}^t \mb{\hat\beta} + \mb{c}^t\mb{DY}.
\]</span></p>
<p>Next we find an expression for the variance of <span class="math inline">\(\mb{c}^t\hat{\mb\beta}^*\)</span>: <span class="math display">\[
  \var{\mb{c}^t\hat{\mb\beta}^*} = \var{\mb{c}^t\hat{\mb\beta}} + \var{\mb{c}^t\mb{DY}} + 2\cov{\mb{c}^t\hat{\mb\beta},\mb{c}^t\mb{DY}}.
\]</span> The covariance in this expression becomes <span class="math display">\[
  \cov{\mb{c}^t\hat{\mb\beta},\mb{c}^t\mb{DY}} = \mb{c}^t(\mb{X}^t\mb{X})^{-1}\mb{X}^t\var{\mb{Y}}\mb{D}^t = \mb{c}^t(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{D}^t\sigma^2.
\]</span></p>
<p>Since <span class="math inline">\(\hat{\mb\beta}^*\)</span> is an unbiased estimator, i.e. <span class="math inline">\(\E{\hat{\mb\beta}^*}=\mb\beta\)</span>, we find for all <span class="math inline">\(\mb\beta\)</span></p>
<span class="math display">\[\begin{eqnarray*}
  \E{\mb{DY}}
  &amp;=&amp; \mb{0} \\
  \E{\mb{D}(\mb{X\beta}+\mb{\eps})}
  &amp;=&amp; \mb{0} \\
  \mb{DX\beta}+\mb{D}\E{\mb{\eps}}
  &amp;=&amp; \mb{0} \\
  \mb{DX\beta}
  &amp;=&amp;\mb{0}.
\end{eqnarray*}\]</span>
<p>Since in general <span class="math inline">\(\mb\beta\)</span> is not equal to zero, the unbiasedness of <span class="math inline">\(\hat{\mb{\beta}}^*\)</span> implies that <span class="math inline">\(\mb{DX}=\mb{0}\)</span>. With this identity, we find that the covariance <span class="math inline">\(\cov{\mb{c}^t\hat{\mb\beta},\mb{c}^t\mb{DY}}\)</span> is identical to zero.</p>
<p>The variance of <span class="math inline">\(\mb{c}^t\hat{\mb\beta}^*\)</span> thus reduces to</p>
<p><span class="math display">\[
  \var{\mb{c}^t\hat{\mb\beta}^*} = \var{\mb{c}^t\hat{\mb\beta}} + \var{\mb{c}^t\mb{DY}}.
\]</span></p>
Finaly, since variances cannot be negative, we find, for all <span class="math inline">\(\mb{c}\in\mathbb{R}^p\)</span>, <span class="math display">\[
  \var{\mb{c}^t\hat{\mb\beta}^*} \leq \var{\mb{c}^t\hat{\mb\beta}} .
\]</span>
</div>

</div>
</div>
<div id="exercise-simulation-study-1" class="section level2 unnumbered">
<h2>Exercise: Simulation study</h2>
<p>We refer here to the simulation study in <a href="#S:PropLSE">2.3</a>. Repeat this simulation study with 10000 Monte Carlo simulation runs, but now with another estimator of <span class="math inline">\(\mb\beta\)</span>. With our conventional notation, define the following esimator, <span class="math display">\[
  \tilde{\mb\beta} = (\mb{X}^t\mb{X}+d\mb{I}_2)^{-1}\mb{X}^t\mb{Y},
\]</span> with <span class="math inline">\(d&gt;0\)</span> and <span class="math inline">\(\mb{I}_2\)</span> the <span class="math inline">\(2\times 2\)</span> identity matrix. In the simulation study you may set <span class="math inline">\(d=0.005\)</span>. Compare the LSE of <span class="math inline">\(\mb\beta\)</span> with this new estimator in terms of bias and variance. What do you conclude? How does this relate to the BLUE property of the LSE? \ Also compute the <em>Mean Squared Error</em> (MSE) of the estimators, defined here as <span class="math display">\[
  \E{(\hat\beta_j-\beta_j)^2} \;\;\text{ and }\;\; \E{(\tilde\beta_j-\beta_j)^2},
\]</span> for <span class="math inline">\(j=0,1\)</span>. Recall that the expectation can be approximated by the average over many Monte Carlo simulation runs. \ What do conclude from these Mean Squared Errors?</p>
<p><details> <summary markdown="span">Try to solve this problem and then you can expend this page to look at a solution. </summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">6247467</span>)
N&lt;-N10000 <span class="co"># number of repeated experiments</span>
x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span>
X&lt;-<span class="kw">cbind</span>(<span class="dv">1</span>,x) <span class="co"># design matrix</span>
betaHatLSE&lt;-<span class="kw">data.frame</span>(<span class="dt">beta0Hat=</span><span class="ot">NA</span>,<span class="dt">beta1Hat=</span><span class="ot">NA</span>)
betaHatNew&lt;-<span class="kw">data.frame</span>(<span class="dt">beta0Hat=</span><span class="ot">NA</span>,<span class="dt">beta1Hat=</span><span class="ot">NA</span>)
MSELSE&lt;-<span class="kw">c</span>()
MSENew&lt;-<span class="kw">c</span>()
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">5</span>,<span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span>
    m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
    betaHatLSE[i,]&lt;-<span class="kw">coef</span>(m)
    MSELSE&lt;-<span class="kw">c</span>(MSELSE,<span class="kw">mean</span>((<span class="kw">coef</span>(m)<span class="op">-</span><span class="kw">c</span>(<span class="dv">90</span>,<span class="fl">0.5</span>))<span class="op">^</span><span class="dv">2</span>))
    betaTilde&lt;-<span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X<span class="op">+</span><span class="fl">0.005</span><span class="op">*</span><span class="kw">diag</span>(<span class="dv">2</span>))<span class="op">%*%</span><span class="kw">t</span>(X)<span class="op">%*%</span>y
    betaHatNew[i,]&lt;-betaTilde
    MSENew&lt;-<span class="kw">c</span>(MSENew,<span class="kw">mean</span>((betaTilde<span class="op">-</span><span class="kw">c</span>(<span class="dv">90</span>,<span class="fl">0.5</span>))<span class="op">^</span><span class="dv">2</span>))
}

<span class="kw">colMeans</span>(betaHatLSE)</code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 90.5853872  0.4966336</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">colMeans</span>(betaHatNew)</code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 56.1436710  0.6931226</code></pre>
<p>For these averages of the estimates over the Monte Carlo runs, we conclude that the LSEs are unbiased (as we already knew), but the new estimator is biased!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">diag</span>(<span class="kw">var</span>(betaHatLSE))</code></pre></div>
<pre><code>##     beta0Hat     beta1Hat 
## 3080.4849279    0.1003883</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">diag</span>(<span class="kw">var</span>(betaHatNew))</code></pre></div>
<pre><code>##     beta0Hat     beta1Hat 
## 1.183233e+03 3.865236e-02</code></pre>
<p>The new, but biased estimator, seems to give smaller variances than the LSE. \ Didn't we learn that the LSE has the smallest bias (BLUE property)? No, the BLUE property says that the LSE has the smallest variance among all <strong>unbiased</strong> estimators. Thus our new estimator, which is biased, does not belong to the class of unbiased estimators and hence it can theoretically have a smaller variance.</p>
<p>Let's now look at the Mean Squared errors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(MSELSE)</code></pre></div>
<pre><code>## [1] 1540.31</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(MSENew)</code></pre></div>
<pre><code>## [1] 1164.721</code></pre>
<p>The MSE of the LSE is larger than the MSE of the new estimator. This means that on average, over many repeated experiments, the new estimator is closer to the true value of the parameter! This is also a very desirable property!</p>
<p>The new estimator that was introduces here, is known as the <strong>ridge estimator</strong>. It will turn out to be a useful estimator in high dimensional prediction problems. </details></p>
<div id="sampling-distribution-of-the-lse" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Sampling distribution of the LSE</h3>

<p>Since <span class="math inline">\(\hat{\mb\beta}\)</span> is a function of the outcome vector <span class="math inline">\(\mb{Y}\)</span> and since the outcome vector is a random variable, the estimtor <span class="math inline">\(\hat{\mb\beta}\)</span> is also a random variable. Its distribution (<strong>sampling distribution</strong>) is determined by the distribution of <span class="math inline">\(\mb{Y}\)</span>. In model <a href="#eq:Mod3">(2.2)</a> we see that <span class="math inline">\(\mb{Y} = \mb{X}\mb\beta + \mb{\eps}\)</span>, but the distribution of <span class="math inline">\(\mb{\eps}\)</span> is not fully specified (only the mean is restricted to zero). This prohibits finding the sampling distribution of <span class="math inline">\(\hat{\mb\beta}\)</span> moeilijk, unless asymptotically (see further).</p>
We shall introduce an extra distribution assumption in the statistical model, and this will allow for finding the sampling distribution. Model <a href="#eq:Mod3">(2.2)</a> is extended to (in matrix notation)
<span class="math display" id="eq:Mod4">\[\begin{equation}
  \mb{Y} = \mb{X}\mb\beta + \mb{\eps}
  \tag{2.7}
\end{equation}\]</span>
<p>with <span class="math inline">\(\mb{X}\)</span> an <span class="math inline">\(n\times p\)</span> (<span class="math inline">\(p\leq n\)</span>) matrix of rank <span class="math inline">\(p\)</span>, <span class="math inline">\(\mb{\eps}^t=(\eps_1,\ldots, \eps_n)\)</span> and <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>. In this model, the error terms are assumed to be normally distributed. The model will be referred to as the <strong>normal linear regression model</strong>. For this model, the next theorem gives the sampling distribution of the LSE. </p>

<div class="theorem">
<span id="thm:DistrMod4" class="theorem"><strong>Theorem 2.4  (Sampling distribution of the LSE in the normal linear regression model)  </strong></span>Assume that model <a href="#eq:Mod4">(2.7)</a> holds. This it holds that <span class="math display">\[
   \hat{\mb\beta} \sim \text{MVN}(\mb\beta,(\mb{X}^t\mb{X})^{-1}\sigma^2). 
\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Write <span class="math inline">\(\hat{\mb\beta} = (\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}\)</span> as <span class="math inline">\(\hat{\mb\beta} =\mb{CY}\)</span>. Matrix algebra tells us that <span class="math display">\[
   \text{rang}(\mb{C})=\text{rang}(\mb{X}^t)=\text{rang}(\mb{X})=p. 
\]</span> The estimator <span class="math inline">\(\hat{\mb\beta}=\mb{CY}\)</span> is thus a vector of linear combinations of the elements in <span class="math inline">\(\mb{Y}\)</span>, which are jointly multivariate normally distributed (see Lemma <a href="#lem:LinTransNorm">B.1</a> in Appendix <a href="#app:LinTrans">B</a>. The mean and the variance of <span class="math inline">\(\hat{\mb\beta}\)</span> were already given in Theorem <a href="#thm:LSEMeanVar">2.2</a>.</p>
Hence, <span class="math display">\[
   \hat{\mb\beta} \sim \text{MVN}(\mb\beta, (\mb{X}^t\mb{X})^{-1}\sigma^2).
\]</span>
</div>

<p>We now repeat the simulation study and this time we will use normal QQ-plots to check whether the sampling distribution is indeed a normal distribution. The results are shown in Figure <a href="#fig:SimReg4">2.8</a>. The QQ-plots clearly show that the sampling distribution of <span class="math inline">\(\hat\beta_1\)</span> is normal.</p>
<div class="figure" style="text-align: center"><span id="fig:SimReg4"></span>
<img src="DASM2_files/figure-html/SimReg4-1.png" alt="Normal QQ-plots of the $N=10000$ repeated estimates of $\beta_1$ for $n=5$ en $n=50$" width="672" />
<p class="caption">
Figure 2.8: Normal QQ-plots of the <span class="math inline">\(N=10000\)</span> repeated estimates of <span class="math inline">\(\beta_1\)</span> for <span class="math inline">\(n=5\)</span> en <span class="math inline">\(n=50\)</span>
</p>
</div>
<p>If the normality assumption of model <a href="#eq:Mod4">(2.7)</a> is violated, but the assumptions of model <a href="#eq:Mod3">(2.2)</a> do hold, then we can still find the sampling distribution of <span class="math inline">\(\hat{\mb\beta}\)</span>, but only for large sample sizes. Without proof, the result is stated in the following theorem. It is an <strong>asymptotic result</strong>, which means that it holds in the limit for sample sizes <span class="math inline">\(n\)</span> going to infinity. Fortunately, such asymptotic results often hold approximately for large, but finite sample sizes. </p>

<div class="theorem">
<span id="thm:DistrMod3" class="theorem"><strong>Theorem 2.5  (Asymptotic sampling distribution of the LSE)  </strong></span>Assume that (1) model <a href="#eq:Mod3">(2.2)</a> holds, (2) the regressor values are fixed by design and (3) that <span class="math inline">\(\lim_{n\rightarrow \infty} \frac{1}{n}\sum_{i=1}^n\mb{x}_i\mb{x}_i^t\)</span> has rank equal to 2. Then, as <span class="math inline">\(n\rightarrow \infty\)</span>, <span class="math display">\[
 \sqrt{n}(\hat{\mb\beta}-\mb\beta)\mb\Sigma_n^{-1} \convDistr \text{MVN}(\mb{0},\mb{I}_2)  
\]</span> with
<span class="math display" id="eq:SigmaBetaLSEAsymp">\[\begin{equation}
   \mb\Sigma_n = \frac{\sigma^2}{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2} \begin{pmatrix}
   \frac{1}{n}\sum_{i=1}^n x_i^2   &amp; - \bar{x} \\
   -\bar{x}                                      &amp; 1 \end{pmatrix}.
   \tag{2.8}
\end{equation}\]</span>
</div>

<p>Some notes regarding this theorem:</p>
<ul>
<li><p>the construction <span class="math inline">\(n\rightarrow \infty T_n \convDistr T\)</span> tells us that the distribution of the stochastic variance <span class="math inline">\(T_n\)</span>, which is based on a sample size of <span class="math inline">\(n\)</span>, converges to the distribution of the random variable <span class="math inline">\(T\)</span>, when the sample size goes to infinity. In the theorem, <span class="math inline">\(T_n\)</span> is the LSE based on a sample of size <span class="math inline">\(n\)</span>, and <span class="math inline">\(T\)</span> is a MVN random variable with mean <span class="math inline">\(\mb{0}\)</span> and covariance matrix <span class="math inline">\(\mb{I}_2\)</span> (<span class="math inline">\(2\times 2\)</span> identity matrix).</p></li>
<li><p>the distribution of <span class="math inline">\(T\)</span> (or the MVN distribution in the theorem) is referred to as the <strong>asymptotic sampling distribution</strong> of <span class="math inline">\(T_n\)</span>.</p></li>
<li><p>in contrast to the <em>asymptotic</em> sampling distribution, we use the term <strong>exact sampling distribution</strong> to refer to a sampling distribution that is correct even for small sample sizes <span class="math inline">\(n\)</span>. Such exact sampling distributions often require strong distributional assumptions.</p></li>
<li><p>the second condition in the theorem says that the regressor values must be <strong>fixed by design</strong>. This means that the regressor may not be a random variable (as it is in the Galton example). Here is an example of a <strong>fixed design</strong>: Randomly sample 10 subjects of each of the following ages: 20, 40, 60 and 70 years old (note: these ages are fixed prior to the executation of the study). For each of the <span class="math inline">\(10\times 4=40\)</span> subjects, measure the blood pressure. In this example, the blood pressure is the outcome (random variable) and the age is the regressor. However, since the ages were fixed by design, this is an example of a fixed design.</p></li>
<li><p>although the theorem only gives the asymptotic sampling distribution for fixed designs, a similar theorem exists for <strong>random designs</strong>. The second condition need to be reformulated such that it makes sense for random regressors (details not given here). The sampling distribution is the same as for fixed designs.</p></li>
</ul>
<p>We now demonstrate the practical meaning of the theorem in a simulation study. We repeat the same simulations as before, but now with error terms <span class="math inline">\(\eps_i\)</span> that are not normally distributed. We choose <span class="math inline">\(\eps_i\)</span> to be distributed as an <em>exponential distribution</em>. Figure <a href="#fig:ExpDistr">2.9</a> shows the shape of an exponential distribution with variance 1 en centered such that the mean is equal to zero (this is a requirement of error terms in our linear regression models).</p>
<div class="figure" style="text-align: center"><span id="fig:ExpDistr"></span>
<img src="DASM2_files/figure-html/ExpDistr-1.png" alt="Histogram of 10000 error terms from an exponential distribution (centered to make the mean equal to zero)" width="672" />
<p class="caption">
Figure 2.9: Histogram of 10000 error terms from an exponential distribution (centered to make the mean equal to zero)
</p>
</div>
<p>The next chunck or R code gives a simulation study in which we simulate <span class="math inline">\(n=5\)</span>, <span class="math inline">\(n=50\)</span> and <span class="math inline">\(n=200\)</span> outcomes according to Model <a href="#eq:Mod3">(2.2)</a> with a centered exponential distribution for the error term. The results show that the LSE is still unbiased (this property does not require the normality assumption). Figure <a href="#fig:SimReg5">2.10</a> shows three normal QQ-plots of the estimates <span class="math inline">\(\hat\beta_1\)</span> for the three sample sizes. For <span class="math inline">\(n=5\)</span> the sampling distribution of <span class="math inline">\(\hat\beta_1\)</span> is clearly not normal, but as the sample size <span class="math inline">\(n\)</span> increases, the approxiation to a normal distribution becomes better.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">6247467</span>)

<span class="co"># definieren een functie voor het simuleren</span>

simRegressionExp&lt;-<span class="cf">function</span>(<span class="dt">N=</span><span class="dv">1000</span>,<span class="dt">nRep=</span><span class="dv">1</span>) {
  <span class="co"># N: number of repeated samples</span>
  <span class="co"># nRep: number of replicated for each value of the regressor</span>
  
  x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># 5 father&#39;s heights</span>
  x&lt;-<span class="kw">rep</span>(x,nRep) <span class="co"># the five regrossor values are replaciated nRep times</span>
  x
  betaHat&lt;-<span class="kw">data.frame</span>(<span class="dt">beta0Hat=</span><span class="ot">NA</span>,<span class="dt">beta1Hat=</span><span class="ot">NA</span>)
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
      y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span>(<span class="kw">rexp</span>(<span class="dv">5</span><span class="op">*</span>nRep)<span class="op">-</span><span class="dv">1</span>) <span class="co"># random sample of outcomes</span>
      m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
      betaHat[i,]&lt;-<span class="kw">coef</span>(m)
  }

  <span class="kw">return</span>(betaHat)
}
  
<span class="co"># for n=5</span>
betaHat5&lt;-<span class="kw">simRegressionExp</span>(<span class="dt">N=</span>N1000,<span class="dt">nRep=</span><span class="dv">1</span>)
<span class="kw">colMeans</span>(betaHat)</code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 88.8751318  0.5051252</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X&lt;-<span class="kw">matrix</span>(<span class="dt">nrow=</span><span class="dv">5</span>,<span class="dt">ncol=</span><span class="dv">2</span>)
X[,<span class="dv">1</span>]&lt;-<span class="dv">1</span>
X[,<span class="dv">2</span>]&lt;-<span class="kw">seq</span>(<span class="dv">150</span>,<span class="dv">250</span>,<span class="dv">25</span>)

EmpVar&lt;-<span class="kw">var</span>(betaHat)
Var&lt;-<span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X)<span class="op">*</span><span class="dv">1</span> 
EmpVar</code></pre></div>
<pre><code>##          beta0Hat    beta1Hat
## beta0Hat 2209.750 -12.6339951
## beta1Hat  -12.634   0.0723973</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Var</code></pre></div>
<pre><code>##        [,1]     [,2]
## [1,]  6.600 -0.03200
## [2,] -0.032  0.00016</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># for n=50</span>
betaHat50&lt;-<span class="kw">simRegressionExp</span>(<span class="dt">N=</span>N1000,<span class="dt">nRep=</span><span class="dv">10</span>)
<span class="kw">colMeans</span>(betaHat50)</code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 90.0568710  0.4997006</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(betaHat50)</code></pre></div>
<pre><code>##             beta0Hat      beta1Hat
## beta0Hat 11.71799942 -0.0668749613
## beta1Hat -0.06687496  0.0003822612</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># for n=200</span>
betaHat200&lt;-<span class="kw">simRegressionExp</span>(<span class="dt">N=</span>N1000,<span class="dt">nRep=</span><span class="dv">40</span>)
<span class="kw">colMeans</span>(betaHat200)</code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 89.9934692  0.5000403</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(betaHat200)</code></pre></div>
<pre><code>##             beta0Hat     beta1Hat
## beta0Hat  3.08244132 -0.017619947
## beta1Hat -0.01761995  0.000100889</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))
<span class="kw">qqnorm</span>(betaHat5<span class="op">$</span>beta1Hat,<span class="dt">main=</span><span class="st">&quot;n=5&quot;</span>)
<span class="kw">qqline</span>(betaHat5<span class="op">$</span>beta1Hat)
<span class="kw">qqnorm</span>(betaHat50<span class="op">$</span>beta1Hat,<span class="dt">main=</span><span class="st">&quot;n=50&quot;</span>)
<span class="kw">qqline</span>(betaHat50<span class="op">$</span>beta1Hat)
<span class="kw">qqnorm</span>(betaHat200<span class="op">$</span>beta1Hat,<span class="dt">main=</span><span class="st">&quot;n=200&quot;</span>)
<span class="kw">qqline</span>(betaHat200<span class="op">$</span>beta1Hat)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:SimReg5"></span>
<img src="DASM2_files/figure-html/SimReg5-1.png" alt="Histograms of the $N=1000$ herhaalde estimates of $\beta_1$ for $n=5$, $n=50$ and $n=200$, with centered exponentially distributed error terms." width="672" />
<p class="caption">
Figure 2.10: Histograms of the <span class="math inline">\(N=1000\)</span> herhaalde estimates of <span class="math inline">\(\beta_1\)</span> for <span class="math inline">\(n=5\)</span>, <span class="math inline">\(n=50\)</span> and <span class="math inline">\(n=200\)</span>, with centered exponentially distributed error terms.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>

</div>
<div id="maximum-likelihood-estimator-of-hatmbbeta" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Maximum likelihood estimator of <span class="math inline">\(\hat{\mb\beta}\)</span></h3>
<p>The LSE does need distributional assumptions, such normality. The method of least squares is thus a parameter estimation method that can be applied to <strong>semiparametric statistical models</strong> such as Model <a href="#eq:Mod3">(2.2)</a>.</p>
<p>For fully parametric statistical model, such as Model <a href="#eq:Mod4">(2.7)</a>, the method of <strong>maximum likelihood</strong> becomes applicable for parameter estimation. With normally distributed error terms, as in Model <a href="#eq:Mod4">(2.7)</a>, it can be shows that the LSE is equivalent to the <strong>maximum likelihood estimator</strong> (MLE). This is demonstrated in this section.</p>
<p>Model <a href="#eq:Mod4">(2.7)</a> is equivalent to <span class="math display">\[
  Y_i \mid x_i \sim N(\beta_0+\beta_1 x_i,\sigma^2).
\]</span> This normal distribution has density function <span class="math display">\[
  f(y;x,\beta_0,\beta_1,\sigma^2) = (2\pi\sigma^2)^{-1/2} \exp\left[-\frac{1}{2}\frac{(y-\beta_0-\beta_1 x )^2}{\sigma^2} \right].
\]</span></p>
<p>Since it is assumed that all <span class="math inline">\(n\)</span> outcomes are mutually independent, the likelihood function becomes <span class="math display">\[
L(\beta_0,\beta_1,\sigma^2) = \prod_{i=1}^n f(Y_i;x_i,\beta_0,\beta_1,\sigma^2) 
=(2\pi\sigma^2)^{-n/2} \exp\left[-\frac{1}{2} \sum_{i=1}^n\frac{(Y_i-\beta_0-\beta_1 x_i)^2}{\sigma^2} \right].
\]</span> Hence, the log-likelihood function is given by <span class="math display">\[
  l(\beta_0,\beta_1,\sigma^2) =\ln L(\beta_0,\beta_1,\sigma^2)
  = -\frac{n}{2}\ln(2\pi\sigma^2)-\frac{1}{2}\sum_{i=1}^n\frac{(Y_i-\beta_0-\beta_1 x_i)^2}{\sigma^2}.
\]</span> In matrix notation this becomes <span class="math display">\[
  l(\beta_0,\beta_1,\sigma^2) = -\frac{n}{2}\ln(2\pi\sigma^2)-\frac{1}{2\sigma^2} (\mb{Y}-\mb{X}\mb\beta)^t(\mb{Y}-\mb{X}\mb\beta).
\]</span> The MLE of <span class="math inline">\(\mb\beta\)</span> is defined as <span class="math display">\[
  \hat{\mb\beta} = \text{ArgMax}_{\mb\beta \in \mathbb{R}^2} l(\mb\beta,\sigma^2).
\]</span> We therefore need the partial derivative of the log-likelihood w.r.t. <span class="math inline">\(\mb\beta\)</span>, <span class="math display">\[
\frac{\partial}{\partial \mb\beta} l(\mb\beta,\sigma^2) = \frac{1}{\sigma^2}\mb{X}^t(\mb{Y}-\mb{X}\mb\beta).
\]</span> Setting this partial derivative equal to zero, gives exactly the normal equations of the LSE (see Theorem <a href="#thm:LSEReg1">2.1</a>).</p>
The MLE of the parameter <span class="math inline">\(\sigma^2\)</span> is the solution to the equation <span class="math display">\[
  \frac{\partial}{\partial \sigma^2} l(\mb\beta,\sigma^2) = 0.
\]</span> After some algebra, we find the MLE
<span class="math display" id="eq:MLESigma2">\[\begin{equation}
  \hat\sigma^2=\frac{1}{n}\sum_{i=1}^n (Y_i - \hat\beta_0-\hat\beta_1 x_i)^2.
  \tag{2.9}
\end{equation}\]</span>
<p>This estimator, however, is not unbiased (without proof), but it is <strong>asymptotically unbiased</strong>, i.e. <span class="math display">\[
  \lim_{n\rightarrow \infty} \E{\hat\sigma^2} = \sigma^2.
\]</span></p>
<p>In the next section we will develop an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
</div>
<div id="an-estimator-of-sigma2" class="section level2">
<h2><span class="header-section-number">2.4</span> An Estimator of <span class="math inline">\(\sigma^2\)</span></h2>
<p>We now know the sampling distribution of the LSE of <span class="math inline">\(\mb\beta\)</span>, but this distribution depends on the variance of the error term, <span class="math inline">\(\sigma^2\)</span>, and this variance is still unknown. To turn the sampling distribution of <span class="math inline">\(\hat{\mb\beta}\)</span> into an instrument that can be used with real data (e.g. for calculating confidence intervals and performing hypothesis tests), we will also need an estimator of <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The estimator (and its unbiasedness) are given in the following theorem. We give the result without a proof, but note that it does not require the normality assumption. Als note that the estimator is similar to the MLE of Equation <a href="#eq:MLESigma2">(2.9)</a>, except that the MLE has a factor <span class="math inline">\(1/n\)</span> instead of a factor <span class="math inline">\(1/(n-2)\)</span>. The estimator of <span class="math inline">\(\sigma^2\)</span> can be denoted by <span class="math inline">\(\hat\sigma^2\)</span>, but it is also known as MSE, which stands for the <strong>mean squared error</strong>. This terminology will become clear later. </p>

<div class="theorem">
<span id="thm:ExpectationMSE" class="theorem"><strong>Theorem 2.6  (An unbiased estimator of <span class="math inline">\(\sigma^2\)</span>)  </strong></span>Assume that model <a href="#eq:Mod3">(2.2)</a> holds true, and let <span class="math inline">\(\mb{x}_i^t\)</span> denote the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mb{X}\)</span> (<span class="math inline">\(i=1,\ldots, n\)</span>). Then <span class="math display">\[
  \MSE = \frac{\SSE}{n-2} = \frac{\sum_{i=1}^n (Y_i-\mb{x}_i^t\hat{\mb\beta})^2}{n-2} =
    \frac{(\mb{Y}-\mb{X}\hat{\mb\beta})^t (\mb{Y}-\mb{X}\hat{\mb\beta})}{n-2}
\]</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>.
</div>

<p>For further purposes, we will also need the sampling distribution of MSE. Note that for the property of unbiasedness the normality assumption was not required, but it will be for developping the sampling distribution. The next theorem is given without proof. </p>

<div class="theorem">
<span id="thm:DistrMSE" class="theorem"><strong>Theorem 2.7  (Sampling distribution of MSE)  </strong></span>Assume that Model <a href="#eq:Mod4">(2.7)</a> is correct. Then, <span class="math display">\[
 \frac{(n-2) \MSE}{\sigma^2} \sim \chi^2_{n-2}.
\]</span>
</div>

</div>
<div id="exercise-simulation-study-2" class="section level2 unnumbered">
<h2>Exercise: Simulation study</h2>
<p>Set up a simulation study to empirically demonstrate that the MLE of <span class="math inline">\(\sigma^2\)</span> is asymptotically unbiased. So we want you to repeat a simulation study for several choices of the sample size <span class="math inline">\(n\)</span> so as to show that the bias reduces as <span class="math inline">\(n\)</span> increases.</p>
<p><details> <summary markdown="span">Try to solve this problem and then you can expend this page to look at a solution. </summary></p>
<p>Since we have to repeat a simulation study for several choices of the sample size <span class="math inline">\(n\)</span>, I will write an R function to perform the simulation study. This is given in the next chunck of R code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">simulate.regression&lt;-<span class="cf">function</span>(<span class="dt">n=</span><span class="dv">10</span>,<span class="dt">sigma2=</span><span class="dv">1</span>,<span class="dt">beta0=</span><span class="dv">1</span>,<span class="dt">beta1=</span><span class="dv">1</span>,<span class="dt">N=</span><span class="dv">100</span>) {
  <span class="co"># function that simulates data from a regression model. </span>
  <span class="co"># The result of the function contains the averages of the MLE estimates of sigma^2, </span>
  <span class="co">#    as well as of the unbiased estimates MSE</span>
  
  <span class="co"># n: sample size</span>
  <span class="co"># sigma2: variance of the error term</span>
  <span class="co"># beta0 and beta1: regression parameters</span>
  <span class="co"># N: number of Monte Carlo simulations</span>
  
  sigma2.MLE&lt;-<span class="kw">c</span>() <span class="co"># initiation of vector that will contain the MLEs</span>
  sigma2.MSE&lt;-<span class="kw">c</span>() <span class="co"># initiation of vector that will contain the MSEs</span>
  x&lt;-<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">10</span>,<span class="dt">length.out =</span> n) <span class="co"># vector with n equally spaced regressor values between 1 and 10</span>
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    y&lt;-beta0<span class="op">+</span>beta1<span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(n,<span class="dt">sd=</span><span class="kw">sqrt</span>(sigma2)) <span class="co"># simulate outcome data</span>
    m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
    sigma2.MLE&lt;-<span class="kw">c</span>(sigma2.MLE,
                  <span class="kw">mean</span>(<span class="kw">residuals</span>(m)<span class="op">^</span><span class="dv">2</span>))
    sigma2.MSE&lt;-<span class="kw">c</span>(sigma2.MSE,
                  <span class="kw">sum</span>(<span class="kw">residuals</span>(m)<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>(n<span class="op">-</span><span class="dv">2</span>))
  }
  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">MLE=</span><span class="kw">mean</span>(sigma2.MLE),<span class="dt">MSE=</span><span class="kw">mean</span>(sigma2.MSE)))
}</code></pre></div>
<p>Now we will apply the function (i.e. perform the simulation study) for sample sizes ranging from <span class="math inline">\(n=3\)</span> to <span class="math inline">\(n=100\)</span> and plot the results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">91869</span>)
sample.sizes&lt;-<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">20</span>,<span class="dv">50</span>,<span class="dv">75</span>,<span class="dv">100</span>) 
sigma2.MLE&lt;-<span class="kw">c</span>()
sigma2.MSE&lt;-<span class="kw">c</span>()

<span class="cf">for</span>(n <span class="cf">in</span> sample.sizes) {
  s2&lt;-<span class="kw">simulate.regression</span>(<span class="dt">n=</span>n,<span class="dt">N=</span>N1000)
  sigma2.MLE&lt;-<span class="kw">c</span>(sigma2.MLE,s2<span class="op">$</span>MLE)
  sigma2.MSE&lt;-<span class="kw">c</span>(sigma2.MSE,s2<span class="op">$</span>MSE)
}

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(sample.sizes,sigma2.MLE,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">1.2</span>),
     <span class="dt">xlab=</span><span class="st">&quot;sample size&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;MLE estimate of sigma^2&quot;</span>,
     <span class="dt">main=</span><span class="st">&quot;MLE&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">1</span>,<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="dv">2</span>)

<span class="kw">plot</span>(sample.sizes,sigma2.MSE,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">1.2</span>),
     <span class="dt">xlab=</span><span class="st">&quot;sample size&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;MSE estimate of sigma^2&quot;</span>,
     <span class="dt">main=</span><span class="st">&quot;MSE&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">1</span>,<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p>The results illustrate that the MLE is biased for small sample sizes, but the bias disappears for larger sample sizes. The MSE, on the other hand, is unbiased even for very small sample sizes.</p>
<p></details></p>
</div>
<div id="sampling-distributions-of-the-standardised-and-the-studentised-lse" class="section level2">
<h2><span class="header-section-number">2.5</span> Sampling Distributions of the Standardised and the Studentised LSE</h2>

<p>In the previous sections we have developped the sampling distributions of the LSE and of the unbiased estimator of <span class="math inline">\(\sigma^2\)</span>. For the construction of confidence intervals and hypothesis tests we will often work with a transformation of the LSE such that the sampling distribution of the transformed LSE does no longer depend on parameters that need to be estimated.</p>
<p>We introduce the notation <span class="math inline">\(\sigma_{\beta_j}^2=\var{\hat\beta_j}\)</span> for the variance of <span class="math inline">\(\hat\beta_j\)</span> (<span class="math inline">\(j=0,1\)</span>). This is thus the appropriate diagonal element of <span class="math inline">\(\var{\hat{\mb\beta}}=(\mb{X}^t\mb{X})^{-1}\sigma^2\)</span>. The latter is often denoted by <span class="math inline">\(\mb\Sigma_{\mb\beta}\)</span>.</p>
<p>With this notation, the <strong>standardised</strong> parameter estimator of <span class="math inline">\(\beta_j\)</span> is then given by <span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{\sigma_{\beta_j}}.
\]</span> The following corollary is given without proof. </p>

<div class="corollary">
<span id="cor:DistrStandBeta" class="corollary"><strong>Corollary 2.1  (Sampling distribution of the standardised LSE)  </strong></span>Assume that Model <a href="#eq:Mod4">(2.7)</a> holds true. Then, <span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{\sigma_{\beta_j}} \sim N(0,1).
\]</span> Assume that Model <a href="#eq:Mod3">(2.2)</a> holds true. Then, as <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{\sigma_{\beta_j}} \convDistr N(0,1).
\]</span>
</div>

<p>When the variance <span class="math inline">\(\sigma^2\)</span> is not known, it can be replaced by its estimator MSE. The estimator of <span class="math inline">\(\var{\hat{\mb\beta}}=(\mb{X}^t\mb{X})^{-1}\sigma^2\)</span> is often denoted by <span class="math inline">\(\hat{\mb\Sigma}_{\mb\beta}=(\mb{X}^t\mb{X})^{-1}\MSE\)</span> and the estimator of <span class="math inline">\(\sigma^2_{\beta_j}\)</span> by <span class="math inline">\(\hat\sigma^2_{\beta_j}\)</span> or by <span class="math inline">\(S^2_{\beta_j}\)</span>. The square root of <span class="math inline">\(S^2_{\beta_j}\)</span>, i.e. <span class="math inline">\(S_{\beta_j}\)</span>, is also known as the <strong>standard error</strong> (SE or se) of the paramater estimator <span class="math inline">\(\hat\beta_j\)</span>.</p>
<p>The <strong>studentised</strong> estimator of <span class="math inline">\(\beta_j\)</span> is then defined as <span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{\hat\sigma_{\beta_j}}=\frac{\hat\beta_j-\beta_j}{S_{\beta_j}}.
\]</span> The following theory gives the (asymptotic) sampling distribution of the stundentised estimators (without proof). </p>

<div class="theorem">
<span id="thm:DistrStudBeta" class="theorem"><strong>Theorem 2.8  (Sampling Distribution of the studentised LSE)  </strong></span>Assume that Model <a href="#eq:Mod4">(2.7)</a> holds true. Then it holds that <span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{S_{\beta_j}} \sim t_{n-2}.
\]</span> Assume that Model <a href="#eq:Mod3">(2.2)</a> holds true. Then, as <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{S_{\beta_j}} \convDistr N(0,1).
\]</span>
</div>

</div>
<div id="S:BIReg1" class="section level2">
<h2><span class="header-section-number">2.6</span> Confidence Intervals</h2>
<p>Theorem <a href="#thm:DistrStudBeta">2.8</a> gives the (asymptotic) sampling distribution of the estimators <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>. This forms the basis for confidence intervals. In this section we give the result for the normal linear regression model <a href="#eq:Mod4">(2.7)</a>, for which we have developped the exact sampling distributions.</p>
<p>We will use the notation <span class="math inline">\(\sigma^2_{\beta_0}=\var{\hat\beta_0}\)</span> and <span class="math inline">\(\sigma^2_{\beta_1}=\var{\hat\beta_1}\)</span>. These are the diagonal elements of the covariance matrix <span class="math inline">\(\mb\Sigma_\beta\)</span>. Theorem <a href="#thm:DistrStudBeta">2.8</a> implies</p>
<span class="math display" id="eq:tmp998765615">\[\begin{equation}
  \frac{\hat\beta_1 - \beta_1}{\hat\sigma_{\beta_1}} \sim t_{n-2} .
  \tag{2.10}
\end{equation}\]</span>
<p>For a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom, say <span class="math inline">\(T\sim t_{n-2}\)</span>, it follows by definition that <span class="math display">\[
   \prob{-t_{n-2;1-\alpha/2} &lt; T &lt; t_{n-2;1-\alpha/2}} = 1-\alpha.
\]</span> Hence, with <span class="math inline">\(T=\frac{\hat\beta_1 - \beta_1}{\hat\sigma_{\beta_1}}\sim t_{n-2}\)</span>, the identity <span class="math display">\[
  \prob{-t_{n-2;1-\alpha/2} &lt; \frac{\hat\beta_1 - \beta_1}{\hat\sigma_{\beta_1}} &lt; t_{n-2;1-\alpha/2}} = 1-\alpha
\]</span> implies that <span class="math display">\[
  \prob{\hat\beta_1-t_{n-2;1-\alpha/2} \hat\sigma_{\beta_1}&lt;  \beta_1 &lt; \hat\beta_1+t_{n-2;1-\alpha/2} \hat\sigma_{\beta_1}} = 1-\alpha.
\]</span> From this equality, the <span class="math inline">\(1-\alpha\)</span> *confidence interval** (CI) of <span class="math inline">\(\beta_1\)</span> follows directly: <span class="math display">\[
 \left[\hat\beta_1-t_{n-2;1-\alpha/2} \hat\sigma_{\beta_1}, \hat\beta_1+t_{n-2;1-\alpha/2} \hat\sigma_{\beta_1}\right].
\]</span> The construction of the confidence interval of <span class="math inline">\(\beta_0\)</span> is analogous.</p>
<p>The interpretation of a confidence interval is now demonstrated by means of repeated sampling in a simulation study. We start with a small sample size of <span class="math inline">\(n=5\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">267213</span>)

simRegressionBI&lt;-<span class="cf">function</span>(<span class="dt">N=</span><span class="dv">10000</span>,<span class="dt">nRep=</span><span class="dv">1</span>) {
  <span class="co"># N: number of repeated samples</span>
  <span class="co"># nRep: number of replicated for each value of the regressor</span>
  
  x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># 5 father&#39;s heights</span>
  x&lt;-<span class="kw">rep</span>(x,nRep) <span class="co"># the five regrossor values are replaciated nRep times</span>
  x
  
  Results&lt;-<span class="kw">data.frame</span>(<span class="dt">beta1Hat=</span><span class="ot">NA</span>,<span class="dt">CI.lower=</span><span class="ot">NA</span>,<span class="dt">CI.upper=</span><span class="ot">NA</span>,
                      <span class="dt">cover=</span><span class="ot">NA</span>)
  
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
      y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span>(<span class="kw">rexp</span>(<span class="dv">5</span><span class="op">*</span>nRep)<span class="op">-</span><span class="dv">1</span>) <span class="co"># random sample of outcomes</span>
      m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
      Results[i,<span class="dv">1</span>]&lt;-<span class="kw">coef</span>(m)[<span class="dv">2</span>]
      Results[i,<span class="dv">2</span><span class="op">:</span><span class="dv">3</span>]&lt;-<span class="kw">confint</span>(m)[<span class="dv">2</span>,] <span class="co"># default is a 95% CI</span>
      Results[i,<span class="dv">4</span>]&lt;-(<span class="fl">0.5</span><span class="op">&lt;</span>Results[i,<span class="dv">3</span>])<span class="op">&amp;</span>(Results[i,<span class="dv">2</span>]<span class="op">&lt;</span><span class="st"> </span><span class="fl">0.5</span>) <span class="co"># 0.5 is true parameter value</span>
  }

  <span class="kw">return</span>(Results)
}

plotBI&lt;-<span class="cf">function</span>(SimBI,<span class="dt">nPlot=</span><span class="kw">nrow</span>(SimBI),<span class="dt">mn=</span><span class="kw">min</span>(SimBI<span class="op">$</span>CI.lower)
                 ,<span class="dt">mx=</span><span class="kw">max</span>(SimBI<span class="op">$</span>CI.upper),...) {
  <span class="co"># SimBI: results of the function SimRegressionBI</span>
  <span class="co"># nPlot: number of repeated experiments that need to be plotted</span>
  <span class="co"># mn: lower limit horizontal axis</span>
  <span class="co"># mx: upper limit horizontal axis</span>
  
  <span class="kw">plot</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dt">xlim=</span><span class="kw">c</span>(mn,mx),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,nPlot),<span class="dt">xlab=</span><span class="st">&quot;&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;experiment&quot;</span>,
       <span class="dt">type=</span><span class="st">&quot;n&quot;</span>,...)
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nPlot) {
      <span class="kw">arrows</span>(SimBI<span class="op">$</span>CI.lower[i],i,SimBI<span class="op">$</span>CI.upper[i],i,
             <span class="dt">code=</span><span class="dv">0</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
             <span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">col=</span><span class="dv">2</span><span class="op">-</span>SimBI<span class="op">$</span>cover[i])
    <span class="kw">points</span>(SimBI<span class="op">$</span>beta1Hat[i],i)
  }
  <span class="kw">abline</span>(<span class="dt">v=</span><span class="fl">0.5</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">1</span>)
}

SimBI5&lt;-<span class="kw">simRegressionBI</span>(<span class="dt">N=</span>N1000,<span class="dt">nRep=</span><span class="dv">1</span>)
<span class="kw">mean</span>(SimBI5<span class="op">$</span>cover) <span class="co"># empirical coverage</span></code></pre></div>
<pre><code>## [1] 0.966</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plotBI</span>(SimBI5,<span class="dt">nPlot=</span><span class="dv">30</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:SimBI5"></span>
<img src="DASM2_files/figure-html/SimBI5-1.png" alt="95\% confidence intervals from repeated sampling ($n=5$). The points represent the point estimates $\hat\beta_1$. Only the results of the first 30 repeated experiments are shown. The vertical line indicates the true parameter value: $\beta_1=0.5$." width="672" />
<p class="caption">
Figure 2.11: 95% confidence intervals from repeated sampling (<span class="math inline">\(n=5\)</span>). The points represent the point estimates <span class="math inline">\(\hat\beta_1\)</span>. Only the results of the first 30 repeated experiments are shown. The vertical line indicates the true parameter value: <span class="math inline">\(\beta_1=0.5\)</span>.
</p>
</div>
<p>From the output we read the <strong>empirical coverage</strong> of the <span class="math inline">\(95\%\)</span> confidence interval: 0.966. This is based on 1000 repeated experiments. For a large number of repeated experiments, the empirical coverage is a good approximation of the true coverage probability. In our simulation study, the empirical coverage is (approximately) equal to the <strong>nominal</strong> <span class="math inline">\(95\%\)</span> confidence level, which (empirically) demonstrates that the theory is correct (the CI has its correct probabilistic interpretation). The results are visualised in Figure <a href="#fig:SimBI5">2.11</a>: the first 30 CIs are shown. Of these 30 intervals, 29 cover the true parameter value <span class="math inline">\(\beta_1=0.5\)</span>. That gives thus an empirical coverage of <span class="math inline">\(29/30=96.7\%\)</span>. Note that the R output shows the empirical coverage of all 1000 repeated experiments.</p>
<p>Figure <a href="#fig:SimBI5b">2.12</a> shows the results of the first 100 repeated experiments.</p>
<div class="figure" style="text-align: center"><span id="fig:SimBI5b"></span>
<img src="DASM2_files/figure-html/SimBI5b-1.png" alt="95\% confidence intervals from repeated sampling ($n=5$). The points represent the point estimates $\hat\beta_1$. Only the results of the first 100 repeated experiments are shown. The vertical line indicates the true parameter value: $\beta_1=0.5$." width="672" />
<p class="caption">
Figure 2.12: 95% confidence intervals from repeated sampling (<span class="math inline">\(n=5\)</span>). The points represent the point estimates <span class="math inline">\(\hat\beta_1\)</span>. Only the results of the first 100 repeated experiments are shown. The vertical line indicates the true parameter value: <span class="math inline">\(\beta_1=0.5\)</span>.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">267213</span>)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))
<span class="kw">plotBI</span>(SimBI5,<span class="dt">nPlot=</span><span class="dv">30</span>,<span class="dt">mn=</span><span class="op">-</span><span class="fl">0.2</span>,<span class="dt">mx=</span><span class="dv">1</span>,<span class="dt">main=</span><span class="st">&quot;n=5&quot;</span>)

SimBI10&lt;-<span class="kw">simRegressionBI</span>(<span class="dt">N=</span>N1000,<span class="dt">nRep=</span><span class="dv">2</span>)
<span class="kw">plotBI</span>(SimBI10,<span class="dt">nPlot=</span><span class="dv">30</span>,<span class="dt">mn=</span><span class="op">-</span><span class="fl">0.2</span>,<span class="dt">mx=</span><span class="dv">1</span>,<span class="dt">main=</span><span class="st">&quot;n=10&quot;</span>)

SimBI50&lt;-<span class="kw">simRegressionBI</span>(<span class="dt">N=</span>N1000,<span class="dt">nRep=</span><span class="dv">10</span>)
<span class="kw">plotBI</span>(SimBI50,<span class="dt">nPlot=</span><span class="dv">30</span>,<span class="dt">mn=</span><span class="op">-</span><span class="fl">0.2</span>,<span class="dt">mx=</span><span class="dv">1</span>,<span class="dt">main=</span><span class="st">&quot;n=50&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:SimBIAll"></span>
<img src="DASM2_files/figure-html/SimBIAll-1.png" alt="95\% confidence intervals from repeated sampling ($n=5$, $n=10$ and $n=50$). The points represent the point estimates $\hat\beta_1$. Only the results of the first 30 repeated experiments are shown. The vertical line indicates the true parameter value: $\beta_1=0.5$." width="672" />
<p class="caption">
Figure 2.13: 95% confidence intervals from repeated sampling (<span class="math inline">\(n=5\)</span>, <span class="math inline">\(n=10\)</span> and <span class="math inline">\(n=50\)</span>). The points represent the point estimates <span class="math inline">\(\hat\beta_1\)</span>. Only the results of the first 30 repeated experiments are shown. The vertical line indicates the true parameter value: <span class="math inline">\(\beta_1=0.5\)</span>.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p>We repeat the simulation study for larger sample sizes. The results are shown in Figure <a href="#fig:SimBIAll">2.13</a> (only for the first 30 repeated experiments). The empirical coverages based on 1000 repeated experiments are:</p>
<ul>
<li><p><span class="math inline">\(n=5\)</span>: 0.966</p></li>
<li><p><span class="math inline">\(n=10\)</span>: 0.962</p></li>
<li><p><span class="math inline">\(n=50\)</span>: 0.948</p></li>
</ul>
<p>For all sample sizes <span class="math inline">\(n\)</span> the empirical covarages are very close to the nominal confidence level of <span class="math inline">\(95\%\)</span>.</p>
<p>Figure <a href="#fig:SimBIAll">2.13</a> demonstrates that the lengths of the confidence intervals become smaller as the sample size increases. This follows directly from the theory: the length of a <span class="math inline">\(95\%\)</span> is <span class="math inline">\(2t_{n-2;1-0.05/2} \hat\sigma_{\beta_1}\)</span> and this decreases because <span class="math inline">\(\hat\sigma_{\beta_1}\)</span> decreases in expectation with increasing sample size (<span class="math inline">\(\sigma_{beta_1} \propto \frac{1}{\sqrt{n}}\)</span>). Thus, the lenght of the intervals decreases as <span class="math inline">\(n\)</span> increases, and still the coverage remains <span class="math inline">\(95\%\)</span>. This phenomenon is related to the variability of the estimates: with increasing sample size <span class="math inline">\(n\)</span>, the variability of the estimates decreases and thus the CI can be smaller while still giving a coverage of <span class="math inline">\(95\%\)</span>.</p>
</div>
<div id="example-galtons-height-data-2" class="section level2 unnumbered">
<h2>Example (Galton's height data)</h2>
<p>We repeat the regression analysis for Galton's data. This time we look at the standard errors and the <span class="math inline">\(95\%\)</span> confidence interval of the regression coefficient <span class="math inline">\(\beta_1\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(son.cm<span class="op">~</span>father.cm,<span class="dt">data=</span>Galton.sons)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = son.cm ~ father.cm, data = Galton.sons)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.9406  -3.5300   0.2605   3.4064  20.5805 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 89.81819   11.73609   7.653 1.37e-12 ***
## father.cm    0.50766    0.06683   7.596 1.91e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.661 on 171 degrees of freedom
## Multiple R-squared:  0.2523, Adjusted R-squared:  0.2479 
## F-statistic:  57.7 on 1 and 171 DF,  p-value: 1.907e-12</code></pre>
<p>The standard errors of the parameter estimates are <span class="math display">\[
   \hat\sigma_{\beta_0} =  11.7 \;\;\text{ and }\;\; \hat\sigma_{\beta_1} =  0.0668.
 \]</span> From the R output we also read the MSE (square of the ``Residual standard error&quot;): <span class="math display">\[
   \MSE = 5.661^2 = 32.05
 \]</span> and the residual number of degrees of freedom is <span class="math inline">\(n-2=173-2=171\)</span>.</p>
<p>Next we use the standard error of <span class="math inline">\(\hat\beta_1\)</span> for computing the <span class="math inline">\(95\%\)</span> confidence interval of <span class="math inline">\(\beta_1\)</span>. We also need <span class="math inline">\(t_{n-2;1-\alpha/2}=t_{171;0.975}\)</span> nodig.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># quantile of t-distribution</span>
<span class="kw">qt</span>(<span class="fl">0.975</span>,<span class="dt">df=</span><span class="dv">171</span>)</code></pre></div>
<pre><code>## [1] 1.973934</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># lower bound of 95% CI</span>
<span class="fl">0.50766</span><span class="op">-</span><span class="kw">qt</span>(<span class="fl">0.975</span>,<span class="dt">df=</span><span class="dv">171</span>)<span class="op">*</span><span class="fl">0.0668</span></code></pre></div>
<pre><code>## [1] 0.3758012</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># upper bound of 95% CI</span>
<span class="fl">0.50766</span><span class="op">+</span><span class="kw">qt</span>(<span class="fl">0.975</span>,<span class="dt">df=</span><span class="dv">171</span>)<span class="op">*</span><span class="fl">0.0668</span></code></pre></div>
<pre><code>## [1] 0.6395188</code></pre>
<p>Het <span class="math inline">\(95\%\)</span> betrouwbaarheidsinterval van <span class="math inline">\(\beta_1\)</span> is dus <span class="math display">\[
   [0.376 , 0.640] .
 \]</span> Hence, with a probability of 95% we expect that the regression coefficient <span class="math inline">\(\beta_1\)</span> is somewhere between <span class="math inline">\(0.376\)</span> and <span class="math inline">\(0.640\)</span>. Thus, if the father's height increases with 1cm, we expect with a probability of 95% that the average son's height increases with <span class="math inline">\(0.376\)</span>cm to <span class="math inline">\(0.640\)</span>cm. Equivalently, we could say that if the father's height increases with 5cm, we expect with a probability of 95% that the average son's height increases with <span class="math inline">\(5\times 0.376=1.88\)</span>cm to <span class="math inline">\(5 \times 0.640=3.2\)</span>cm.</p>
<p>Note that all values within the CI are positive. The data are thus consistent with a positive effect of father's height on the expected son's height.</p>
<p>The next R code can also be used for the computation of the CI.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m)</code></pre></div>
<pre><code>##                  2.5 %      97.5 %
## (Intercept) 66.6519190 112.9844701
## father.cm    0.3757362   0.6395761</code></pre>
<p>Finally we note that the correct probabilistic interpretation of the CI depends on the distributional assumption that part of the statistical model. In particular: correct specification of the the conditional mean <span class="math inline">\(m(x)\)</span> as a linear function of <span class="math inline">\(x\)</span>; normality of the error term; homoskedasticity; mutual independence of the outcomes. Later we will introduce methods that can be used for assessing these assumptions.</p>
</div>
<div id="exercise-blood-pressure-1" class="section level2 unnumbered">
<h2>Exercise: Blood Pressure</h2>
<p>Consider again the blood pressure dataset and calculate and interpret a <span class="math inline">\(95\%\)</span> confidence interval of the regression slope.</p>
<p><details> <summary markdown="span">Try to make this exercise and expand the page to see the soluation.</summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the data</span>
<span class="kw">load</span>(<span class="st">&quot;Data/BloodPressure.RData&quot;</span>)

<span class="co"># fit the model</span>
m&lt;-<span class="kw">lm</span>(bp.reduction<span class="op">~</span>dose,<span class="dt">data=</span>BloodPressure)

<span class="co"># 95% confidence intervals</span>
<span class="kw">confint</span>(m, <span class="dt">level=</span><span class="fl">0.95</span>)</code></pre></div>
<pre><code>##                 2.5 %   97.5 %
## (Intercept) -1.948188 2.014267
## dose         1.437469 2.135218</code></pre>
<p>Based on this output we conclude that with a probability of <span class="math inline">\(95\%\)</span> we expect the blood pressure to be reduced with 1.4 to 2.1 mmHg when the dose increases with 1mg per day.</p>
<p></details></p>
</div>
<div id="S_RegTests" class="section level2">
<h2><span class="header-section-number">2.7</span> Hypothesis Tests</h2>
<p>A frequent reason for performing a regression analysis, is to give an answer to the research questions whether or not the average outcome is linearly associated with a regressor. In the Galton's data example, Galton wanted to know whether or not the average height of son's linearly depend on their father's height. In terms of regression model <a href="#eq:Mod4">(2.7)</a> this research question translates into the null hypothesis <span class="math display">\[
  H_0: \beta_1 = 0.
\]</span> The alternative hypothesis depends on the exact formulation of the research question, or on prior knowledge. In the Galton's data example there may be two sensible situations one could think of:</p>
<ul>
<li><p>Galton did not have a clue as to what the relation between father's and son's heights could be, because there is also the mother. In this setting, the alternative hypothesis becomes <span class="math inline">\(H_1: \beta_1\neq 0\)</span>. (<em>two-sided alternative</em>)</p></li>
<li><p>Galton had prior knowledge (from literature, from discussions with other scientists, or from other independent datasets) that a negative relation (i.e. <span class="math inline">\(\beta_1&lt;0\)</span>) is not plausible. In this setting, the alternative hypothesis would be formulated as <span class="math inline">\(H_1: \beta_1&gt; 0\)</span>. (<em>one-sided alternative to the right</em>)</p></li>
<li><p>For completeness, we also give this third option, which does probably make much sense for the Galton data example. Galton could not have been interested in detecting a negative association, in which case he would again have chosen for the alternative <span class="math inline">\(H_1: \beta_1&gt; 0\)</span>.</p></li>
</ul>
<p>We propose the test statistic <span class="math display">\[
  T =  \frac{\hat\beta_1}{\hat\sigma_{\beta_1}} .
\]</span> From this expression, it is evident that this statistic is sensitive for deviations from <span class="math inline">\(H_0\)</span> in the direction of the two-sided as well as the one-sided alternatives.</p>
<p>If we assume that the normal regression model <a href="#eq:Mod4">(2.7)</a> holds, then from Equation <a href="#eq:tmp998765615">(2.10)</a> it follows that <span class="math display">\[
 T \HSim t_{n-2}.
\]</span> (i.e. under the null hypothesis the test statistic <span class="math inline">\(T\)</span> has a <span class="math inline">\(t_{n-2}\)</span> <strong>null distribution</strong>) Based on this null distribution, <span class="math inline">\(p\)</span>-values and rejection regions can be computed. In particular:</p>
<ul>
<li><p>The one-sided alternative hypothesis <span class="math inline">\(H_1:\beta_1&gt;0\)</span>.<br />
We wish to reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_1\)</span> for large values of <span class="math inline">\(T\)</span>. Hence, <span class="math display">\[
   p = \probf{0}{T\geq t} = 1-\probf{0}{T\leq t} = 1-F_T(t;n-2),
 \]</span> with <span class="math inline">\(F_T(.;n-2)\)</span> the CDF of <span class="math inline">\(t_{n-2}\)</span>.<br />
The rejection region for the test at the <span class="math inline">\(\alpha\)</span> significance level follows from <span class="math display">\[
   \alpha= \prob{\text{type I error}}=\probf{0}{\text{reject }H_0}=\probf{0}{T&gt;t_{n-2;1-\alpha}}.
 \]</span> The rejection region is thus <span class="math inline">\([t_{n-2;1-\alpha},+\infty[\)</span>. We also say that <span class="math inline">\(t_{n-2;1-\alpha}\)</span> is the <em>critical value</em>.</p></li>
<li><p>The one-sided alternative hypothesis <span class="math inline">\(H_1:\beta_1&lt;0\)</span>.<br />
We wish to reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_1\)</span> for small values of <span class="math inline">\(T\)</span> (i.e. large, but negative values of <span class="math inline">\(T\)</span>). Hence, <span class="math display">\[
   p = \probf{0}{T\leq t} = F_T(t;n-2).
 \]</span> The rejection region for the test at the <span class="math inline">\(\alpha\)</span> significance level follows from <span class="math display">\[
   \alpha= \prob{\text{type I error}}=\probf{0}{\text{reject }H_0}=\probf{0}{T&lt;t_{n-2;\alpha}}=\probf{0}{T&lt;-t_{n-2;1-\alpha}},
 \]</span> and is thus given by <span class="math inline">\(]-\infty, -t_{n-2;1-\alpha}]\)</span>. Or, equivalently, the critical value is <span class="math inline">\(t_{n-2;\alpha}=-t_{n-2;1-\alpha}\)</span>.</p></li>
<li><p>The two-sided alternative hypothesis <span class="math inline">\(H_1:\beta_1\neq 0\)</span>.<br />
We wish to reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_1\)</span> for large and small values of <span class="math inline">\(T\)</span> (i.e. large positive and negative values of <span class="math inline">\(T\)</span>). Hence, <span class="math display">\[
   p = \probf{0}{|T|\geq |t|} = \probf{0}{T\leq -|t| \text{ of } T \geq |t|} = 2 \probf{0}{T\geq |t|}=2(1-F_T(|t|;n-2)).
 \]</span> The rejection region for the test at the <span class="math inline">\(\alpha\)</span> significance level follows from <span class="math display">\[
   \alpha= \prob{\text{type I error}}=\probf{0}{\text{reject }H_0}=\probf{0}{|T|&gt;t_{n-2;1-\alpha/2}},
 \]</span> and it is thus give by <span class="math inline">\([-t_{n-2;1-\alpha/2},t_{n-2;1-\alpha/2}]\)</span>, or, equivalently, the critical value for the test based on <span class="math inline">\(|T|\)</span> is given by <span class="math inline">\(t_{n-2;1-\alpha/2}\)</span>.</p></li>
</ul>
</div>
<div id="example-galtons-height-data-3" class="section level2 unnumbered">
<h2>Example (Galton's height data)</h2>
<p>Let's look again at the results of the regression analysis in R.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(son.cm<span class="op">~</span>father.cm,<span class="dt">data=</span>Galton.sons)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = son.cm ~ father.cm, data = Galton.sons)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.9406  -3.5300   0.2605   3.4064  20.5805 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 89.81819   11.73609   7.653 1.37e-12 ***
## father.cm    0.50766    0.06683   7.596 1.91e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.661 on 171 degrees of freedom
## Multiple R-squared:  0.2523, Adjusted R-squared:  0.2479 
## F-statistic:  57.7 on 1 and 171 DF,  p-value: 1.907e-12</code></pre>
<p>We want to test the null hypothesis <span class="math inline">\(H_0: \beta_1=0\)</span> against the alternative hypothesis <span class="math inline">\(H_1: \beta_1\neq 0\)</span>. In the output we read on the line of : <span class="math inline">\(t=7.596\)</span> and <span class="math inline">\(p=1.91\times 10^{-12}\)</span>. We verify these results in the next chunck.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># observed test statistic</span>
t.obs&lt;-<span class="fl">0.50766</span> <span class="op">/</span><span class="st"> </span><span class="fl">0.06683</span> 
t.obs</code></pre></div>
<pre><code>## [1] 7.596289</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># two-sided p-value</span>
<span class="dv">2</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">pt</span>(<span class="kw">abs</span>(t.obs),<span class="dt">df=</span><span class="dv">171</span>))</code></pre></div>
<pre><code>## [1] 1.905587e-12</code></pre>
<p>Hence, at the 5% level of significance we conclude that there is a positive effect of the father's height on the average height of their sons (<span class="math inline">\(p&lt;0.0001\)</span>). From previous analyses we know that the effect is estimated as <span class="math inline">\(\hat\beta_1=0.51\)</span> with a 95% confidence interval of <span class="math inline">\([0.376,0.640]\)</span>.</p>
<p>Suppose that the researchers know that the effect of the father's height on the average son's height can never be negative. The the alternative hypothesis becomes <span class="math inline">\(H_1: \beta_1&lt;0\)</span>. The R output from the  function, however, always gives the <span class="math inline">\(p\)</span>-value for a two-sided alternative hypothesis. Upon using the symmetry of the <span class="math inline">\(t_{n-2}\)</span> null distribution we can easily convert the two-sided <span class="math inline">\(p\)</span>-value to a one-sided <span class="math inline">\(p\)</span>-value. In particular, the R output gives the two-sided <span class="math inline">\(p\)</span>-value, which is defined as <span class="math display">\[
   2\probf{0}{T\geq |t|} .
 \]</span> For the one-sided test we need <span class="math display">\[
   \probf{0}{T\leq t}.
\]</span> If <span class="math inline">\(t&lt;0\)</span>, then <span class="math inline">\(t=-|t|\)</span> and thus <span class="math display">\[
   p=\probf{0}{T\leq t} = \probf{0}{T\leq -|t|} = \probf{0}{-T\geq |t|} = \probf{0}{T\geq |t|} ,
\]</span> in which the last step made use of the symmetry of the null distrubution of <span class="math inline">\(T\)</span> (i.e. the distribution of <span class="math inline">\(-T\)</span> equals the distribution of <span class="math inline">\(T\)</span> under <span class="math inline">\(H_0\)</span>). Consequently, the one-sided <span class="math inline">\(p\)</span>-value equals half of the two-sided <span class="math inline">\(p\)</span>-value if <span class="math inline">\(t\)</span> is negative. Thus,<br />
<span class="math display">\[
   p = \frac{1}{2}\times 1.9\times 10^{-12} = 9.5\times 10^{-13}.
\]</span> For this data example we come to the same conclusion as before.</p>
</div>
<div id="exercise-muscle-mass" class="section level2 unnumbered">
<h2>Exercise: Muscle mass</h2>
<p>Scientists suspect that the muscle mass of people starts declining from a certain age onwards. To verify this research question, a nutritionist randomly sampled 59 women, aged between 41 and 78. For these women; also the muscle mass was measured (we actually only have a proxy based on bioelectrical impedance measurements).</p>
<p>Perform a regression analysis and formulate an answer to this research question (including parameter estimates, confidence interval and hypothesis test). You may use the next chunk of R code for reading the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">muscles&lt;-<span class="kw">read.csv</span>(<span class="st">&quot;Data/muscles.txt&quot;</span>, <span class="dt">sep=</span><span class="st">&quot; &quot;</span>)
<span class="kw">names</span>(muscles)&lt;-<span class="kw">c</span>(<span class="st">&quot;muscle.mass&quot;</span>,<span class="st">&quot;age&quot;</span>)
<span class="kw">skim</span>(muscles)</code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-40">Table 2.2: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">muscles</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">59</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">2</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">muscle.mass</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">84.61</td>
<td align="right">16.11</td>
<td align="right">52</td>
<td align="right">73.0</td>
<td align="right">84</td>
<td align="right">96.5</td>
<td align="right">119</td>
<td align="left">▃▇▇▆▃</td>
</tr>
<tr class="even">
<td align="left">age</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">60.27</td>
<td align="right">11.68</td>
<td align="right">41</td>
<td align="right">51.5</td>
<td align="right">60</td>
<td align="right">70.0</td>
<td align="right">78</td>
<td align="left">▇▃▇▆▇</td>
</tr>
</tbody>
</table>
<p><details> <summary markdown="span">Try to make this exercise and expand the page to see the soluation.</summary></p>
<p>First we fit the linear regression model and make a graph of the data and the fitted regression line.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(muscle.mass<span class="op">~</span>age, <span class="dt">data=</span>muscles)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = muscle.mass ~ age, data = muscles)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -16.121  -6.373  -0.674   6.968  23.455 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 156.22438    5.68612   27.48   &lt;2e-16 ***
## age          -1.18820    0.09265  -12.82   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.244 on 57 degrees of freedom
## Multiple R-squared:  0.7426, Adjusted R-squared:  0.7381 
## F-statistic: 164.5 on 1 and 57 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m)</code></pre></div>
<pre><code>##                  2.5 %     97.5 %
## (Intercept) 144.838119 167.610636
## age          -1.373721  -1.002678</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(muscles,
       <span class="kw">aes</span>(<span class="dt">x=</span>age, <span class="dt">y=</span>muscle.mass)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">color=</span><span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;age (years)&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;muscle mass&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>), <span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept=</span>m<span class="op">$</span>coefficients[<span class="dv">1</span>],<span class="dt">slope=</span>m<span class="op">$</span>coefficients[<span class="dv">2</span>]) </code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<p>From the output we can formulate the following conclusions.</p>
<p>On average the muscle mass of women aged between 41 and 78 years is significantly negatively associated with the age (<span class="math inline">\(p&lt;0.001\)</span>) at the <span class="math inline">\(5\%\)</span> level of significance. We estimate that the mean muscle mass decreases with 1.19 (SE=0.09) units with an increase of age of 1 year. The <span class="math inline">\(95\%\)</span> confidence interval of this estimate is -1.37 to -1 units per increase of age with one unit.</p>
<p>For women between the age of </details></p>
</div>
<div id="S_AssessAssumptions" class="section level2">
<h2><span class="header-section-number">2.8</span> Assessment of the Model Assumptions</h2>

<p>The normal simple linear regression model <a href="#eq:Mod4">(2.7)</a> is the most restrictive in the sense that it requires the largest numer of assumptions. In this section we discuss the importance of the model assumptions and how they can be verified based on the observed data. We will use Galton's height data to illustrate the methods.</p>
</div>
<div id="example-galtons-height-data-4" class="section level2 unnumbered">
<h2>Example (Galton's height data)</h2>
<p>We will verify one-by-one the model assumption of model <a href="#eq:Mod4">(2.7)</a>.</p>
<div id="linearity-of-the-regression-model" class="section level4 unnumbered">
<h4>Linearity of the regression model</h4>
<p>The conditional mean of the outcome must satisfy <span class="math display">\[
   \E{Y\mid x} = m(x;\mb\beta) = \beta_0+\beta_1 x.
 \]</span> If the parameters are known, then this is equivalent to the condition <span class="math display">\[
   0 = \E{\eps \mid x} = \E{Y-m(x;\mb\beta)\mid x}=\E{Y-\beta_0-\beta_1 x\mid x} .
 \]</span></p>
<p>If there are replicated outcomes available for a given <span class="math inline">\(x\)</span>, then <span class="math inline">\(\E{Y-\beta_0-\beta_1 x\mid x}\)</span> can be (unbiasedly) estimated as the sample mean of the residuals <span class="math inline">\(e_i=y_i-\hat\beta_0-\hat\beta_1x_i\)</span> for which <span class="math inline">\(x_i=x\)</span>. These avarage residuals can be computed for all <span class="math inline">\(x\in \{x_1,\ldots, x_n\}\)</span>. Note that for Galton's data, for some of the regressor values (father's heights) there is only one observed outcome. For these regressor values, the sample mean of the residuals equals the residual. Figure <a href="#fig:GaltonRes1">2.14</a> shows the result for the Galton data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(son.cm<span class="op">~</span>father.cm,<span class="dt">data=</span>Galton.sons)
e&lt;-m<span class="op">$</span>residuals
x.all&lt;-<span class="kw">unique</span>(Galton.sons<span class="op">$</span>father.cm)
ave.e&lt;-<span class="kw">c</span>()
<span class="cf">for</span>(x <span class="cf">in</span> x.all) {
    ave.e&lt;-<span class="kw">c</span>(ave.e,<span class="kw">mean</span>(e[Galton.sons<span class="op">$</span>father.cm<span class="op">==</span>x]))
}
<span class="kw">plot</span>(Galton.sons<span class="op">$</span>father.cm,e,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Father&#39;s height (cm)&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;residuals&quot;</span>)
<span class="kw">points</span>(x.all,ave.e,<span class="dt">col=</span><span class="dv">2</span>,<span class="dt">pch=</span><span class="dv">8</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:GaltonRes1"></span>
<img src="DASM2_files/figure-html/GaltonRes1-1.png" alt="Scatterplot of the residuals against the father's heights (Galton's height example). The red stars represent the sample means of the residuals for a given father's height." width="672" />
<p class="caption">
Figure 2.14: Scatterplot of the residuals against the father's heights (Galton's height example). The red stars represent the sample means of the residuals for a given father's height.
</p>
</div>
<p>Since the sample means of the residuals are only estimates of <span class="math inline">\(\E{Y-\beta_0-\beta_1 x\mid x}\)</span>, we cannot expect that these sample means are exactly equal to zero (i.e. the sample means also show a sampling distribution). The larger the number of replicates on which such a sample mean is computed, the smaller the sampling variability and the closer we expect the sample mean to zero.</p>
<p>We hope that the average residuals do not show a systematic pattern as a function of the regressor; this would agree with the model assumption <span class="math inline">\(\E{\eps \mid x}=0\)</span>. Figure <a href="#fig:GaltonRes1">2.14</a> shows no such systematic pattern, and therefore we conclude that the linear relation between the regressor (father's height) and the mean response (son's height) is linear.</p>
<p>If there are no replicates at the regressor values, then the sample plot can be constructed, but with no sample means of the residuals. Such graphs (with or without the average residuals) are known as <strong>residual plots</strong>.</p>
</div>
<div id="normality-of-the-error-term" class="section level3 unnumbered">
<h3>Normality of the error term</h3>
<p>The model implies that <span class="math display">\[ 
  \eps_i=Y_i-m(x_i;\mb\beta) \mid x_i \sim N(0,\sigma^2). 
\]</span></p>
<p>To some extent the residuals <span class="math inline">\(e_i=Y_i-m(x_i;\hat{\mb\beta})\)</span> can be considered as ``estimates'' of <span class="math inline">\(\ep_i\)</span>. Therefore we will use the residuals for assessing the normality assumption. We could use histograms, boxplots and normal QQ-plots for this purpose. Particularly the normal QQ-plots are informative, because they are specifically developped for this assessing normality.</p>
<p>Figure <a href="#fig:GaltonQQResid">2.15</a> shows the normal QQ-plot of the residuals of the Galton example. Most of the points in the QQ-plot are close to the straight line, with only a few larger deviations in the right hand tail of the distribution. However, the number of outliers (2 or 3) is very small as compared to the sample size (173). The plot does also not reveal systematic deviations from the straight line.</p>
<p>Finally, note that the normality assumption is not very important for this example, because the rather large sample size of <span class="math inline">\(n=173\)</span> tells us that the parameter estimators will be approximately normally distributed thanks to the central limit theorem. So only a very strong deviation from normality would have been worrisome. Also recall that the <span class="math inline">\(p\)</span>-value for the two-sided test for <span class="math inline">\(H_0:\beta_1=0\)</span> was very small (<span class="math inline">\(p&lt;0.001\)</span>); thus even a small deviation from normality would not have caused doubt over the conclusion of the statistical test.</p>
<p>The R code for the QQ-plot in Figure <a href="#fig:GaltonQQResid">2.15</a> is shown below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qqnorm</span>(m<span class="op">$</span>resid,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
       <span class="dt">xlab=</span><span class="st">&quot;expected quantiles&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;residuals&quot;</span>,<span class="dt">main=</span><span class="st">&quot;&quot;</span>)
<span class="kw">qqline</span>(m<span class="op">$</span>resid)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:GaltonQQResid"></span>
<img src="DASM2_files/figure-html/GaltonQQResid-1.png" alt="Normal QQ-plot of the residuals of the Galton example." width="672" />
<p class="caption">
Figure 2.15: Normal QQ-plot of the residuals of the Galton example.
</p>
</div>
</div>
<div id="homoskedasticity" class="section level3 unnumbered">
<h3>Homoskedasticity</h3>
<p>Model <a href="#eq:Mod4">(2.7)</a> implies that <span class="math display">\[
  \var{\eps_i}=\var{\eps_i\mid x_i} =\var{Y_i \mid x_i}=\sigma^2,
\]</span> i.e. the variance of the outcomes (and of the error terms) is constant and does not depend on the values of the regressor.</p>
<p>If there are replicated observations for each value of the regressor in the dataset, then the sample variance of the outcome can be calculated for unique observed value of the regressor. The sample variances can then be plotted against the regressor <span class="math inline">\(x\)</span>. If this graph shows no clear pattern that deviates from the assumption of a constant variance, then the graph suggests that the constant-variance assumption is satisfied.</p>
<p>If there are no replicated observations for the unique regressor values, than one may plot <span class="math inline">\(e_i^2\)</span> versus <span class="math inline">\(x_i\)</span>. If we say that the residuals <span class="math inline">\(e_i\)</span> are (approximately) estimates for the error terms <span class="math inline">\(\eps_i\)</span>, and becuase <span class="math inline">\(\var{\eps_i} = \E{\eps_i^2}\)</span>, we expect that <span class="math inline">\(\E{e_i^2} \approx \E{\eps_i}\)</span> and hence the plot of <span class="math inline">\(e_i^2\)</span> versus <span class="math inline">\(x_i\)</span> should not indicate a systematic pattern under the assumption of constant-variance.</p>
<p>If there are regressor values with and without replicates, than one could either choose to simply plot <span class="math inline">\(e_i^2\)</span> for all <span class="math inline">\(i=1,\ldots, n\)</span>, or for the replicated observations one may plot the average of the <span class="math inline">\(e_i^2\)</span> (note that this is different from the sample variances because of the factor <span class="math inline">\(1/(n-1)\)</span> in the calculation of the sample variance, and the factor <span class="math inline">\(1/n\)</span> in the calculation of the average). The reason for using <span class="math inline">\(1/n\)</span> in this case, is to make individual <span class="math inline">\(e_i^2\)</span> more comparable to the averages.</p>
<p>Figure <a href="#fig:GaltonResidVar">2.16</a> shows the plot for the Galton example. No systematic pattern can be observed, except for some outliers, particularly in the plot in the left panel (variance). The plot in the right panel is less extreme because of the square-root transformation. Note that particularly in the graph in the left panel we expect strong skewness to the right, because the sampling distribution of the sample variance is related to a <span class="math inline">\(\chi^2\)</span> distribution.</p>
<p>We conclude that there is no indication for a violation of the constant-variance assumption.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
heights&lt;-<span class="kw">unique</span>(Galton.sons<span class="op">$</span>father.cm)
var.y&lt;-<span class="kw">c</span>()
<span class="cf">for</span>(x <span class="cf">in</span> heights) {
    <span class="co">#var.y&lt;-c(var.y,var(Galton.sons$son.cm[Galton.sons$father.cm==x]))</span>
  var.y&lt;-<span class="kw">c</span>(var.y,<span class="kw">sum</span>(m<span class="op">$</span>residuals[Galton.sons<span class="op">$</span>father.cm<span class="op">==</span>x]<span class="op">^</span><span class="dv">2</span>))
}
<span class="kw">plot</span>(heights,var.y,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Father&#39;s height (cm)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;sample variance&quot;</span>,<span class="dt">col=</span><span class="dv">2</span>,<span class="dt">pch=</span><span class="dv">8</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">sum</span>(m<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="dv">173</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="co"># note that the reference line is at MSE (n-2)/n</span>

<span class="kw">plot</span>(heights,<span class="kw">sqrt</span>(var.y),<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Father&#39;s height (cm)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;sample standard deviation&quot;</span>,
     <span class="dt">col=</span><span class="dv">2</span>,<span class="dt">pch=</span><span class="dv">8</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">sqrt</span>(<span class="kw">sum</span>(m<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="dv">173</span>),<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:GaltonResidVar"></span>
<img src="DASM2_files/figure-html/GaltonResidVar-1.png" alt="Sample variances (left) en standard deviations (right) against the  regressor (father's height). The horizontal reference line corresponds to the MSE (left) and en $\sqrt{\MSE}$ (right)." width="672" />
<p class="caption">
Figure 2.16: Sample variances (left) en standard deviations (right) against the regressor (father's height). The horizontal reference line corresponds to the MSE (left) and en <span class="math inline">\(\sqrt{\MSE}\)</span> (right).
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># note that the reference line is at MSE (n-2)/n</span>
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p>The next two graphs in Figure <a href="#fig:GaltonResid2">2.17</a> show the squared residuals <span class="math inline">\(e_i^2\)</span> versus <span class="math inline">\(x_i\)</span> (thus no averaging, even at regressor values with replicated observatiobs) and the absolute values <span class="math inline">\(\vert e_i \vert\)</span> against <span class="math inline">\(x_i\)</span>. The latter graph is also supposed to give no systematic pattern if the assumption of constant-variance holds, and it is less sensitive to outliers are compared to the plot with the squared residuals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
e&lt;-m<span class="op">$</span>residuals
<span class="kw">plot</span>(Galton.sons<span class="op">$</span>father.cm,e<span class="op">^</span><span class="dv">2</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Father&#39;s height (cm)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;Squared residuals&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">sum</span>(m<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>(<span class="dv">173</span><span class="op">-</span><span class="dv">2</span>),<span class="dt">lty=</span><span class="dv">2</span>)

e&lt;-m<span class="op">$</span>residuals
<span class="kw">plot</span>(Galton.sons<span class="op">$</span>father.cm,<span class="kw">abs</span>(e),<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Father&#39;s height (cm)&quot;</span>
     ,<span class="dt">ylab=</span><span class="st">&quot;absolute value of the residuals&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">sum</span>(<span class="kw">abs</span>(m<span class="op">$</span>residuals))<span class="op">/</span>(<span class="dv">173</span><span class="op">-</span><span class="dv">2</span>),<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:GaltonResid2"></span>
<img src="DASM2_files/figure-html/GaltonResid2-1.png" alt="Scatter plots of $e_i^2$ against $x_i$ and of $| e_i |$ against $x_i$ for the Galton example. The horizontal reference lines correspond to MSE (left) and $\frac{1}{n}\sum_{i=1}^n | e_i |$ (right)." width="672" />
<p class="caption">
Figure 2.17: Scatter plots of <span class="math inline">\(e_i^2\)</span> against <span class="math inline">\(x_i\)</span> and of <span class="math inline">\(| e_i |\)</span> against <span class="math inline">\(x_i\)</span> for the Galton example. The horizontal reference lines correspond to MSE (left) and <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n | e_i |\)</span> (right).
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
</div>
</div>
<div id="exercise-muscle-mass-1" class="section level2 unnumbered">
<h2>Exercise: Muscle mass</h2>
<p>Assess the assumption for the regression analysis of the muscle mass example.</p>
<p><details> <summary markdown="span">Try to make this exercise and expand the page to see the soluation.</summary></p>
<p>First we fit the regression model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(muscle.mass<span class="op">~</span>age, <span class="dt">data=</span>muscles)</code></pre></div>
<p><strong>Linearity of the regression model</strong></p>
<p>We can assess the linearity by simple plotting the residuals versus the regressor (age). If there are multiple observations for an age, we can also compute and plot the average rediduals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">e&lt;-m<span class="op">$</span>residuals
x.all&lt;-<span class="kw">unique</span>(muscles<span class="op">$</span>age)
ave.e&lt;-<span class="kw">c</span>()
<span class="cf">for</span>(x <span class="cf">in</span> x.all) {
    ave.e&lt;-<span class="kw">c</span>(ave.e,<span class="kw">mean</span>(e[muscles<span class="op">$</span>age<span class="op">==</span>x]))
}
<span class="kw">plot</span>(muscles<span class="op">$</span>age,e,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Age (years)&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;residuals&quot;</span>)
<span class="kw">points</span>(x.all,ave.e,<span class="dt">col=</span><span class="dv">2</span>,<span class="dt">pch=</span><span class="dv">8</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<p>This graph does not suggest any deviation from linearity because we do not observe a systematic pattern of the (average) residuals versus the age.</p>
<p>One may also add a <em>nonparametric smoother</em> to the residual plot. Although you may perhaps still do not know what is a nonparametric smoother (outside of the scope of this course), you may simply interpret it as a nonparametric estimate of <span class="math inline">\(\E{E \mid x}\)</span>, with <span class="math inline">\(E\)</span> the residual. This is illustrated next.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(muscles<span class="op">$</span>age,e,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Age (years)&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;residuals&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="dv">2</span>)
<span class="kw">lines</span>(<span class="kw">lowess</span>(muscles<span class="op">$</span>age,e))</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>The smooth line again does not suggest a deviation from linearity.</p>
<p><strong>Normality of the error terms</strong></p>
<p>We make a normal QQ-plot of the residuals, as well as a boxplot The normal QQ-plot shows small systematic deviations in the two tails of the distribution, but the shape of the distribution is still quite symmetric (see also the boxplot). Given the moderately large sample size of <span class="math inline">\(n=59\)</span>, we do consider this a problematic deviation from the normality assumption and we can still trust the results for confidence intervals and the hypothesis test (moreover, the <span class="math inline">\(p\)</span>-value is extremely small).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">qqnorm</span>(m<span class="op">$</span>resid,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
       <span class="dt">xlab=</span><span class="st">&quot;expected quantiles&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;residuals&quot;</span>,<span class="dt">main=</span><span class="st">&quot;&quot;</span>)
<span class="kw">qqline</span>(m<span class="op">$</span>resid)

<span class="kw">boxplot</span>(m<span class="op">$</span>residuals, <span class="dt">ylab=</span><span class="st">&quot;residuals&quot;</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p><strong>Homoskedasticity</strong></p>
<p>In the next chunck of R code two plots are produced (squared residuals against regressor and absolute value against regressor). These graphs indicate no violation against the constant-variance assumption.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
e&lt;-m<span class="op">$</span>residuals
<span class="kw">plot</span>(muscles<span class="op">$</span>age,e<span class="op">^</span><span class="dv">2</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Age (years)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;Squared residuals&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">sum</span>(m<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>(<span class="dv">173</span><span class="op">-</span><span class="dv">2</span>),<span class="dt">lty=</span><span class="dv">2</span>)

e&lt;-m<span class="op">$</span>residuals
<span class="kw">plot</span>(muscles<span class="op">$</span>age,<span class="kw">abs</span>(e),<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Age (years)&quot;</span>
     ,<span class="dt">ylab=</span><span class="st">&quot;absolute value of the residuals&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">sum</span>(<span class="kw">abs</span>(m<span class="op">$</span>residuals))<span class="op">/</span>(<span class="dv">173</span><span class="op">-</span><span class="dv">2</span>),<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p></details></p>
</div>
<div id="binary-dummy-regressors" class="section level2">
<h2><span class="header-section-number">2.9</span> Binary Dummy Regressors</h2>
<p>In this section we will demonstrate how the <strong>two-sample problem</strong> is a special case of simple linear regression. In the two sample problem, we are interested in comparing two means. For example, consider the dose finding study, but we only look at the patients that received 0mg (control group) and 2mg per day. We want to know the mean difference in blood pressure reduction between these two doses. Thus we basically have two samples of patients: a sample of 10 patients receiving placebo and a sample of 10 patients receiving 2mg/day. We are not interested in assessing a linear effect of the dose. We now consider the 0mg/day and 2mg/day as two treatments (placebo and active treatment).</p>
<p>More formally, we consider two groups, populations are treatments (whatever term you prefer), say treatment U and treatment V. The interest is in estimating <span class="math display">\[
  \mu_U = \E{Y \mid U} \;\; \text{ and } \;\; \mu_V=\E{Y \mid V}
\]</span> and the treatment <em>effect size</em> <span class="math display">\[
 \delta = \mu_V-\mu_U.
\]</span></p>
<p>Of course we know the solution: <span class="math display">\[
  \hat\mu_U = \bar{Y}_U \;\;\text{ and } \;\; \hat\mu_V=\bar{Y}_V \;\;\text{ and } \hat\delta = \hat\mu_V-\hat\mu_V,
\]</span> in which we used the obvious notation of <span class="math inline">\(\bar{Y}_U\)</span> and <span class="math inline">\(\bar{Y}_V\)</span> representing the sample means of the outcomes in the <span class="math inline">\(U\)</span> and the <span class="math inline">\(V\)</span> sample, respectively. Confidence intervals can be easily obtained as well as hypothesis tests for testing <span class="math inline">\(H_0: \delta=0\)</span> against one-sided or two-sided alternatives (this is the well known two-sample <span class="math inline">\(t\)</span>-test).</p>
<p>For the confidence intervals and two-sample hypothesis test, we often assume normality in the two treatment groups (unless the samples sizes are large), i.e. <span class="math display">\[
  Y_i \mid U \sim N(\mu_U, \sigma^2) \;\;\text{ and }\;\; Y_i \mid V \sim N(\mu_V, \sigma^2).
\]</span></p>
<p>This two-sample problem can also be formulated as a regression model, which will allow is to apply all theory that we have seen before.</p>
Define a <strong>dummy regressor</strong> <span class="math inline">\(x_i\)</span> as
<span class="math display">\[\begin{eqnarray*}
  x_i 
    &amp;=&amp; 1 \text{ if observation i belongs to treatment V} \\
    &amp;=&amp; 0 \text{ if observation i belongs to treatment U} .
\end{eqnarray*}\]</span>
<p>With this definition, we build the conventional regression model, <span class="math display">\[
  Y_i = \beta_0 + \beta_1 x_i + \eps_i
\]</span> with <span class="math inline">\(\eps_i\)</span> i.i.d. <span class="math inline">\(N(0,\sigma^2)\)</span>. Or, equivalently, <span class="math display">\[
 Y_i \mid x_i \sim N(\beta_0+\beta_1 x_i , \sigma^2).
\]</span> Since <span class="math inline">\(x_i\)</span> can take only two values we can explicitely look at the <span class="math inline">\(x_i=1\)</span> and <span class="math inline">\(x_i=0\)</span> possibilities: <span class="math display">\[
  Y_i \mid x_i=0 \sim N(\beta_0 , \sigma^2)
\]</span> and <span class="math display">\[
  Y_i \mid x_i=1 \sim N(\beta_0 + \beta_1 , \sigma^2).
\]</span> Comparing these expressions with our earlier formulation of the two-sample problem, we find for outcomes in treatment group U: <span class="math display">\[
  \mu_U = \E{Y\mid U} = \E{Y \mid x=0} = \beta_0 
\]</span> and for for outcomes in group V: <span class="math display">\[
  \mu_V = \E{Y\mid V} = \E{Y \mid x=1} = \beta_0 +\beta_1.
\]</span> This immediately gives <span class="math display">\[
  \delta=\mu_V-\mu_U = \beta_1.
\]</span> Thus the regression parameter <span class="math inline">\(\beta_1\)</span> in the linear regression model with the 0/1 binary dummy variable can be directly interpreted as the effect size <span class="math inline">\(\delta=\mu_V-\mu_U\)</span>. The methods for confidence intervals and hypothesis testing for the parameter <span class="math inline">\(\beta_1\)</span> can now be directly applied.</p>
<p>Also note that the regression parameter <span class="math inline">\(\beta_0=\mu_U\)</span>.</p>
</div>
<div id="example" class="section level2 unnumbered">
<h2>Example</h2>
Consider now the blood pressure example with only the 0mg/day and 2mg/day observations, and define
<span class="math display">\[\begin{eqnarray*}
  x_i 
    &amp;=&amp; 1 \text{ if observation i belongs to the 2mg/day group} \\
    &amp;=&amp; 0 \text{ if observation i belongs to the 0mg/day group (placebo)} .
\end{eqnarray*}\]</span>
<p>The next chunck of R code shows the analysis with R. We start with subsetting the blood pressure dataset and with defining the binary dummy regressor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># extract the 0mg/day and 2mg/day observations</span>
BloodPressure2&lt;-BloodPressure <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(dose<span class="op">==</span><span class="dv">0</span><span class="op">|</span>dose<span class="op">==</span><span class="dv">2</span>)

<span class="co"># define binary dummy</span>
BloodPressure2<span class="op">$</span>x&lt;-<span class="kw">ifelse</span>(BloodPressure2<span class="op">$</span>dose<span class="op">==</span><span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">0</span>)
<span class="kw">table</span>(BloodPressure2<span class="op">$</span>dose,BloodPressure2<span class="op">$</span>x)</code></pre></div>
<pre><code>##    
##      0  1
##   0 10  0
##   2  0 10</code></pre>
<p>The table demonstrates that we correctly defined the regressor. Now we fit the linear regression model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(bp.reduction<span class="op">~</span>x,<span class="dt">data=</span>BloodPressure2)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = bp.reduction ~ x, data = BloodPressure2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -7.10  -1.75   0.20   1.50   5.90 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)   -0.900      1.168  -0.771  0.45083   
## x              5.400      1.651   3.270  0.00425 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.692 on 18 degrees of freedom
## Multiple R-squared:  0.3727, Adjusted R-squared:  0.3378 
## F-statistic: 10.69 on 1 and 18 DF,  p-value: 0.004252</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m)</code></pre></div>
<pre><code>##                 2.5 %   97.5 %
## (Intercept) -3.353076 1.553076
## x            1.930827 8.869173</code></pre>
<p>And now the same analysis but with the t.test function in R.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(bp.reduction<span class="op">~</span>dose,<span class="dt">data=</span>BloodPressure2,
       <span class="dt">var.equal=</span><span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  bp.reduction by dose
## t = -3.2702, df = 18, p-value = 0.004252
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -8.869173 -1.930827
## sample estimates:
## mean in group 0 mean in group 2 
##            -0.9             4.5</code></pre>
<p>The agreement between the results can be seen directly.</p>
</div>
<div id="exercise-smoking" class="section level2 unnumbered">
<h2>Exercise: Smoking</h2>
<p>We are presented with a sample of 654 youths, aged 3 to 19 years, in the area of East Boston during middle to late 1970's. Interest concerns the relationship between smoking and FEV (forced expiratory volume; it measures how much air a person can exhale during a forced breath; measured in liters). In the dataset, the smoke variable is already coded as a dummy variable: smoke=0 refers to non-smokers and smoke=1 refers to smokers.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fev&lt;-<span class="kw">read.csv</span>(<span class="st">&quot;data/fevdata.txt&quot;</span>,<span class="dt">sep=</span><span class="st">&quot; &quot;</span>)
<span class="kw">names</span>(fev)&lt;-<span class="kw">c</span>(<span class="st">&quot;age&quot;</span>,<span class="st">&quot;fev&quot;</span>,<span class="st">&quot;height&quot;</span>,<span class="st">&quot;sex&quot;</span>,<span class="st">&quot;smoke&quot;</span>)</code></pre></div>
<p>Fit a linear regression model with the dummy <em>smoke</em> as regressor and interpret the model fit (estimated regression coefficient, <span class="math inline">\(95\%\)</span> confidence interval, and hypohtesis test for <span class="math inline">\(H_0:\beta_1=0\)</span> versus <span class="math inline">\(H_1: \beta_1\neq 0\)</span> at the <span class="math inline">\(5\%\)</span> level of significance).</p>
<p><details> <summary markdown="span">Try to make this exercise yourself. If you are ready you can expand this page and look at a solution</summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(fev<span class="op">$</span>fev<span class="op">~</span>fev<span class="op">$</span>smoke, <span class="dt">ylab=</span><span class="st">&quot;FEV (liters)&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;smoke&quot;</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(fev<span class="op">~</span>smoke,<span class="dt">data=</span>fev)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = fev ~ smoke, data = fev)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -10.140  -3.491  -2.093   4.467  13.467 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   5.5329     0.2022   27.36   &lt;2e-16 ***
## smoke         6.5597     0.3820   17.17   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.384 on 651 degrees of freedom
##   (345 observations deleted due to missingness)
## Multiple R-squared:  0.3117, Adjusted R-squared:  0.3107 
## F-statistic: 294.9 on 1 and 651 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m)</code></pre></div>
<pre><code>##                2.5 %   97.5 %
## (Intercept) 5.135845 5.930036
## smoke       5.809588 7.309810</code></pre>
<p>We conclude that at the <span class="math inline">\(5\%\)</span> level of significance smoker have a significant larger FEV (<span class="math inline">\(p&lt;0.001\)</span>). We estimate that on average the FEV of smokers is 6.56 (SE=0.38) liters larger than among the non-smokers. The corresponding <span class="math inline">\(95\%\)</span> confidence interval ranges from <span class="math inline">\(5.81\)</span> to <span class="math inline">\(7.31\)</span>.</p>
<p></details></p>
</div>
<div id="association-versus-causation" class="section level2">
<h2><span class="header-section-number">2.10</span> Association versus Causation</h2>
<div id="introduction" class="section level3">
<h3><span class="header-section-number">2.10.1</span> Introduction</h3>
<p>Recall some of the examples that we have seen before:</p>
<ul>
<li><p>Galton's data: we found a significant positive effect of the height of the father on the average height of their sons <span class="math inline">\((p&lt;0.001\)</span>).</p></li>
<li><p>Blood pressure data: we found a significant positive effect of the dose on the average blood pressure reduction (<span class="math inline">\(p=0.003\)</span>).</p></li>
<li><p>Muscle mass data: we found a significant negative effect of the age on the mean muscle mass (<span class="math inline">\(p&lt;0.001\)</span>).</p></li>
<li><p>FEV data: we found a significant larger FEV among the smokers as compared to the non-smokers (<span class="math inline">\(p&lt;0.001\)</span>)</p></li>
</ul>
<p>The conclusions all refer to <strong>associations</strong> between a regressor and the (mean) outcome, but they do not necessarily imply a <strong>causation</strong>. Think about the following questions:</p>
<ul>
<li><p>Can we conclude that increasing fathers' heights (e.g. by means of a better diet in the fathers' youth) causes the average height of their sons to become larger?</p></li>
<li><p>Can we conclude that increasing the daily dose of the active compound in the blood pressure lowering medication causes the average blood pressure reduction to increase?</p></li>
<li><p>Can we conclude that smoking causes the FEV to become larger on average (i.e. smoking causes the lung function to become better)?</p></li>
</ul>
<p>In this section we will argue that for the blood pressure reduction example, we do have established a causal relationship, but for the other examples we do not have demonstrated causality (only association).</p>
<p>All examples listed above, except for the blood pressure study, are <strong>observational studies</strong>. This means that the regressor <span class="math inline">\(X_i\)</span> and the outcome <span class="math inline">\(Y_i\)</span> variables are sampled together, without the researcher having control over the decision which subject <span class="math inline">\(i\)</span> is assigned to what value of <span class="math inline">\(X_i\)</span>. In the blood pressure study, on the other hand, the research planned how the subjects would be assigned to what daily dose. In particular, the subjects in the study were <strong>randomised</strong> over the doses. This is an example of an <strong>experimental study</strong>. We will demonstrate that in the latter case, associations can be interpreted as causal effects.</p>
</div>
<div id="causal-inference-and-counterfactuals" class="section level3">
<h3><span class="header-section-number">2.10.2</span> Causal inference and counterfactuals</h3>
<p><strong>Causal inference</strong> is a discipline in statistics that aims to develop methods that can be used to assess causal relationships. Although it is only rather recent (last 20 years) that it has become a very active research area, concepts of causality were already proposed by Sir Ronald Fisher and Jerzy Neyman in the 1920s. The modern revival started in the early 1980 with the work of Donald Rubin.</p>
<p>In this section we explain some of the basic concepts of causal inference. As an example we will consider the blood pressure reduction study with only the 0mg/day (placebo) (<span class="math inline">\(X_i=0\)</span>) and the 2mg/day treatment groups (<span class="math inline">\(X_i=1\)</span>) (two-sample problem).</p>
<p>We now define two <strong>counterfactual</strong> outcomes:</p>
<ul>
<li><p><span class="math inline">\(Y_i(1)\)</span> is the outcome of subject <span class="math inline">\(i\)</span> if subject <span class="math inline">\(i\)</span> would receive treatment <span class="math inline">\(X_i=1\)</span></p></li>
<li><p><span class="math inline">\(Y_i(0)\)</span> is the outcome of subject <span class="math inline">\(i\)</span> if subject <span class="math inline">\(i\)</span> would receive treatment <span class="math inline">\(X_i=0\)</span>.</p></li>
</ul>
<p>In reality a subject can only receice a single treatment, either <span class="math inline">\(X_i=1\)</span> or <span class="math inline">\(X_i=0\)</span>, and hence both counterfactuals can never be observed simultaneously. Still we can think of <span class="math inline">\(Y_i(1)\)</span> and <span class="math inline">\(Y_i(0)\)</span> as inherent properties of subject <span class="math inline">\(i\)</span>.</p>
<p>Thus, what we observe is <span class="math inline">\((X_i,Y_i)\)</span>, with <span class="math inline">\(X_i\)</span> (0 or 1) the actual treatment that is received by subject <span class="math inline">\(i\)</span> and <span class="math inline">\(Y_i\)</span> the observed outcome under treatment <span class="math inline">\(X_i\)</span>.</p>
<p>In the context of the blood pressure example:</p>
<ul>
<li><p><span class="math inline">\(Y_i(0)\)</span> is the blood pressure reduction if subject <span class="math inline">\(i\)</span> would have received placebo</p></li>
<li><p><span class="math inline">\(Y_i(1)\)</span> is the blood pressure reduction if subject <span class="math inline">\(i\)</span> would have received 2mg/day.</p></li>
</ul>
<p>Thus conceptually we can also think of <span class="math inline">\(Y_i(1)-Y_i(0)\)</span> as the <strong>causal effect</strong> of the treatment for subject <span class="math inline">\(i\)</span>. This could then serve as the basis for testing the following null hypothesis: <span class="math display">\[
  H_0: Y_i(1)=Y_i(0) \;\;\text{ for all subjects i in the population}.
\]</span> This is known as the <strong>sharp causal null hypothesis</strong>. However, we will be interested in the <strong>average causal effect</strong>, defined in terms of population averages: <span class="math display">\[
  \E{Y(1)-Y(0)}=\E{Y(1)}- \E{Y(0)}.
\]</span> In the remainder of this course, we will simply call this the <strong>causal effect</strong>. In the context of the two-sample problem, it is also known as the <strong>average treatment effect</strong> (ATE).</p>
<p>The question is now: do we have unbiased estimators of <span class="math inline">\(\E{Y(1)}\)</span> and <span class="math inline">\(\E{Y(0)}\)</span>? If yes, we also have an unbiased estimator of the causal effect. It may be tempting to consider <span class="math inline">\(\bar{Y}_1\)</span> (sample mean of outcomes in the <span class="math inline">\(X=1\)</span> group) as an estimator of <span class="math inline">\(\E{Y(1)}\)</span>, and <span class="math inline">\(\bar{Y}_0\)</span> (sample mean of outcomes in the <span class="math inline">\(X=0\)</span> group) as an estimator of <span class="math inline">\(\E{Y(0)}\)</span>. We will show that this holds true for studies that involve complete randomisation, but it does not hold generally.</p>
<p>Suppose we would use <span class="math inline">\(\bar{Y}_a\)</span> (<span class="math inline">\(a=0,1\)</span>). What is the estimand of this estimator (i.e. what population parameter is estimated by the estimator). Thus we need to look at <span class="math inline">\(\E{\bar{Y}_a}\)</span>. To be more precise, we write <span class="math inline">\(\Ef{XY}{\bar{Y}_a}\)</span> to stress that both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> may be random.</p>
<p>Let <span class="math inline">\(N_a\)</span> denote the number of sample observations in treatment group <span class="math inline">\(a\)</span>, which can be written as</p>
<span class="math display">\[
  N_1 = \sum_{i=1}^n X_i  \;\;\text{ and }\;\; N_0 = \sum_{i=1}^n (1-X_i).
\]</span> Then,
<span class="math display">\[\begin{eqnarray*}
  \Ef{XY}{\bar{Y}_a}
    &amp;=&amp; \Ef{XY}{\frac{1}{N_a}\sum_{i: X_i=a} Y_i} \\
    &amp;=&amp; \Ef{X}{\Ef{Y\mid X}{\frac{1}{N_a}\sum_{i: X_i=a} Y_i}} \\
    &amp;=&amp; \Ef{X}{\frac{1}{N_a} \sum_{i: X_i=a} \Ef{Y\mid X}{Y_i \mid X_i=a}} \\
    &amp;=&amp; \Ef{X}{\frac{1}{N_a} N_a\Ef{Y\mid X}{Y \mid X=a}} \\
    &amp;=&amp; \Ef{Y\mid X}{Y \mid X=a}.
\end{eqnarray*}\]</span>
<p>Hence, for <span class="math inline">\(\bar{Y}_a\)</span> to be an unbiased estimator of <span class="math inline">\(\E{Y(a)}\)</span>, we need to demonstrate that <span class="math display">\[
  \E{Y \mid X=a} = \E{Y(a)}.
\]</span> This is also illustrated in Figure <a href="#fig:Causal1">2.18</a></p>
<div class="figure" style="text-align: center"><span id="fig:Causal1"></span>
<img src="figures/Causal1.pdf" alt="Difference between means of counterfactuals and conditional means (Figure 1.1. from Hern\'an and Robins, Causal Inference: What if, 2020)."  />
<p class="caption">
Figure 2.18: Difference between means of counterfactuals and conditional means (Figure 1.1. from Hern'an and Robins, Causal Inference: What if, 2020).
</p>
</div>
<p>We will need the following assumptions:</p>
<ul>
<li><p><strong>consistency</strong>: For all subjects <span class="math inline">\(i\)</span>, it must hold that <span class="math inline">\(X_i=a\)</span> implies that <span class="math inline">\(Y_i(a)=Y_i\)</span>. In other words, when a subject receives treatment a, the observed outcome of the subject must be equal to the counterfacual for that treatment.</p></li>
<li><p><strong>mean exchangeability</strong>: <span class="math inline">\(\E{Y(a) \mid X=0} = \E{Y(a) \mid X=1} = \E{Y(a)}\)</span>, for <span class="math inline">\(a=0/1\)</span>. This condition tells us that the populations of subjects that receive placebo (<span class="math inline">\(X=0\)</span>) and that receive treatment (<span class="math inline">\(X=1\)</span>), have the same mean value for the counterfactual <span class="math inline">\(Y(a)\)</span> for treatment <span class="math inline">\(a\)</span> (<span class="math inline">\(a=0/1\)</span>), and, as a consequence these two conditional means are equal to the marginal mean <span class="math inline">\(\E{Y(a)}\)</span>. This means that the treatment allocation <span class="math inline">\(X\)</span> is not associated with the mean counterfactual outcomes.</p></li>
</ul>
<p>A property that is even stronger than mean exchangeability, is <strong>full exchangeability</strong>. This states that the treatment assignment <span class="math inline">\(X\)</span> is stochastically indepedent of the vector of counterfactual outcomes <span class="math inline">\((Y(0),Y(1))\)</span>, i.e. <span class="math display">\[
  (Y(0), Y(1)) \ind X.
\]</span> This implies that the treatment <span class="math inline">\(X_i\)</span> given to subject <span class="math inline">\(i\)</span>, is jointly independent of the counterfactual outcomes <span class="math inline">\(Y_i(0)\)</span> and <span class="math inline">\(Y_i(1)\)</span>. Full exchangeability implies mean exchangeability.</p>
<p>Now we are ready to prove the identity <span class="math inline">\(\E{Y \mid X=a} = \E{Y(a)}\)</span>. For <span class="math inline">\(a=0,1\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\E{Y(a)}=\E{Y(a) \mid X=a}\)</span> (implied by mean exchangeability)</p></li>
<li><p><span class="math inline">\(\E{Y(a) \mid X=a} = \E{Y \mid X=a}\)</span> (implied by consistency).</p></li>
</ol>
</div>
<div id="randomised-studies" class="section level3">
<h3><span class="header-section-number">2.10.3</span> Randomised studies</h3>
<p>Full exchangeability is implied by a <strong>completely randomised study</strong>, i.e. a study in which subjects are randomly assigned to the treatment groups. For example, the binary treatment indicator can be Bernoulli distributed with <span class="math display">\[
  \prob{X_i=1}=\frac{1}{2}.
\]</span> This may come with or without the extra condition <span class="math inline">\(\sum_{i=1}^n X_i=n_1\)</span> (fixed group sample sizes). This randomisation scheme is independent of the counterfactual outcomes and hence it implies full exchangeability.</p>
<p>Recall that the blood pressure reduction study is a randomised study: 20 subjects were randomised over the placebo and 2mg/day treatment. The conclusions that we have reached in our previous statistical analysis can thus be causally interpreted. The difference in sample means is thus an estimate of the ATE.</p>
<p>The reasoning that we have build in this section does not only apply to 2 treatment groups. It can easily be extended to more than 2 treatment groups or to the setting of linear regression.</p>
<p>Let us reconsider a few earlier examples:</p>
<ul>
<li><p>The full blood pressure reduction study, with the 4 treatment groups, that was previously analysed with a linear regression analysis also allows for causal conclusions, because all 40 students were randomised over the 4 treatment groups (doses). It is still a randomised study.</p></li>
<li><p>The study on the effect of smoking on the FEV consisted of a sample of 654 subjects, and for these subjects the smoking status (<span class="math inline">\(X_i\)</span>) and the FEV outcome (<span class="math inline">\(Y_i\)</span>) were observed. At best the sample may be a random sample from a larger population, but the <em>treatment</em> `smoking' was not randomly assigned to the subjects. This would of course not be ethical! So this is an <strong>observational study</strong>, and the full or mean exchangeability condition is not satisfied. Our conclusion that the average FEV is larger among the smokers than among the non-smokers (suggesting that smoking is beneficial for the lung fuction) is thus not a causal conclusion. Modern causal inference methods provide other data analysis methods that could allow for causal conclusions in observational studies (outside the scope of this course).</p></li>
<li><p>The study designs of the Galton and the muscle mass example are also observational, and hence the conclusions for these studies are also only in terms of association rather than in terms of causation.</p></li>
</ul>
<p>To conclude we give an example that demonstrates how the full exchangeability condition can be violated, even in experimental studies (but without proper randomisation).</p>
<hr />

</div>
</div>
</div>
<div id="reporting" class="section level1">
<h1><span class="header-section-number">Hoofdstuk 3</span> Reporting</h1>
<p>Here we provide some general guidelines for reporting the statistical results of a linear regression analysis. Many of these guidelines are more generally applicable to reporting other types of statistical analysis.</p>
<ul>
<li><p>Always report the study design. This is extremely important to determine what statistical methods can be applied in a valid way, and what kind of conclusions are allowed (e.g. causal conclusion or not).</p></li>
<li><p>Always report the research question.</p></li>
<li><p>It is strongly recommended to always start with a data exploration. If strange or unexpected observations are detected, then identify the observations are report them. It is not up to the statistician to decide to remove observations from the analysis! In no case the dataset must be adapted to the statistical method (by e.g. removing outliers), but in any case the statistical method must be chosen so that the method is valid and can produce an answer to the research question.</p></li>
<li><p>Always report results in their correct units. E.g. in the blood pressure example we have estimated <span class="math inline">\(\beta_1\)</span> as <span class="math inline">\(1.79\)</span>. Thus we say write <span class="math inline">\(\hat\beta_1=1.79\)</span> mmHg / mg/day, or in words: we estimate that the blood pressure reduces on average with <span class="math inline">\(1.79\)</span> mmHg per increase of of the daily dose with 1 mg.</p></li>
<li><p>Always report estimates with confidence intervals. If the confidence interval is centered at the estimate <span class="math inline">\(\hat\beta_1=1.79\)</span> and stretches <span class="math inline">\(0.35\)</span> to either side, then the best way to report to the confidence interval is <span class="math display">\[
  (1.44 \text{ to } 2.14) \text{ mmHg / mg/day}
\]</span> Some bad examples: <span class="math inline">\((1.44 - 2.14)\)</span> mmHg / mg/day, or (1.44, 2.14) mmHg/ mg/day, or <span class="math inline">\(1.79 \pm 0.35\)</span> mmHg / mg/day.</p></li>
<li><p>When reporting confidence intervals, the nominal coverage should also be mentioned (e.g. <span class="math inline">\(95\%\)</span> or <span class="math inline">\(90\%\)</span> confidence interval).</p></li>
<li><p>Standard errors of estimates may be reported as e.g. <span class="math inline">\(1.79\)</span> (SE=<span class="math inline">\(0.17\)</span>) mmHg / mg/day, or <span class="math inline">\(1.79\)</span> mmHg / mg/day (SE=<span class="math inline">\(0.17\)</span> mmHg / mg/day).</p></li>
<li><p>When reporting the results of a hypothesis test, always mention the null and alternative hypothesis, as well as the significance level and the <span class="math inline">\(p\)</span>-value. A hypothesis test is seldom to be performed without also estimating a population parameter (e.g. an effect size or a regression coefficient). The estimate (with confidence interval) should also be reported to complement the interpretation of the hypothesis test.</p></li>
<li><p>Make sure that you have formulated an answer to the original research question. If this is not possible (e.g. inconclusive results) then also report this.</p></li>
<li><p>The report must be written in neat English. The formulation of a conclusion from a statistical analysis must be very precise and with appropriate nuance.</p></li>
<li><p>Never copy-and-paste software output to the report, but instead extract the relevant information from the output and place it in the report in either the text or in a nicely formatted table. You may list the software output in the appendix. Exception: when the report is meant to be read by other statisticians or data scientists, you may show them the software output (as for example in these course notes).</p></li>
</ul>

</div>



<div id="app:VecDiff" class="section level1">
<h1><span class="header-section-number">A</span> Vector Differentiation</h1>
<p>For <span class="math inline">\(\mb\beta^t = (\beta_0, \ldots, \beta_{p-1})\)</span>,</p>
<span class="math display">\[\begin{eqnarray*}
  \frac{d}{d\beta} = \left(\begin{array}{c}
    \frac{\partial}{\partial \beta_0}\\
    \vdots\\
    \frac{\partial}{\partial \beta_{p-1}}
    \end{array}\right).
\end{eqnarray*}\]</span>
<p>Upon using this notation, the following differentiation rules hold for <span class="math inline">\(\mb{a}\)</span> (a <span class="math inline">\((p\times 1)\)</span> vector) and <span class="math inline">\(\mb{A}\)</span> (a symmetric <span class="math inline">\((p\times p)\)</span> matrix):</p>
<ul>
<li><p><span class="math inline">\(\frac{d(\mb\beta^t \mb{a})}{d\mb\beta} = \frac{d(\mb{a}^t\mb\beta)}{d\mb\beta} = \mb{a}\)</span></p></li>
<li><p><span class="math inline">\(\frac{d(\mb\beta^t\mb{A\beta})}{d\mb\beta} = 2\mb{A\beta}\)</span>.</p></li>
</ul>
</div>
<div id="app:LinTrans" class="section level1">
<h1><span class="header-section-number">B</span> Linear Transformations of MVN</h1>

<div class="lemma">
<span id="lem:LinTransNorm" class="lemma"><strong>Lemma B.1  (Linear transformation of MVN)  </strong></span>If <span class="math inline">\(\mb{Z}\sim \text{MVN}(\mb\mu,\mb\Sigma)\)</span> (<span class="math inline">\(n\)</span>-dimensional stochastic vector), and if <span class="math inline">\(\mb{A}\)</span> is a constant <span class="math inline">\(p\times n\)</span> matrix <span class="math inline">\(\mb{A}\)</span> of rank <span class="math inline">\(p\)</span> and <span class="math inline">\(\mb{a}\)</span> a constant <span class="math inline">\(p\times 1\)</span> vector <span class="math inline">\(\mb{a}\)</span>, then it holds that <span class="math display">\[ 
  \mb{AZ}+\mb{a} \sim \text{MVN}(\mb{A\mu}+\mb{a},\mb{A\Sigma}\mb{A}^t).
\]</span>
</div>

</div>
<div id="app:Slutsky" class="section level1">
<h1><span class="header-section-number">C</span> Slutsky's Theorem</h1>

<div class="theorem">
<span id="thm:Slutsky" class="theorem"><strong>Theorem C.1  (Slutsky's Theorem)  </strong></span>Consider two sequences of stochastic variables, <span class="math inline">\(X_1,\ldots, X_n\)</span> and <span class="math inline">\(Y_1,\ldots, Y_n\)</span>. Assume that, as <span class="math inline">\(n\rightarrow \infty\)</span>, <span class="math display">\[
   X_n \convDistr X \;\;\;\text{ and }\;\;\; Y_n \convProb c
 \]</span> with <span class="math inline">\(X\)</span> a stochastic variable and <span class="math inline">\(c\)</span> a constant. Then, it holds that
<span class="math display">\[\begin{eqnarray*}
  X_n + Y_n &amp;\convDistr&amp; X+c \\
  X_n Y_n &amp;\convDistr&amp; cX \\
  X_n / Y_n &amp;\convDistr&amp; X/c \;\;\;\text{(if }1/c\text{ exists)}.
 \end{eqnarray*}\]</span>
</div>

</div>
<div id="types-of-statistical-models" class="section level1">
<h1><span class="header-section-number">D</span> Types of Statistical Models</h1>
<p>We introduce the concept of a statistical model in the setting of a single outcome <span class="math inline">\(Y\)</span> and a single regressor <span class="math inline">\(x\)</span>.</p>
<p>Given the regressor <span class="math inline">\(x\)</span>, the outcome can be described by the conditional distribution. We use the notation <span class="math display">\[
  Y \mid x
\]</span> to refer to the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x\)</span>. The corresponding distribution function is denoted by <span class="math inline">\(F(y\mid x)\)</span> and the density function <span class="math inline">\(f(y\mid x)\)</span>.</p>
<p>Without any further restrictions on this conditional distribution, we say that it is a {}. However, often we imply further restrictions on the conditional distribution. Let us start with a simple example. Suppose that <span class="math inline">\(x\)</span> is a 0/1 indicator (0: placebo and 1: active treatment). Then we can write <span class="math inline">\(Y \mid x=0 \sim F(y\mid x=0)\)</span> and <span class="math inline">\(Y \mid x=1 \sim F(y\mid x=1)\)</span>. But again, without any further restrictions, this is a nonparametric model, because it only says that for <span class="math inline">\(x=0\)</span> and <span class="math inline">\(x=1\)</span> the distribution of <span class="math inline">\(Y\)</span> is given by two distinct distribution function, but there is no relationship between the distributions assumed. Even in this nonparametric model, we can identify parameters. For example, <span class="math display">\[
  \mu_0 = E(Y \mid x=0) = \int y f(y\mid x=0) dy  
\]</span> and <span class="math display">\[
  \mu_1 = E(Y \mid x=1) = \int y f(y\mid x=1) dy.
\]</span> So we have two meaningful parameters, but they do not impose any restrictions on <span class="math inline">\(f(y\mid x=0)\)</span> and <span class="math inline">\(f(y\mid x=1)\)</span>. These nonparametric models can be represented by the set of all proper conditional distribution functions. For the example with <span class="math inline">\(x=0/1\)</span> this can be written as <span class="math display">\[
  \left\{ F(\cdot \mid x): F(\cdot \mid x) \text{ is a distribution function}, x\in \{0,1\} \right\}.
\]</span></p>
<p>The model can be turned into a {}, by imposing strong distributional assumptions on <span class="math inline">\(f(y\mid x=0)\)</span> and <span class="math inline">\(f(y\mid x=1)\)</span>. For example, (<span class="math inline">\(x=0,1\)</span>) <span class="math display">\[
  Y \mid x \sim N(\mu_x, \sigma^2).
\]</span> This model says that when <span class="math inline">\(x=0\)</span>, the outcome has a normal distribution with mean <span class="math inline">\(\mu_0\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, and when <span class="math inline">\(x=1\)</span>,the outcome has a normal distribution with mean <span class="math inline">\(\mu_1\)</span> and also variance <span class="math inline">\(\sigma^2\)</span>. Thus the parameters <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\mu_1\)</span> have the same interpretation as in the nonparametric model, but now the distributions are explicitly specified and there is the additional restriction that the variances in the two <span class="math inline">\(x=0/1\)</span> groups are identical.</p>
<p>Another example of a parametric model is the simple linear regression model. Now <span class="math inline">\(x\)</span> is a continuous regressor. We write the models as <span class="math display">\[
   Y \mid x \sim N(\beta_0 +\beta_1 x, \sigma^2).
\]</span> Thus, for each <span class="math inline">\(x\)</span> the model completely specifies the distribution of the outcome. The model specification also included a description of how the mean of the outcome varies with <span class="math inline">\(x\)</span>.</p>
<p>Let us write this parametric model in a more generic way. Let <span class="math inline">\(\theta\)</span> denote the parameter vector with <span class="math inline">\(p\)</span> elements and let <span class="math inline">\(F_\theta(y \mid)\)</span> denote the distribution function of the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x\)</span>. This distribution is thus completely known, up to some parameter <span class="math inline">\(\theta\)</span> that has to be estimated from the data. If <span class="math inline">\(\theta\)</span> contains real-values parameter, we have <span class="math inline">\(\theta \in \mathbb{R}^p\)</span>. The statistical model can then be represented by the following set of conditional distribution functions, <span class="math display">\[
  \left\{ F(\cdot\mid x) :  F(\cdot\mid x)=F_\theta(\cdot\mid x), \theta\in\mathbb{R}^p, x\in \mathbb{R} \right\}.
\]</span></p>
<p>We can also define {} models. These are models that do not complete specify the conditional distribution, but still they imply restrictions. For example, we could impose the following linear model for the conditional mean, <span class="math display">\[
  E(Y \mid x) = \beta_0 + \beta_1 x
\]</span> without any further restrictions on the conditional distribution. This is not a nonparametric model, because there is a restriction on some aspects of the distribution (here: the conditional mean). It is neither a parametric model, because up to the parameter <span class="math inline">\(\theta^t=(\beta_0,\beta_1)\)</span>, the distribution is not completely specified. This is an example of a {}. It can be represented by the following set of conditional distribution functions, <span class="math display">\[
  \left\{ F(\cdot\mid x) :  E_F(Y\mid x)=\beta_0 + \beta_1 x, (\beta_0,\beta_1) \in\mathbb{R}^2, x\in \mathbb{R} \right\},
\]</span> in which the subscript <span class="math inline">\(F\)</span> in <span class="math inline">\(E_F(Y\mid x)\)</span> is used to stress that the expectation is defined w.r.t. the distribution function <span class="math inline">\(F\)</span>.</p>
<p>Semiparametric models are not limited to include restrictions on the conditional mean. It could for example also involve restrictions on the variance, <span class="math display">\[
  \left\{ F(\cdot\mid x) :  E_F(Y\mid x)=\beta_0 + \beta_1 x,  \text{ and } \text{Var}(Y\mid x)=\sigma^2, (\beta_0,\beta_1) \in\mathbb{R}^2, \sigma^2 \in \mathbb{R}^+, x\in \mathbb{R} \right\}.
\]</span></p>
<p>So far we have always looked at conditional distribution functions, i.e. <span class="math inline">\(x\)</span> is considered a fixed constant. Sometimes <span class="math inline">\(x\)</span> can be random as well. For example, reconsider the example with <span class="math inline">\(x\)</span> a binary 0/1 indicator for two treatments. In a randomised clinical trial, the treatment allocation happens completely at random. Thus <span class="math inline">\(x\)</span> is a random variable; it is thus better to write it as the capital letter <span class="math inline">\(X\)</span>. In this example, <span class="math display">\[
  X \sim \text{Binom}(1,1/2).
\]</span> This marginal distribution for <span class="math inline">\(X\)</span>, together with the model for the conditional distribution of <span class="math inline">\(Y\mid X\)</span>, specifies the joint statistical model of <span class="math inline">\((Y,X)\)</span>.</p>

</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DASM2.pdf"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
