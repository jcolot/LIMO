<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Linear Models</title>
  <meta name="description" content="Course notes of LIMO" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Linear Models" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course notes of LIMO" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Linear Models" />
  
  <meta name="twitter:description" content="Course notes of LIMO" />
  

<meta name="author" content="Olivier Thas" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path=""><a href="#Ch_Introduction"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path=""><a href="#versions-and-changes"><i class="fa fa-check"></i><b>1.1</b> Versions and Changes</a></li>
<li class="chapter" data-level="1.2" data-path=""><a href="#introduction"><i class="fa fa-check"></i><b>1.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3" data-path=""><a href="#how-to-work-through-the-course-notes"><i class="fa fa-check"></i><b>1.3</b> How to work through the course notes</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#exercise"><i class="fa fa-check"></i>Exercise</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path=""><a href="#two-versions-of-this-course-oc-and-dl"><i class="fa fa-check"></i><b>1.4</b> Two versions of this course: OC and DL</a></li>
<li class="chapter" data-level="1.5" data-path=""><a href="#communication"><i class="fa fa-check"></i><b>1.5</b> Communication</a></li>
<li class="chapter" data-level="1.6" data-path=""><a href="#software"><i class="fa fa-check"></i><b>1.6</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path=""><a href="#Ch:Reg1"><i class="fa fa-check"></i><b>2</b> Simple Linear Regression Analysis</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height-data"><i class="fa fa-check"></i>Example (Galton's height data)</a></li>
<li class="chapter" data-level="2.1" data-path=""><a href="#S:RegSimStudy"><i class="fa fa-check"></i><b>2.1</b> Interpretation via simulations</a></li>
<li class="chapter" data-level="2.2" data-path=""><a href="#S:LSE1"><i class="fa fa-check"></i><b>2.2</b> Least squares estimators</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height-data-1"><i class="fa fa-check"></i>Example (Galton's height data)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-blood-pressure"><i class="fa fa-check"></i>Exercise: blood pressure</a></li>
<li class="chapter" data-level="2.3" data-path=""><a href="#S:PropLSE"><i class="fa fa-check"></i><b>2.3</b> Properties of the Least Squares Estimator</a><ul>
<li class="chapter" data-level="2.3.1" data-path=""><a href="#mean-and-variance-of-the-lse"><i class="fa fa-check"></i><b>2.3.1</b> Mean and variance of the LSE</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-simulation-study"><i class="fa fa-check"></i>Exercise: simulation study</a><ul>
<li class="chapter" data-level="2.3.2" data-path=""><a href="#best-linear-unbiased-estimator-blue"><i class="fa fa-check"></i><b>2.3.2</b> Best Linear Unbiased Estimator (BLUE)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-simulation-study-1"><i class="fa fa-check"></i>Exercise: Simulation study</a><ul>
<li class="chapter" data-level="2.3.3" data-path=""><a href="#sampling-distribution-of-the-lse"><i class="fa fa-check"></i><b>2.3.3</b> Sampling distribution of the LSE</a></li>
<li class="chapter" data-level="2.3.4" data-path=""><a href="#maximum-likelihood-estimator-of-hatmbbeta"><i class="fa fa-check"></i><b>2.3.4</b> Maximum likelihood estimator of <span class="math inline">\(\hat{\mb\beta}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path=""><a href="#an-estimator-of-sigma2"><i class="fa fa-check"></i><b>2.4</b> An Estimator of <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-simulation-study-2"><i class="fa fa-check"></i>Exercise: Simulation study</a></li>
<li class="chapter" data-level="2.5" data-path=""><a href="#sampling-distributions-of-the-standardised-and-the-studentised-lse"><i class="fa fa-check"></i><b>2.5</b> Sampling Distributions of the Standardised and the Studentised LSE</a></li>
<li class="chapter" data-level="2.6" data-path=""><a href="#S:BIReg1"><i class="fa fa-check"></i><b>2.6</b> Confidence Intervals</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height-data-2"><i class="fa fa-check"></i>Example (Galton's height data)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-blood-pressure-1"><i class="fa fa-check"></i>Exercise: Blood Pressure</a></li>
<li class="chapter" data-level="2.7" data-path=""><a href="#S:RegTests"><i class="fa fa-check"></i><b>2.7</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height-data-3"><i class="fa fa-check"></i>Example (Galton's height data)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-converting-two-sided-p-values"><i class="fa fa-check"></i>Exercise: Converting two-sided p-values</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-muscle-mass"><i class="fa fa-check"></i>Exercise: Muscle mass</a></li>
<li><a href="#exercise-choosing-h_1-after-looking-at-the-data">Exercise: choosing <span class="math inline">\(H_1\)</span> after looking at the data</a></li>
<li class="chapter" data-level="2.8" data-path=""><a href="#S:AssessAssumptions"><i class="fa fa-check"></i><b>2.8</b> Assessment of the Model Assumptions</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height-data-4"><i class="fa fa-check"></i>Example (Galton's height data)</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#normality-of-the-error-term"><i class="fa fa-check"></i>Normality of the error term</a></li>
<li class="chapter" data-level="" data-path=""><a href="#homoskedasticity"><i class="fa fa-check"></i>Homoskedasticity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-muscle-mass-1"><i class="fa fa-check"></i>Exercise: Muscle mass</a></li>
<li class="chapter" data-level="2.9" data-path=""><a href="#binary-dummy-regressors"><i class="fa fa-check"></i><b>2.9</b> Binary Dummy Regressors</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example"><i class="fa fa-check"></i>Example</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-smoking"><i class="fa fa-check"></i>Exercise: Smoking</a></li>
<li class="chapter" data-level="2.10" data-path=""><a href="#S:Causality"><i class="fa fa-check"></i><b>2.10</b> Association versus Causation</a><ul>
<li class="chapter" data-level="2.10.1" data-path=""><a href="#introduction"><i class="fa fa-check"></i><b>2.10.1</b> Introduction</a></li>
<li class="chapter" data-level="2.10.2" data-path=""><a href="#causal-inference-and-counterfactuals"><i class="fa fa-check"></i><b>2.10.2</b> Causal inference and counterfactuals</a></li>
<li class="chapter" data-level="2.10.3" data-path=""><a href="#randomised-studies"><i class="fa fa-check"></i><b>2.10.3</b> Randomised studies</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path=""><a href="#S:Reg1BIPI"><i class="fa fa-check"></i><b>2.11</b> Estimation of the Conditional Mean Outcome</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-blood-pressure"><i class="fa fa-check"></i>Example (Blood Pressure)</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#sampling-distribution"><i class="fa fa-check"></i>Sampling Distribution</a></li>
<li class="chapter" data-level="" data-path=""><a href="#confidence-interval-ci"><i class="fa fa-check"></i>Confidence Interval (CI)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-blood-pressure-1"><i class="fa fa-check"></i>Example (Blood Pressure)</a></li>
<li class="chapter" data-level="2.12" data-path=""><a href="#S:PI"><i class="fa fa-check"></i><b>2.12</b> Predictions and Prediction Intervals (PI)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-blood-pressure-2"><i class="fa fa-check"></i>Example (Blood Pressure)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-blood-pressure-3"><i class="fa fa-check"></i>Example (Blood Pressure)</a><ul>
<li class="chapter" data-level="2.12.1" data-path=""><a href="#simulation-study"><i class="fa fa-check"></i><b>2.12.1</b> Simulation Study</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-galtons-data"><i class="fa fa-check"></i>Exercise: Galton's data</a></li>
<li class="chapter" data-level="2.13" data-path=""><a href="#decomposition-of-the-total-sum-of-squares"><i class="fa fa-check"></i><b>2.13</b> Decomposition of the Total Sum of Squares</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height"><i class="fa fa-check"></i>Example (Galton's height)</a></li>
<li><a href="#exercise-r2-and-prediction">Exercise: <span class="math inline">\(R^2\)</span> and prediction</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path=""><a href="#Ch:Reg2"><i class="fa fa-check"></i><b>3</b> Multiple Linear Regression Analysis</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration"><i class="fa fa-check"></i>Example (Lead concentration)</a></li>
<li class="chapter" data-level="3.1" data-path=""><a href="#S:AddMeervoudigModel"><i class="fa fa-check"></i><b>3.1</b> The Additive Multiple Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#the-statistical-model"><i class="fa fa-check"></i>The Statistical Model</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-1"><i class="fa fa-check"></i>Example (Lead concentration)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-lead-concentration"><i class="fa fa-check"></i>Exercise: Lead concentration</a></li>
<li class="chapter" data-level="3.2" data-path=""><a href="#the-non-additive-multiple-linear-regression-model"><i class="fa fa-check"></i><b>3.2</b> The Non-Additive Multiple Linear Regression Model</a><ul>
<li class="chapter" data-level="3.2.1" data-path=""><a href="#interaction"><i class="fa fa-check"></i><b>3.2.1</b> Interaction</a></li>
<li class="chapter" data-level="3.2.2" data-path=""><a href="#parameter-estimators"><i class="fa fa-check"></i><b>3.2.2</b> Parameter Estimators</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-2"><i class="fa fa-check"></i>Example (Lead concentration)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-blood-pressure"><i class="fa fa-check"></i>Example (Blood Pressure)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-blood-pressure"><i class="fa fa-check"></i>Exercise: Blood Pressure</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-3"><i class="fa fa-check"></i>Example (Lead concentration)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-lead-concentration-1"><i class="fa fa-check"></i>Exercise: Lead concentration</a></li>
<li class="chapter" data-level="3.3" data-path=""><a href="#the-anova-table"><i class="fa fa-check"></i><b>3.3</b> The ANOVA Table</a><ul>
<li class="chapter" data-level="3.3.1" data-path=""><a href="#sstot-ssr-and-sse"><i class="fa fa-check"></i><b>3.3.1</b> SSTot, SSR and SSE</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path=""><a href="#multicollinearity"><i class="fa fa-check"></i><b>3.4</b> Multicollinearity</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-4"><i class="fa fa-check"></i>Example (Lead concentration)</a><ul>
<li class="chapter" data-level="3.4.1" data-path=""><a href="#illustrations-via-simulations"><i class="fa fa-check"></i><b>3.4.1</b> Illustrations via Simulations</a></li>
</ul></li>
<li><a href="#excercise-repeatedly-fitting-model-with-rho0.99">Excercise: repeatedly fitting model with <span class="math inline">\(\rho=0.99\)</span></a><ul>
<li class="chapter" data-level="3.4.2" data-path=""><a href="#variance-inflation-factor"><i class="fa fa-check"></i><b>3.4.2</b> Variance Inflation Factor</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-5"><i class="fa fa-check"></i>Example (Lead concentration)</a></li>
<li class="chapter" data-level="3.5" data-path=""><a href="#leverage"><i class="fa fa-check"></i><b>3.5</b> Leverage</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-6"><i class="fa fa-check"></i>Example (Lead concentration)</a></li>
<li class="chapter" data-level="3.6" data-path=""><a href="#assessment-of-the-model-assumptions-and-remedial-measures"><i class="fa fa-check"></i><b>3.6</b> Assessment of the Model Assumptions and Remedial Measures</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#residual-analysis"><i class="fa fa-check"></i>Residual analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-7"><i class="fa fa-check"></i>Example (Lead Concentration)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-lead-concentration-2"><i class="fa fa-check"></i>Exercise: Lead concentration</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-blood-pressure-1"><i class="fa fa-check"></i>Exercise: Blood pressure</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-bacterial-count"><i class="fa fa-check"></i>Example (Bacterial count)</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#remedial-measures"><i class="fa fa-check"></i>Remedial measures</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-bacterial-count-1"><i class="fa fa-check"></i>Example (Bacterial count)</a></li>
<li class="chapter" data-level="3.7" data-path=""><a href="#model-selection"><i class="fa fa-check"></i><b>3.7</b> Model Selection</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#selection-methods-based-on-hypothesis-testing"><i class="fa fa-check"></i>Selection methods based on hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-8"><i class="fa fa-check"></i>Example (Lead concentration)</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#model-selection-for-building-a-prediction-model"><i class="fa fa-check"></i>Model selection for building a prediction model</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-lead-concentration-3"><i class="fa fa-check"></i>Exercise: Lead concentration</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path=""><a href="#Ch:DesignCausal"><i class="fa fa-check"></i><b>4</b> Design-related Topics and Causal Inference</a><ul>
<li class="chapter" data-level="4.1" data-path=""><a href="#confounding"><i class="fa fa-check"></i><b>4.1</b> Confounding</a><ul>
<li class="chapter" data-level="4.1.1" data-path=""><a href="#causal-diagrams-and-controling-for-confounders"><i class="fa fa-check"></i><b>4.1.1</b> Causal diagrams and controling for confounders</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration"><i class="fa fa-check"></i>Example (Lead concentration)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-diabetes"><i class="fa fa-check"></i>Example (Diabetes)</a><ul>
<li class="chapter" data-level="4.1.2" data-path=""><a href="#causality-and-confounders"><i class="fa fa-check"></i><b>4.1.2</b> Causality and confounders</a></li>
<li class="chapter" data-level="4.1.3" data-path=""><a href="#colliders"><i class="fa fa-check"></i><b>4.1.3</b> Colliders</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-study-time"><i class="fa fa-check"></i>Example (Study time)</a></li>
<li class="chapter" data-level="4.2" data-path=""><a href="#collapsibility"><i class="fa fa-check"></i><b>4.2</b> Collapsibility</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-blood-pressure"><i class="fa fa-check"></i>Example (Blood pressure)</a></li>
<li class="chapter" data-level="4.3" data-path=""><a href="#randomisation-restriction"><i class="fa fa-check"></i><b>4.3</b> Randomisation restriction</a></li>
<li class="chapter" data-level="4.4" data-path=""><a href="#sample-size-and-power"><i class="fa fa-check"></i><b>4.4</b> Sample size and power</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-two-01-dummies"><i class="fa fa-check"></i>Example (two 0/1 dummies)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path=""><a href="#reporting"><i class="fa fa-check"></i><b>5</b> Reporting</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path=""><a href="#app:VecDiff"><i class="fa fa-check"></i><b>A</b> Vector Differentiation</a></li>
<li class="chapter" data-level="B" data-path=""><a href="#app:LinTrans"><i class="fa fa-check"></i><b>B</b> Linear Transformations of MVN</a></li>
<li class="chapter" data-level="C" data-path=""><a href="#app:Slutsky"><i class="fa fa-check"></i><b>C</b> Slutsky's Theorem</a></li>
<li class="chapter" data-level="D" data-path=""><a href="#types-of-statistical-models"><i class="fa fa-check"></i><b>D</b> Types of Statistical Models</a></li>
<li class="chapter" data-level="E" data-path=""><a href="#r-session-info"><i class="fa fa-check"></i><b>E</b> R Session Info</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Linear Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Linear Models</h1>
<p class="author"><em>Olivier Thas</em></p>
<p class="date"><em>2021-2021</em></p>
</div>
<!--- For HTML Only --->
<p><span class="math inline">\(\newcommand{\prob}[1]{\text{P}\left\{#1\right\}}\)</span> <span class="math inline">\(\newcommand{\mb}[1]{\boldsymbol{#1}}\)</span> <span class="math inline">\(\newcommand{\E}[1]{\mbox{E}\left\{#1\right\}}\)</span> <span class="math inline">\(\newcommand{\Ef}[2]{\mbox{E}_{#1}\left\{#2\right\}}\)</span> <span class="math inline">\(\newcommand{\cov}[1]{\mbox{Cov}\left\{#1\right\}}\)</span> <span class="math inline">\(\newcommand{\cor}[1]{\mbox{Cor}\left\{#1\right\}}\)</span> <span class="math inline">\(\newcommand{\covf}[2]{\mbox{Cov}_{#1}\left\{#2\right\}}\)</span> <span class="math inline">\(\newcommand{\var}[1]{\mbox{Var}\left\{#1\right\}}\)</span> <span class="math inline">\(\newcommand{\varf}[2]{\mbox{Var}_{#1}\left\{#2\right\}}\)</span> <span class="math inline">\(\newcommand{\ind}[0]{\perp \!\!\! \perp}\)</span> <span class="math inline">\(\newcommand{\eps}[0]{\varepsilon}\)</span> <span class="math inline">\(\newcommand{\SSE}[0]{\text{SSE}}\)</span> <span class="math inline">\(\newcommand{\iid}[0]{\text{ i.i.d. }}\)</span> <span class="math inline">\(\newcommand{\convDistr}[0]{\stackrel{d}{\longrightarrow}}\)</span> <span class="math inline">\(\newcommand{\convProb}[0]{\stackrel{p}{\longrightarrow}}\)</span> <span class="math inline">\(\newcommand{\QED}[0]{\null\nobreak\hfill\ensuremath{\blacksquare}}\)</span> <span class="math inline">\(\newcommand{\MSE}[0]{\text{MSE}}\)</span> <span class="math inline">\(\newcommand{\SSTot}[0]{\text{SSTot}}\)</span> <span class="math inline">\(\newcommand{\SSR}[0]{\text{SSR}}\)</span> <span class="math inline">\(\newcommand{\MSR}[0]{\text{MSR}}\)</span> <span class="math inline">\(\newcommand{\probf}[2]{\text{P}_{#1}\left\{#2\right\}}\)</span> <span class="math inline">\(\newcommand{\HSim}[0]{\stackrel{H_0}{\sim}}\)</span> <span class="math inline">\(\newcommand{\ApproxSim}[0]{\stackrel{\cdot}{\sim}}\)</span></p>

<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:MASS&#39;:
## 
##     select</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre><code>## 
## Attaching package: &#39;plotly&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     last_plot</code></pre>
<pre><code>## The following object is masked from &#39;package:MASS&#39;:
## 
##     select</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     filter</code></pre>
<pre><code>## The following object is masked from &#39;package:graphics&#39;:
## 
##     layout</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;GGally&#39;:
##   method from   
##   +.gg   ggplot2</code></pre>
<pre><code>## Loading required package: carData</code></pre>
<pre><code>## 
## Attaching package: &#39;car&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     recode</code></pre>

<div id="Ch_Introduction" class="section level1">
<h1><span class="header-section-number">Hoofdstuk 1</span> Introduction</h1>
<div id="versions-and-changes" class="section level2">
<h2><span class="header-section-number">1.1</span> Versions and Changes</h2>
<p>Since this is a new course and the course notes are still under development, I will every now and then update the course notes. The updates may contain new materials and/or corrections to previous versions. In this section I will briefly list the changes as compared to previous versions.</p>
<ul>
<li><p><strong>Version</strong>: November 17</p></li>
<li><p><strong>Changes</strong>:</p>
<ul>
<li><p>Chapter 4</p></li>
<li><p>Small corrections (again thanks to the students for notifying me)</p></li>
</ul></li>
<li><p><strong>Version</strong>: November 16</p></li>
<li><p><strong>Changes</strong>:</p>
<ul>
<li><p>Section 2.3.1: Exercise (simulation study). In the solution I now use the function <em>colMeans</em> for calculating the MSEs. This gives you the MSEs of the two parameter estimators separately (instead of the average MSE over the two parameter estimators).</p></li>
<li><p>Sections 3.4 and 3.5: in the examples I illustrated specific R function for computing leverage and VIF</p></li>
<li><p>Sections 3.6 and 3.7</p></li>
<li><p>Small corrections (thanks to the students for notifying me)</p></li>
</ul></li>
<li><p><strong>Version</strong>: November 3</p></li>
<li><p><strong>Changes</strong>:</p>
<ul>
<li><p>Section 2.7.: extra exercise on data-driven choice of the alternative hypothesis.</p></li>
<li><p>new sections: Section 2.11, 2.12, 2.13, 3.1, 3.2, 3.3, 3.4 and 3.5</p></li>
<li><p>new appendix: Appendix E contains the R session information. Here can see e.g. the packages that are required.</p></li>
</ul></li>
<li><p><strong>Version</strong>: October 27</p></li>
<li><p><strong>Changes</strong>:</p>
<ul>
<li><p>Section 2.3.2, Theorem 2.3: small correction in the proof (there was a factor <span class="math inline">\(\mb{c}\)</span> missing)</p></li>
<li><p>Section 2.6, simulation study: In the function simRegressionBI, the error terms were simulated from a centered exponential distribution. This is changed to a normal distribution. (Note that the results with the exponential distribution are approximately correct for large sample sizes).</p></li>
<li><p>Section 2.7, Galton's height example: the one-sided alternative hypothesis is corrected to <span class="math inline">\(H_1:\beta_1\)</span>.</p></li>
<li><p>Section 2.7: an extra exercise on the calculation of one-sided p-values is added.</p></li>
<li><p>Section 2.7: the solution of exercise (muscle mass) is given in some more detail.</p></li>
</ul></li>
<li><p><strong>Version</strong>: October 26</p></li>
<li><p><strong>Changes</strong>:</p>
<ul>
<li><p>Section 2.3.2: correction to the R code of the simulation exercise (calculation of MSE)</p></li>
<li><p>Section 2.3.3: simulation study with exponentially distributed error term (the wrong covariance matrices were shown)</p></li>
</ul></li>
</ul>
</div>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">1.2</span> Introduction</h2>
<p>This course (<em>Linear Models</em>) is a classical treatment of linear models, which are among the most popular and simple models for statistical data analysis and prediction. The models describe the relationship between the mean of an outcome variable <span class="math inline">\(Y\)</span> and one or more regressors <span class="math inline">\(x\)</span> via a function that is linear in the parameters. The model thus focusses on the conditional expectation <span class="math inline">\(\E{Y \mid x}\)</span>. Here is a brief overview of the content of the course:</p>
<ul>
<li><p>simple linear regression model: here the models are limited to a single regressor. In this chapter most of the concepts and theory will be presented so that in later chapters we can rely on many of the theoretical results of this chapter. Throughout the whole chapter, the methods will be illustrated by means of example datasets, and the meaning of the model and the statistical inference procedures will be demonstrated with Monte Carlo simulation studies. In particular, the following topics will be discussed:</p>
<ul>
<li><p>the model and its interpretation</p></li>
<li><p>parameter estimation (ordinary least squares and maximum likelihood) and the properties of the estimators</p></li>
<li><p>sampling distributions of the estimators. They form the basis of statistical inference.</p></li>
<li><p>confidence intervals of the parameters</p></li>
<li><p>hypothesis tests related to the parameters</p></li>
<li><p>assessment of the model assumptions</p></li>
<li><p>the use of binary dummy regressors to mimic the two-sample problem (two-sample t-test)</p></li>
<li><p>association versus causation. We will give a brief introduction to causal inference.</p></li>
<li><p>linear regression models for prediction purposes</p></li>
</ul></li>
<li><p>the multiple linear regression model: the simple linear regression model is extended to include more than one regressor. For many topics (e.g. parameter estimation, confidence intervals and hypopthesis tests) we will be able to refer to the previous chapter, which will make this chapter less theoretical. Quite some focus will be on the interpretation of the parameters and specific issues that are irrelevant for the simple linear regression model. These topics will be discussed:</p>
<ul>
<li><p>the additive model and its interpretation</p></li>
<li><p>parameter estimation, confidence intervals and hypothesis testing (with a lot of references to the previous chapter)</p></li>
<li><p>interaction effects and the non-additive model</p></li>
<li><p>sum of squares and the ANOVA table</p></li>
<li><p>multicolinearity</p></li>
<li><p>assessment of the model assumptions</p></li>
<li><p>prediction modelling and model selection</p></li>
</ul></li>
<li><p>design-related topics and causal inference. In this chapter we discuss in some more detail the importance of the study design and some further concepts in causal inference. In particular:</p>
<ul>
<li><p>blocking and stratification</p></li>
<li><p>estimability</p></li>
<li><p>randomisation</p></li>
<li><p>confouders</p></li>
<li><p>causality, causal graphs, collapsibility</p></li>
</ul></li>
<li><p>Analysis of variance (ANOVA). The approach taken is this course, is to embed anova models in regression models. So in this chapter you first learn about the conventional formulation of anova models, and subsequently how these models can be rewritten as linear regression models so that the theory and methods from the previous chapters become available. The following topics will be treated:</p>
<ul>
<li><p>cell means and factor effects models and their reformulation as linear regression models</p></li>
<li><p>sum of squares and the ANOVA table</p></li>
<li><p>one way, two way and multiple way models</p></li>
<li><p>contrasts and multiple comparisons of means</p></li>
</ul></li>
<li><p>This course is concluded with a chapter on reporting: how to write a good statistical report. This can be seen as a stand alone chapter and can be read at any time.</p></li>
</ul>
</div>
<div id="how-to-work-through-the-course-notes" class="section level2">
<h2><span class="header-section-number">1.3</span> How to work through the course notes</h2>
<p>These course notes are written with R Bookdown in R Studio, and rendered to an html file that is made available on GitHub. If you wish, you can convert the html to a pdf file. The best way to do this, is via the <em>print</em> function of your browser. Before doing so, you can best first collapse the sidebar (that contains the table of content).</p>
<p>This course is work in progress. It is a new course and <strong>the course notes are still work in progress</strong>. So every week there will be an update. If you access the course notes via the link to GitHub you will see the updates automatically.</p>
<p>Many <strong>data analyses examples</strong> are worked out in detail in the course notes, including the R code. When you access the course notes via GitHub, you can easily copy the R code and paste it to your local R software. Here is an example of a chunck of R code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a&lt;-<span class="dv">1</span>
b&lt;-<span class="dv">2</span>
a<span class="op">+</span>b</code></pre></div>
<pre><code>## [1] 3</code></pre>
<p>If you now move your mouse towards the upper right corner of the box that contains the R code, then you will see an icon appearing in that corner:</p>
<p><img src="figures/copy.png" width="5%" style="display: block; margin: auto;" /> If you click on that icon, the content of the box will be copied to your clipboard so that you can paste it wherever you want (e.g. in a local R file).</p>
<p>I actually advise that you also <strong>work with your own local R Markdown or Notebook file</strong> to which you copy some of the R code of the course notes so that you can <em>play</em> with the data and the R code yourself. All datafiles are also available on Blackboard.</p>
<p>Throughout the course there are several exercises. The introduction to the excercise, as well as the data set and some instructions are given, but you do not directly get to see the solution. However, there is a option to expand the html so that the solution becomes visible. This should invite you to first try to make the exercise yourself (in a local R Markdown or Notebook file) before looking at the solution in the course notes. Here is an illustration:</p>
<div id="exercise" class="section level3 unnumbered">
<h3>Exercise</h3>
<p>Consider the following data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dataset&lt;-<span class="kw">data.frame</span>(<span class="dt">y=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>),
                    <span class="dt">x=</span><span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>))</code></pre></div>
<p>Make a scatter plot of <span class="math inline">\(y\)</span> versus <span class="math inline">\(x\)</span>.</p>
<p><details> <summary markdown="span">Try to make this exercise yourself. If you are ready you can expand this page and look at a solution</summary></p>
<p>Here is a solution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(dataset<span class="op">$</span>x,dataset<span class="op">$</span>y,
     <span class="dt">xlab=</span><span class="st">&quot;x&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;y&quot;</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-6-1.png" width="672" /> </details></p>
</div>
</div>
<div id="two-versions-of-this-course-oc-and-dl" class="section level2">
<h2><span class="header-section-number">1.4</span> Two versions of this course: OC and DL</h2>
<p>As for all courses in our Master of Statistics and Data Science program, each course has two versions: the <strong>on-campus (OC)</strong> and the <strong>distance learning (DL)</strong> versions.</p>
<p>The course notes are the same for both groups of students. Each group, however, has another Blackboard course site at which you can find all files and information. I keep these two Blackboard sites separate because the contact modes and hours are different:</p>
<ul>
<li><p>the OC students have live lectures (either in class, on campus, if permitted in corona times, or online) and self study assignments followed by online Q&amp;A sessions. Their lectures and Q&amp;A sessions are scheduled in their official course schedule.</p></li>
<li><p>the DL students will get access to web lectures and they can watch them at their own pace. On Blackboard I will suggest a time schedule. There are also online Q&amp;A sessions which give the students the opportunity to ask questions to the lecturer. The dates of the Q&amp;A sessions are in the Blackboard Calendar.</p></li>
</ul>
<p>The project assignment is the same for the DL and OC students.</p>
</div>
<div id="communication" class="section level2">
<h2><span class="header-section-number">1.5</span> Communication</h2>
<p>On Blackboard there is a <strong>discussion forum</strong> that can be used for asking questions.</p>
<p>Questions can also be asked during the <strong>Q&amp;A sessions</strong>, and the OC students can of course also ask questions in class (either physically when we are on-campus, or in the virtual class room when we are online).</p>
<p>You are also welcome to send emails to the lecturer, but the discussion forum is preferred so that other students can also learn from the answers to the questions.</p>
</div>
<div id="software" class="section level2">
<h2><span class="header-section-number">1.6</span> Software</h2>
<p>In the course notes all data analyses are demonstrated with the R software, but on Blackboard also documentation on the SAS software will be provided. You will also have to learn how to perform regression analysis and ANOVA with the SAS software.</p>

</div>
</div>
<div id="Ch:Reg1" class="section level1">
<h1><span class="header-section-number">Hoofdstuk 2</span> Simple Linear Regression Analysis</h1>
<div id="example-galtons-height-data" class="section level2 unnumbered">
<h2>Example (Galton's height data)</h2>
<p>As a first example, we consider the original dataset from Francis Galton, who invented <em>regression</em> by looking at this dataset. We obtained the data from <a href="https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/T0HSJ1/LKC7PJ&amp;version=1.1" class="uri">https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/T0HSJ1/LKC7PJ&amp;version=1.1</a> Here is the reference to the orginal paper of Francis Galton from 1886.</p>
<p>Galton, F. (1886). Regression Towards Mediocrity in Hereditary Stature. Journal of the Anthropological Institute, 15, 246-263</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Galton&lt;-<span class="kw">read.csv</span>(<span class="st">&quot;Data/Galton.tab&quot;</span>,<span class="dt">sep=</span><span class="st">&quot;</span><span class="ch">\t</span><span class="st">&quot;</span>)
<span class="kw">head</span>(Galton)</code></pre></div>
<pre><code>##   family father mother gender height kids male female
## 1      1   78.5   67.0      M   73.2    4    1      0
## 2      1   78.5   67.0      F   69.2    4    0      1
## 3      1   78.5   67.0      F   69.0    4    0      1
## 4      1   78.5   67.0      F   69.0    4    0      1
## 5      2   75.5   66.5      M   73.5    4    1      0
## 6      2   75.5   66.5      M   72.5    4    1      0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glimpse</span>(Galton)</code></pre></div>
<pre><code>## Rows: 898
## Columns: 8
## $ family &lt;fct&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5,…
## $ father &lt;dbl&gt; 78.5, 78.5, 78.5, 78.5, 75.5, 75.5, 75.5, 75.5, 75.0, 75.0, 75…
## $ mother &lt;dbl&gt; 67.0, 67.0, 67.0, 67.0, 66.5, 66.5, 66.5, 66.5, 64.0, 64.0, 64…
## $ gender &lt;fct&gt; M, F, F, F, M, M, F, F, M, F, M, M, F, F, F, M, M, M, F, F, F,…
## $ height &lt;dbl&gt; 73.2, 69.2, 69.0, 69.0, 73.5, 72.5, 65.5, 65.5, 71.0, 68.0, 70…
## $ kids   &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,…
## $ male   &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,…
## $ female &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,…</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">skim</span>(Galton)</code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-7">Table 2.1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">Galton</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">898</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">8</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">factor</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">6</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: factor</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="left">ordered</th>
<th align="right">n_unique</th>
<th align="left">top_counts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">family</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">197</td>
<td align="left">185: 15, 166: 11, 66: 11, 130: 10</td>
</tr>
<tr class="even">
<td align="left">gender</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">2</td>
<td align="left">M: 465, F: 433</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">father</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">69.23</td>
<td align="right">2.47</td>
<td align="right">62</td>
<td align="right">68</td>
<td align="right">69.0</td>
<td align="right">71.0</td>
<td align="right">78.5</td>
<td align="left">▁▅▇▂▁</td>
</tr>
<tr class="even">
<td align="left">mother</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">64.08</td>
<td align="right">2.31</td>
<td align="right">58</td>
<td align="right">63</td>
<td align="right">64.0</td>
<td align="right">65.5</td>
<td align="right">70.5</td>
<td align="left">▂▅▇▃▁</td>
</tr>
<tr class="odd">
<td align="left">height</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">66.76</td>
<td align="right">3.58</td>
<td align="right">56</td>
<td align="right">64</td>
<td align="right">66.5</td>
<td align="right">69.7</td>
<td align="right">79.0</td>
<td align="left">▁▇▇▅▁</td>
</tr>
<tr class="even">
<td align="left">kids</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">6.14</td>
<td align="right">2.69</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">6.0</td>
<td align="right">8.0</td>
<td align="right">15.0</td>
<td align="left">▃▇▆▂▁</td>
</tr>
<tr class="odd">
<td align="left">male</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.52</td>
<td align="right">0.50</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1.0</td>
<td align="right">1.0</td>
<td align="right">1.0</td>
<td align="left">▇▁▁▁▇</td>
</tr>
<tr class="even">
<td align="left">female</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.48</td>
<td align="right">0.50</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0.0</td>
<td align="right">1.0</td>
<td align="right">1.0</td>
<td align="left">▇▁▁▁▇</td>
</tr>
</tbody>
</table>
<p>The dataset contains data on heights of parents and their adult children; there can be more than one child per family. For our purpose we will select fathers and one son (we will select the first son that appears in the family).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Galton.sons&lt;-Galton <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(gender<span class="op">==</span><span class="st">&quot;M&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(family) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span>ungroup <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">father.cm=</span>father<span class="op">*</span><span class="fl">2.54</span>,
         <span class="dt">son.cm=</span>height<span class="op">*</span><span class="fl">2.54</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(father.cm, son.cm)
<span class="kw">glimpse</span>(Galton.sons)</code></pre></div>
<pre><code>## Rows: 173
## Columns: 2
## $ father.cm &lt;dbl&gt; 199.390, 175.260, 175.260, 175.260, 175.260, 176.530, 175.2…
## $ son.cm    &lt;dbl&gt; 185.928, 180.848, 190.500, 177.800, 185.420, 179.070, 180.3…</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(Galton.sons,
       <span class="kw">aes</span>(<span class="dt">x=</span>father.cm, <span class="dt">y=</span>son.cm)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">color=</span><span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;length of father (cm)&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;length of son (cm)&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>), <span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>))</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>This scatter plot suggests a possitive correlation between the heights of the father and the son.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(Galton.sons<span class="op">$</span>father.cm, Galton.sons<span class="op">$</span>son.cm)</code></pre></div>
<pre><code>## [1] 0.5022938</code></pre>
<p>In this course, however, we are not only interested in the correlation, but we want to quantify how the expected (or average) height of sons varies with the age of the father.</p>
<p>In the context of regression analysis we will use the following terminology:</p>
<ul>
<li><p>height of son is the <strong>outcome</strong> or <strong>response</strong> (or <strong>response variable</strong>) or <strong>dependent variable</strong>. This will be denoted by <span class="math inline">\(Y\)</span>.</p></li>
<li><p>height of father is the <strong>regressor</strong>, <strong>covariate</strong> or <strong>independent variable</strong>. This will be denoted by <span class="math inline">\(x\)</span>.</p></li>
</ul>
<p>One way of looking at the problem, is to consider the outcomes that correspond to fathers of a given height, as a population. In this way, for example, the heights of sons of fathers of height <span class="math inline">\(172.72\)</span>cm can be considered as a random sample of outcomes from the population of heights of sons of fathers of height <span class="math inline">\(172.72\)</span>cm. We can apply this reasoning for all heights of fathers included in the dataset, and in a more abstract way we can think of samples of outcomes for each value of the regressor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Galton.sons <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(father.cm<span class="op">==</span><span class="fl">172.72</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>father.cm,<span class="dt">y=</span>son.cm)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_boxplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">position =</span> <span class="kw">position_jitter</span>(<span class="fl">0.2</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;length of son (cm)&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>), <span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title.x=</span><span class="kw">element_blank</span>(),<span class="dt">axis.text.x=</span><span class="kw">element_blank</span>())</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>We will use the following notation:</p>
<ul>
<li><p><span class="math inline">\(n\)</span>: total number of observations (subjects, elements) in the dataset.</p></li>
<li><p><span class="math inline">\(x_i\)</span>: the value of the regressor of observation <span class="math inline">\(i=1,\ldots, n\)</span></p></li>
<li><p><span class="math inline">\(Y_i\)</span>: the outcome of observation <span class="math inline">\(i=1,\ldots, n\)</span>.</p></li>
</ul>
<p>Since every <span class="math inline">\(x_i\)</span> can identify another population, we say that <span class="math inline">\(Y_i\)</span> is a random outcome from the population with (cumulative) distribution function (CDF) <span class="math display">\[
  F_i(y)=F(y;x_i,\mb\beta),
\]</span> i.e. the distribution is determined by <span class="math inline">\(x_i\)</span> and possibly by a parameter vector <span class="math inline">\(\mb\beta\)</span>. We also assume that all <span class="math inline">\(n\)</span> outcomes are mutually independent. Note that we do not say that the <span class="math inline">\(n\)</span> outcomes are i.i.d. (<em>identically and independently distributed</em>), because not all <span class="math inline">\(F_i\)</span> coincide.</p>
<p>Regression analysis is a method that allows to study the effect of a regressor on the <strong>mean outcome</strong>. We therefore also introduce a notation for the mean of the distribution <span class="math inline">\(F_i\)</span>, <span class="math display">\[
  \mu_i = \E{Y_i} = \Ef{F_i}{Y_i} = \int_{-\infty}^{+\infty} y dF(y;x_i,\mb\beta) = \E{Y_i \mid x_i}.
\]</span> This notation stresses that <span class="math inline">\(\mu_i\)</span> is the mean of <span class="math inline">\(Y_i\)</span> and that this mean depends on the value of the regressor <span class="math inline">\(x_i\)</span> because the distribution of <span class="math inline">\(Y_i\)</span> depends on <span class="math inline">\(x_i\)</span>. Therefore, <span class="math inline">\(\mu_i=\E{Y_i \mid x_i}\)</span> is the <strong>conditional mean</strong> of the outcome, given <span class="math inline">\(x_i\)</span>. To stress that the conditional mean is a function of the regressor and possibly of a parameter <span class="math inline">\(\mb\beta\)</span> we write <span class="math display">\[
  m(x;\mb\beta) = \E{Y \mid x}.
\]</span></p>
<p>In the two previous paragraphs we actually gave a generic description of a <strong>statistical model</strong>. We repeat here the description, with in slightly more general fashion.</p>
<ul>
<li><p>For a given <span class="math inline">\(x_i\)</span>, we provide the conditional distribution of the outcome, <span class="math display">\[
   Y_i \mid x_i \sim F_i(\cdot;x_i,\mb\beta,\mb\nu)
\]</span> in which <span class="math inline">\(\mb\beta\)</span> and <span class="math inline">\(\mb\nu\)</span> are two parameter vectors.</p></li>
<li><p>The conditional mean of this distribution is described as <span class="math display">\[
   \E{Y_i\mid x_i}=m(x_i;\mb\beta).
\]</span></p></li>
<li><p>The <span class="math inline">\(n\)</span> outcomes <span class="math inline">\(Y_i\)</span> are mutually independent.</p></li>
</ul>
<p>Consider the follow special cases:</p>
<ul>
<li><p>Suppose there are no parameters <span class="math inline">\(\mb\beta\)</span> and <span class="math inline">\(\mb\nu\)</span> and the functions <span class="math inline">\(m\)</span> and <span class="math inline">\(F_i\)</span> are not known. Then the model imposes no restriction on the conditional distribution <span class="math inline">\(Y_i \mid x_i\)</span>. This is a <strong>nonparametric model</strong>.</p></li>
<li><p>Suppose that the function <span class="math inline">\(m\)</span> is known, up to the parameter vector <span class="math inline">\(\mb\beta\)</span>, but the CDFs <span class="math inline">\(F_i\)</span> have no further restrictions (it does of course satisfy the restriction <span class="math inline">\(\E{Y_i\mid x_i}=m(x_i;\mb\beta)\)</span>). Then the statistical model only imposes restrictions on the conditional mean, but leaves other aspects of the conditional distribution unspecified. This is a <strong>semiparametric model</strong>.</p></li>
<li><p>Suppose that the function <span class="math inline">\(m\)</span> is known, up to the parameter vector <span class="math inline">\(\mb\beta\)</span>, and that the CDFs <span class="math inline">\(F_i\)</span> are also known, up to the parameter vector <span class="math inline">\(\mb\nu\)</span>. In this case, the <span class="math inline">\(n\)</span> CDFs <span class="math inline">\(F_i\)</span> are often equal to one another, i.e. <span class="math inline">\(F_i=F\)</span>. Of course, as for the semiparametric model, the CDF <span class="math inline">\(F\)</span> must be compatible with the restriction <span class="math inline">\(\E{Y_i\mid x_i}=m(x_i;\mb\beta)\)</span>. This model is known as a <strong>parametric model</strong>. It basically specifies the full conditional distribution up to finite dimensional parameter vectors <span class="math inline">\(\mb\beta\)</span> and <span class="math inline">\(\mb\nu\)</span>.</p></li>
</ul>
<p>With respect to the parametric models: if the interest is in the model <span class="math inline">\(\E{Y_i\mid x_i}=m(x_i;\mb\beta)\)</span>, i.e. the focus is on parameter <span class="math inline">\(\mb\beta\)</span>, then we call the parameter <span class="math inline">\(\mb\nu\)</span> a <strong>nuisance parameter</strong>.</p>
<p>In later sections we will sometimes refer to semiparametric and parametric models, and then their meaning may become clear.</p>
<p>The general regression model is illustrated in Figure <a href="#fig:RegModel">2.1</a> (left). The focus of a regression analysis is the estimation of the function <span class="math inline">\(m(x;\mb\beta)\)</span>. If the function <span class="math inline">\(m\)</span> is known, then this reduces to the estimation of the parameter <span class="math inline">\(\mb\beta\)</span> using the sample observations. Based on the estimates, regression analysis also aims to formulate conclusions on the relation between the regressor and the conditional mean of the outcome. Sometimes the estimated regression model may also be used for predicting an outcome for a given value of the regressor. These topics will all be discussed later in this course.</p>
<div class="figure" style="text-align: center"><span id="fig:RegModel"></span>
<img src="DASM2_files/figure-html/RegModel-1.png" alt="Illustration of the regression model. The black line represents the function $m$. The red points are observed outcomes, sampled from the conditional distribution of $Y$ given $x=160$. The red line shows the shape of the density function of this conditional distribution. The blue points are observed outcomes, sampled from the conditional distribution of $Y$ given $x=200$. The blue line shows the shape of the density function of this conditional distribution. Left: $m$ is a non-linear function and the conditional distributions have different shapes. Middle: $m$ is a non-linear function and the conditional distribution functions have equal shapes. Right: $m$ is a linear function and the conditional distribution functions have equal shapes (normal distributions)." width="672" />
<p class="caption">
Figure 2.1: Illustration of the regression model. The black line represents the function <span class="math inline">\(m\)</span>. The red points are observed outcomes, sampled from the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x=160\)</span>. The red line shows the shape of the density function of this conditional distribution. The blue points are observed outcomes, sampled from the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x=200\)</span>. The blue line shows the shape of the density function of this conditional distribution. Left: <span class="math inline">\(m\)</span> is a non-linear function and the conditional distributions have different shapes. Middle: <span class="math inline">\(m\)</span> is a non-linear function and the conditional distribution functions have equal shapes. Right: <span class="math inline">\(m\)</span> is a linear function and the conditional distribution functions have equal shapes (normal distributions).
</p>
</div>
We now simplify the model by assuming that the conditional distribution only depends on the regressor via its conditional mean. In other words: for every value of the regressor, the shape of the conditional outcome distribution is the same, and only the location (mean) may depend on the regressor. This is illustrated in Figure <a href="#fig:RegModel">2.1</a> (middle). This extra assumption allows us to write the statistical model as
<span class="math display" id="eq:Mod1">\[\begin{equation}
  Y_i = m(x_i;\mb\beta) + \eps_i
  \tag{2.1}
\end{equation}\]</span>
<p>with <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb\nu)\)</span>, with <span class="math inline">\(F_\eps(\cdot;\mb\nu)\)</span> the CDF of <span class="math inline">\(\eps_i\)</span>, <span class="math inline">\(\mb\nu\)</span> a parameter vector and <span class="math display">\[ 
  \E{\eps_i}=\E{\eps_i\mid x_i}=0.
\]</span> For the latter assumption, we find a useful property: <span class="math display">\[
  \E{Y\mid x}=m(x;\mb\beta)
\]</span> i.e. it allows to interpret <span class="math inline">\(m(x;\mb\beta)\)</span> as the conditional mean. The stochastic variable <span class="math inline">\(\eps_i\)</span> is often referred to as the <strong>error term</strong>. The model thus suggests that the outcome can be decomposed into two components: a <strong>systematic component</strong>, <span class="math inline">\(m(x_i;\mb\beta)\)</span>, and a <strong>stochastic component</strong> (or random component, or error term) <span class="math inline">\(\eps_i\)</span>. The latter gives the deviation between the outcome <span class="math inline">\(Y_i\)</span> and the systematic component <span class="math inline">\(m(x_i;\mb\beta)\)</span>.</p>
<p>It is important, however, to note that this decomposition often does not agree with how the outcomes are generated or realised: the variability of the outcomes <span class="math inline">\(Y_i\)</span> about the conditional mean <span class="math inline">\(m(x_i;\mb\beta)\)</span> is often inherently present in the population (e.g. biological variability). In this sense, <span class="math inline">\(\eps_i\)</span> may not be looked at as an <em>error</em> on the measurement. In other cases, however, part of the variability in <span class="math inline">\(Y_i\)</span> may be due to imprecise measurements and then <span class="math inline">\(\eps_i\)</span> can (at least partly) be considered as a random error term.</p>
<p>Note that the assumption <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb{\nu})\)</span> implies that all error terms have the same distribution and hence, for a fixed <span class="math inline">\(\sigma^2\geq 0\)</span>, <span class="math display">\[
  \var{\eps_i} = \var{\eps_i \mid x_i} = \sigma^2
\]</span> for all <span class="math inline">\(i=1,2,\ldots, n\)</span>. This restriction on the variance is referred to as the assumption of <strong>homoskedasticiteit</strong> or <strong>constant variance</strong>. The variance <span class="math inline">\(\sigma^2\)</span> is called the <strong>residual variance</strong>.</p>
<p>In this course we only discuss <strong>linear regression analysis</strong>, for which the function <span class="math inline">\(m(x;\mb\beta)\)</span> is restricted to linear functions of <span class="math inline">\(\mb\beta\)</span>: <span class="math display">\[
  m(x;\mb\beta) = \beta_0 + \beta_1x,
\]</span> with <span class="math inline">\(\mb{\beta}^t=(\beta_0, \beta_1)\)</span>.</p>
This equation represents a linear line which is referred to as the <strong>regression line</strong>. We write model <a href="#eq:Mod1">(2.1)</a> now as
<span class="math display" id="eq:Mod3">\[\begin{equation}
  Y_i = \beta_0 + \beta_1x_i + \eps_i
  \tag{2.2}
\end{equation}\]</span>
<p>with <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb\nu)\)</span> and <span class="math inline">\(\E{\eps_i}=\E{\eps_i\mid x_i}=0\)</span>. This is called the <strong>simple linear regression model</strong>. See also Figure <a href="#fig:RegModel">2.1</a> (right).</p>
<p>The interpretation of the parameter <span class="math inline">\(\beta_1\)</span> follows from the identity <span class="math display">\[
  \E{Y\mid x+1} - \E{Y\mid x} = \left(\beta_0+\beta_1(x+1)\right)-\left(\beta_0+\beta_1 x\right) = \beta_1 .
\]</span> The parameter <span class="math inline">\(\beta_1\)</span> is thus the average increase in the outcome when the regressor increases with one unit. This parameter is also the <strong>slope</strong> of the regression line. The parameter is often referred to as the <strong>regression coefficient</strong>.</p>
<p>The interpretation of the parameter <span class="math inline">\(\beta_0\)</span> follows from the identity <span class="math display">\[
  \E{Y\mid x=0} = \beta_0+\beta_1 \times 0 = \beta_0 .
\]</span> The parameter <span class="math inline">\(\beta_0\)</span> is thus the average outcome when the regressor takes value zero. It is the <strong>intercept</strong> of the regression line.</p>
<p>Sometimes the situation <span class="math inline">\(x=0\)</span> does not have a physical meaning or it falls outside of the <strong>scope</strong> of the model (i.e. the range of <span class="math inline">\(x\)</span>-values that forms the focus of the data analysis). For this reason, the regression model is sometimes formulated as <span class="math display">\[
  Y_i = \beta_0 + \beta_1(x_i-\bar{x}) + \eps_i
\]</span> with <span class="math inline">\(\bar{x}\)</span> the sample mean of the regressor observations in the dataset, <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb\nu)\)</span> and <span class="math inline">\(\E{\eps_i}=\E{\eps_i\mid x_i}=0\)</span>. The interpretation of <span class="math inline">\(\beta_0\)</span> now becomes <span class="math inline">\(\beta_0=\E{Y\mid x=\bar{x}}\)</span>. Since the sample mean <span class="math inline">\(\bar{x}\)</span> is often withing the scope of the model, the parameter <span class="math inline">\(\beta_0\)</span> now has a real meaning. The interpretation of <span class="math inline">\(\beta_1\)</span> remains unchanged.</p>
<p>In the Galton example, the scope is approximately <span class="math inline">\([150 \text{cm},200 \text{cm}]\)</span>.</p>
<p>In this chapter we discuss methods for the estimation of the parameters in the linear regression model <a href="#eq:Mod3">(2.2)</a>. We will find the sampling distribution of the parameter estimators. This will form the basis for hypothesis tests and for confidence intervals.</p>
</div>
<div id="S:RegSimStudy" class="section level2">
<h2><span class="header-section-number">2.1</span> Interpretation via simulations</h2>

<p>Regression model <a href="#eq:Mod3">(2.2)</a> thus gives an interpretation to the <span class="math inline">\(\beta\)</span> parameters via the conditional expectation of the outcomes. In this section we provide an interpretation via the principle of <strong>repeated sampling</strong>, which can be easily demonstrated with Monte Carlo simulations. Since this is the first simulation in this course, we shall go through it step by step.</p>
<p>We start with model <a href="#eq:Mod3">(2.2)</a>: <span class="math display">\[
  Y_i = \beta_0 + \beta_1x_i + \eps_i
\]</span> with <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb\nu)\)</span> and <span class="math inline">\(\E{\eps_i}=\E{\eps_i\mid x_i}=0\)</span>. We set <span class="math inline">\(F_\eps\)</span> to the normal distribution with mean zero and variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>We start with the simulation of a sample of 5 outcomes (son's heights) at <span class="math inline">\(\mb{x}^t=(165, 170, 175, 180, 185)\)</span> (father's heights).</p>
<p>Before we can start the simulations, we need to set the parameters to specific values. We choose: <span class="math display">\[
  \beta_0=90 \;\;\; \beta_1=0.5 \;\;\; \sigma=5.
\]</span> Thus, we simulate as if we known the truth (i.e. the population). In this context, we refer to this model as the <strong>data generating model</strong>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">724245</span>)
x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span>
x</code></pre></div>
<pre><code>## [1] 165 170 175 180 185</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">eps&lt;-<span class="kw">rnorm</span>(<span class="dv">5</span>,<span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 error terms</span>
eps</code></pre></div>
<pre><code>## [1]  2.6552611 -4.3398292  2.1952809  6.1667742 -0.9250292</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span>eps <span class="co"># random sample van uitkomsten</span>
y   </code></pre></div>
<pre><code>## [1] 175.1553 170.6602 179.6953 186.1668 181.5750</code></pre>
<p>The following R code gives Figure <a href="#fig:RegSim1">2.2</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(x,y,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>)
<span class="kw">abline</span>(<span class="kw">c</span>(<span class="dv">90</span>,<span class="fl">0.5</span>),<span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:RegSim1"></span>
<img src="DASM2_files/figure-html/RegSim1-1.png" alt="Scatter plot of 1 simulated sample from the (data generating) regression model. The red line is the linear regression model with the true parameter values." width="672" />
<p class="caption">
Figure 2.2: Scatter plot of 1 simulated sample from the (data generating) regression model. The red line is the linear regression model with the true parameter values.
</p>
</div>
<p>Next we repeat this procedure (experiment) multiple times. Each time other outcomes will be generated. The following R code generates <span class="math inline">\(N=100\)</span> repeated experiments, each with <span class="math inline">\(n=5\)</span> outcomes as described earlier. The resulst are visualised in Figure <a href="#fig:RegSim2">2.3</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">254111</span>)
N&lt;-N100 <span class="co"># number of repeated experiments</span>
x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span>
y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">5</span>,<span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span>
<span class="kw">plot</span>(x,y,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">160</span>,<span class="dv">195</span>))
<span class="kw">abline</span>(<span class="kw">c</span>(<span class="dv">90</span>,<span class="fl">0.5</span>),<span class="dt">col=</span><span class="dv">2</span>)

Data&lt;-<span class="kw">data.frame</span>(<span class="dt">experiment=</span><span class="dv">1</span>,<span class="dt">x=</span>x,<span class="dt">y=</span>y)

<span class="cf">for</span>(experiment <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>N) {
    y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">5</span>,<span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span>
    <span class="kw">points</span>(x,y,<span class="dt">col=</span>experiment)
    Data&lt;-<span class="kw">rbind</span>(Data,<span class="kw">cbind</span>(experiment,x,y))
}</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:RegSim2"></span>
<img src="DASM2_files/figure-html/RegSim2-1.png" alt="Scatter plot of $N=100$ simulated samples (experiments) from a regression model. Each color corresponds to a repeated experiment. The red line is the linear regression line with the true parameter values." width="672" />
<p class="caption">
Figure 2.3: Scatter plot of <span class="math inline">\(N=100\)</span> simulated samples (experiments) from a regression model. Each color corresponds to a repeated experiment. The red line is the linear regression line with the true parameter values.
</p>
</div>
<p>Now we look at the histograms of the repeated samples; each histogram corresponds to another value of <span class="math inline">\(x\)</span>. See Figure <a href="#fig:RegSim3">2.4</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {
  y&lt;-Data<span class="op">$</span>y[Data<span class="op">$</span>x<span class="op">==</span>x[i]]
  <span class="kw">hist</span>(y,<span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&quot;x=&quot;</span>,x[i]),<span class="dt">xlab=</span><span class="st">&quot;y&quot;</span>)
  <span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x[i],<span class="dt">col=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>)
  <span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">mean</span>(y),<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>)
}
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:RegSim3"></span>
<img src="DASM2_files/figure-html/RegSim3-1.png" alt="Histogram of the $N=100$ repeated experiments. Each histogram corresponds to another value of $x$. The red vertical lines show the average outcomes according to the true regression model and the blue dashed lines are the averages of the repeated outcomes at the corresponding temperatures." width="672" />
<p class="caption">
Figure 2.4: Histogram of the <span class="math inline">\(N=100\)</span> repeated experiments. Each histogram corresponds to another value of <span class="math inline">\(x\)</span>. The red vertical lines show the average outcomes according to the true regression model and the blue dashed lines are the averages of the repeated outcomes at the corresponding temperatures.
</p>
</div>
<p>Every histogram in Figure <a href="#fig:RegSim3">2.4</a> approximately shows a normal distribution. This is expected, because we have sampled from a normal distribution for each value of <span class="math inline">\(x\)</span>. In particular, for a given value of <span class="math inline">\(x\)</span>, we have sampled the outcomes <span class="math inline">\(Y\)</span> from the distribution <span class="math inline">\(N(90+0.5x,25)\)</span>.</p>
<p>Figure <a href="#fig:RegSim3">2.4</a> also shows the sample means of the N=100 repeated outcomes for each given <span class="math inline">\(x\)</span>. If <span class="math inline">\(N\)</span> is very large, then these sample means are approximately equal to the corresponding expected values <span class="math inline">\(\E{Y\mid x}=\beta_0+\beta_1x\)</span>. Also these expected values (with the given parameter values) are depicted in the graph; these are the points on the true regression line. Both lines are very close to one another. If <span class="math inline">\(N\)</span> were much larger then 100, we would expect that the lines would be even closer to one another. The next R code gives the numerical values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Results&lt;-<span class="kw">data.frame</span>(<span class="dt">x=</span><span class="ot">NA</span>,<span class="dt">EmpMean=</span><span class="ot">NA</span>,<span class="dt">ExpectedValue=</span><span class="ot">NA</span>,
                    <span class="dt">EmpVariance=</span><span class="ot">NA</span>,<span class="dt">Variance=</span><span class="ot">NA</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {
  y&lt;-Data<span class="op">$</span>y[Data<span class="op">$</span>x<span class="op">==</span>x[i]]
  Results[i,]&lt;-<span class="kw">c</span>(x[i],<span class="kw">round</span>(<span class="kw">mean</span>(y),<span class="dv">2</span>),<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x[i],
                    <span class="kw">round</span>(<span class="kw">var</span>(y),<span class="dv">2</span>),<span class="dv">25</span>)
}
Results</code></pre></div>
<pre><code>##     x EmpMean ExpectedValue EmpVariance Variance
## 1 165  172.63         172.5       22.46       25
## 2 170  174.69         175.0       27.60       25
## 3 175  177.92         177.5       23.17       25
## 4 180  181.08         180.0       24.82       25
## 5 185  182.94         182.5       25.68       25</code></pre>
<p>This R code also shows the sample variances of the simulated outcomes for the 5 values of <span class="math inline">\(x\)</span>. The true variance according the data generating model equals 25. Also here we see a good agreement.</p>
</div>
<div id="S:LSE1" class="section level2">
<h2><span class="header-section-number">2.2</span> Least squares estimators</h2>
<p>Consider again model <a href="#eq:Mod3">(2.2)</a>, <span class="math display">\[
  Y_i = \beta_0 + \beta_1x_i + \eps_i
\]</span> with <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb\nu)\)</span> and <span class="math inline">\(\E{\eps_i}=\E{\eps_i\mid x_i}=0\)</span>.</p>
<p>The <strong>least squares</strong> estimation method can be applied without knowledge of the exact shape of the distribution of the error term. In this sense, the model is an example of <strong>semiparametric statistical model</strong>: the conditional mean is parameterised (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>) and posesses a restriction on the conditional distributions, but the other moments of the conditional outcome distribution (or error term distribution) remain unspecified.</p>
<p>In this section we aim at estimating the regression parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, which will be denoted by <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>. The estimated regression line should come <em>as close as possible</em> to the observed outcomes in the sample. We will need a measure for the distance between the estimated regression line and the observed sample data. For given estimates <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>, the estimated regression line is given by <span class="math display">\[
  \hat{m}(x)=m(x;\hat{\mb\beta})= \hat\beta_0+\hat\beta_1 x.
\]</span> This line is sometimes referred to as the <strong>fitted</strong> regression line. For given <span class="math inline">\(x\)</span>, this gives an estimate of the conditional mean of the outcome <span class="math inline">\(Y\)</span>. For the <span class="math inline">\(n\)</span> sample observations, the points on the estimated regression line are denoted by <span class="math display">\[
  \hat{Y}_i = \hat{m}(x_i)=m(x_i;\hat{\mb\beta})=\hat\beta_0+\hat\beta_1 x_i \;\;\; i=1,\ldots, n.
\]</span> The <span class="math inline">\(\hat{Y}_i\)</span>s are often called the <strong>predictions</strong> of the outcomes, but in many cases this is a misleading terminology because the points on the estimated regression line should in the first place not be considered as predictions, but rather as estimates of the conditional mean. On the other hand, if no extra information is available, then <span class="math inline">\(\hat{m}(x)\)</span> is a good prediction for an outcome at regression value <span class="math inline">\(x\)</span> (see further). In this course we will stick to the conventional terminology (predictions), but in the first place we consider them as estimates of the conditional means.</p>
<p>Good parameter estimates <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> should make the predictions <span class="math inline">\(\hat{Y}_i\)</span> come as close as possible to the observed outcomes <span class="math inline">\(Y_i\)</span>. This can be quantified by the <strong>least squares criterion</strong></p>
<span class="math display" id="eq:SSEReg1">\[\begin{equation}
  \SSE(\hat{\mb\beta}) = \sum_{i=1}^n \left(Y_i - \hat{Y}_i\right)^2 = \sum_{i=1}^n \left(Y_i - m(x_i;\hat{\mb\beta})\right)^2.
  \tag{2.3}
\end{equation}\]</span>
<p>SSE is the abbreviation of <strong>sum of squares of the error</strong>, which is also referred to as the <strong>residual sum of squares</strong> or the <strong>sum of squared errors</strong>. This brings us to the following definition.</p>

<div class="definition">
<span id="def:SSE1" class="definition"><strong>Definition 2.1  (Least squares parameter estimator)  </strong></span>The least squares parameter estimator of <span class="math inline">\(\mb\beta\)</span> is given by <span class="math display">\[
  \hat{\mb\beta} = \text{ArgMin}_{\mb\beta} \SSE(\mb\beta).
\]</span>
</div>

<p>We use the abbreviation LSE for the <strong>least squares estimator</strong>.</p>
<p>Figure <a href="#fig:RegLSE">2.5</a> shows three datasets with fitted regression lines and with the indication of <span class="math inline">\(Y_i - \hat{Y}_i\)</span>. The deviation <span class="math inline">\(Y_i - \hat{Y}_i\)</span> is called the <strong>residual</strong>, which is often written as <span class="math display">\[
  e_i = Y_i - \hat{Y}_i \;\text{ or }\; e_i(\hat{\mb{\beta}}) =Y_i -m(x_i;\hat{\mb{\beta}}).
\]</span> With this notation we write <span class="math display">\[
  \SSE=\sum_{i=1}^n e_i^2 \;\text{ or }\; \SSE(\hat{\mb{\beta}})=\sum_{i=1}^n e_i^2(\hat{\mb{\beta}}).
\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:RegLSE"></span>
<img src="DASM2_files/figure-html/RegLSE-1.png" alt="Illustration of the estimation of the regession line. The black line shows the function $m$; this function is in practice not known and need to be estimated based on five sample observations (red points). The estimated regression line is depicted as the red dashed line. The vertical lines connect the observed outcomes $y_i$ with the predictions. The lengths of these vertical lines form the basis for SSE. Each of the three graphs start with another (randomly selected) sample. Left: estimates of $\beta_0$ and $\beta_1$ are $51.9$ and $0.71$ with SSE$=52.75$. Middle: $133.3$ and $0.26$ with SSE$=35.94$. Right: $122.3$ and $0.31$ with SSE$=3.18$." width="672" />
<p class="caption">
Figure 2.5: Illustration of the estimation of the regession line. The black line shows the function <span class="math inline">\(m\)</span>; this function is in practice not known and need to be estimated based on five sample observations (red points). The estimated regression line is depicted as the red dashed line. The vertical lines connect the observed outcomes <span class="math inline">\(y_i\)</span> with the predictions. The lengths of these vertical lines form the basis for SSE. Each of the three graphs start with another (randomly selected) sample. Left: estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are <span class="math inline">\(51.9\)</span> and <span class="math inline">\(0.71\)</span> with SSE<span class="math inline">\(=52.75\)</span>. Middle: <span class="math inline">\(133.3\)</span> and <span class="math inline">\(0.26\)</span> with SSE<span class="math inline">\(=35.94\)</span>. Right: <span class="math inline">\(122.3\)</span> and <span class="math inline">\(0.31\)</span> with SSE<span class="math inline">\(=3.18\)</span>.
</p>
</div>
<p>Before we continue, we introduce the <strong>matrix notation</strong> for the model. We introduce the following notation:</p>
<ul>
<li><p>parameter vector <span class="math inline">\(\mb\beta^t=(\beta_0, \beta_1)\)</span> and estimate <span class="math inline">\(\hat{\mb\beta}^t=(\hat\beta_0, \hat\beta_1)\)</span></p></li>
<li><p>outcome vector <span class="math inline">\(\mb{Y}^t=(Y_1,\ldots, Y_n)\)</span></p></li>
<li><p><strong>design matrix</strong> (<span class="math inline">\(n\times 2\)</span> matrix) <span class="math display">\[
 \mb{X}=\begin{pmatrix}
   1 &amp; x_1 \\
   1 &amp; x_2 \\
   \vdots &amp; \vdots \\
   1 &amp; x_n
 \end{pmatrix}.
 \]</span> The <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mb{X}\)</span> is represented by <span class="math inline">\(\mb{x}_i^t=(1,x_i)\)</span>.</p></li>
<li><p>error vector <span class="math inline">\(\mb{\eps}^t=(\eps_1,\ldots, \eps_n)\)</span>.</p></li>
</ul>
<p>With the vector and matrix notation we rewrite model <a href="#eq:Mod3">(2.2)</a> as <span class="math display">\[
  Y_i = \mb{x}_i^t\mb\beta+\eps_i
\]</span> or as <span class="math display">\[
  \mb{Y} = \mb{X}\mb\beta + \mb{\eps}
\]</span> with <span class="math inline">\(\eps_i \iid\)</span> and <span class="math inline">\(\E{\eps_i}=\E{\eps_i \mid x_i}=0\)</span> and <span class="math inline">\(\var{\eps_i}=\sigma^2\)</span>.</p>
<p>With this notation we write SSE from <a href="#eq:SSEReg1">(2.3)</a> as <span class="math display">\[
  \SSE(\hat{\mb\beta})=\Vert \mb{Y} - \mb{X}\hat{\mb\beta}\Vert^2
\]</span> and the LSE from Definition <a href="#def:SSE1">2.1</a> becomes <span class="math display">\[
  \hat{\mb\beta} = \text{ArgMin}_{\mb\beta} \SSE(\mb\beta) = \text{ArgMin}_{\mb\beta} \Vert \mb{Y} - \mb{X}\mb\beta\Vert^2.
\]</span></p>
<p>The solution of the minimisation problem that results in the LSE is given next and formulated as a theorem.</p>

<div class="theorem">
<span id="thm:LSEReg1" class="theorem"><strong>Theorem 2.1  (LSE for simple linear regression)  </strong></span> Assume that model <a href="#eq:Mod3">(2.2)</a> is correct is and that the <span class="math inline">\(n\times 2\)</span> design matrix <span class="math inline">\(\mb{X}\)</span> has rank <span class="math inline">\(2\)</span> heeft. Then, the LSE of <span class="math inline">\(\mb\beta\)</span> is given by <span class="math display">\[
  \hat{\mb\beta} = (\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}  
\]</span> and this solution is unique.
</div>

<p><strong>Proof</strong></p>
<p>First we show that <span class="math inline">\(\hat{\mb\beta} = (\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}\)</span>.</p>
<p>We find the LSE by means of vector differentiation (see Appendix <a href="#app:VecDiff">A</a>).</p>
First we write
<span class="math display">\[\begin{eqnarray*}
\|\mb{Y} - \mb{X}\mb\beta \|^2 
   &amp;=&amp; (\mb{Y} - \mb{X} \mb\beta)^t (\mb{Y}-\mb{X}\mb\beta)\\
   &amp;=&amp; \mb{Y}^t\mb{Y} - \mb\beta^t \mb{X}^t \mb{Y} - \mb{Y}^t \mb{X} \mb\beta + \mb\beta^t \mb{X}^t \mb{X} \mb\beta .
\end{eqnarray*}\]</span>
<p>Applying vector differentiation, we find <span class="math display">\[
  \frac{d}{d\mb\beta} \|\mb{Y} - \mb{X} \mb\beta\|^2 = -2\mb{X}^t\mb{Y} + 2\mb{X}^t\mb{X} \mb\beta .
\]</span></p>
<p>The LSE of <span class="math inline">\(\mb\beta\)</span> satisfies</p>
<span class="math display">\[\begin{eqnarray*}
  \frac{d}{d\mb\beta} \|\mb{Y}- \mb{X} \mb\beta\|^2 
   &amp;=&amp; 0\\
   &amp;\Updownarrow&amp; \\
  \mb{X}^t \mb{X} \mb\beta 
   &amp;=&amp; \mb{X}^t \mb{Y}
\end{eqnarray*}\]</span>
<p>The solution (<span class="math inline">\(\mb{X}\)</span> has full rank and hence <span class="math inline">\(\mb{X}^t\mb{X}\)</span> is invertible) is thus given by <span class="math display">\[
   \hat{\mb\beta} = (\mb{X}^t\mb{X})^{-1} \mb{X}^t \mb{Y}.
\]</span> For demonstrating that this estimate minimises the least squares criterion, we must show that the matrix of partial derivatives of second order is positive definite. <span class="math display">\[
    \frac{d^2}{d\mb\beta d\mb\beta^t}\|\mb{Y}- \mb{X} \mb\beta\|^2 = \frac{d}{d\mb\beta} (-2\mb{X}^t\mb{Y} + 2\mb{X}^t\mb{X} \mb\beta) = 2\mb{X}^t\mb{X}.
  \]</span> The <span class="math inline">\(2 \times 2\)</span> matrix <span class="math inline">\(\mb{X}^t\mb{X}\)</span> is positive definite because <span class="math inline">\(\mb{X}\)</span> is of full rank.</p>
<p>Finally we have to prove that we have a unique solution.</p>
<p>Suppose that there are two different solutions (say <span class="math inline">\(\hat{\mb\beta}_1\)</span> and <span class="math inline">\(\hat{\mb\beta}_2\)</span> such that <span class="math inline">\(\hat{\mb\beta}_1 \neq \hat{\mb\beta}_2\)</span>); then it holds that</p>
<span class="math display">\[\begin{eqnarray*}
  \mb{X}^t\mb{Y} 
     &amp;=&amp; \mb{X}^t \mb{X} \hat{\mb\beta}_1\\
     &amp;=&amp; \mb{X}^t \mb{X} \hat{\mb\beta}_2.
\end{eqnarray*}\]</span>
<p>Hence, <span class="math inline">\(\mb{X}^t\mb{X}(\hat{\mb\beta}_1 - \hat{\mb\beta}_2)=0\)</span>. Because <span class="math inline">\(\mb{X}^t\mb{X}\)</span> is of full rank, the unique solution of the system of equation given by <span class="math inline">\(\mb{X}^t\mb{X}\mb{v} = 0\)</span> is provided by the null solution <span class="math inline">\(\mb{v} = \mb{0}\)</span>. Therefore the following equality must hold true: <span class="math inline">\(\hat{\mb\beta}_1 -\hat{\mb\beta}_2 = \mb{0}\)</span>. Hence, the supposition <span class="math inline">\(\hat{\mb\beta}_1 \neq \hat{\mb\beta}_2\)</span> gives a contradiction and hence <span class="math inline">\(\hat{\mb\beta}_1=\hat{\mb\beta}_2\)</span> must be true, i.e. there is only one unique solution. </p>
<hr />
<p>If we work out the matrix formulation for <span class="math inline">\(\hat{\mb\beta}=(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}\)</span> we find</p>
<span class="math display">\[\begin{eqnarray*}
  \hat\beta_0 
    &amp;=&amp; \bar{Y} - \hat\beta_1 \bar{x}\\
  \hat\beta_1
    &amp;=&amp; \frac{\sum_{i=1}^n (Y_i-\bar{Y})(x_i-\bar{x})}{\sum_{i=1}^n (x_i-\bar{x})^2}.
 \end{eqnarray*}\]</span>
<p>This solution could also be found directly by computing the partial derivatives of SSE w.r.t. the two parameters, setting these derivatives to zero and solving the system of equations for the two parameters (later we will see that our solution via vector differentiation is more general and also applies to multiple linear regression).</p>
<span class="math display">\[\begin{eqnarray*}
  \frac{\partial}{\partial \beta_0} \SSE(\mb\beta)
    &amp;=&amp; 0\\
  \frac{\partial}{\partial \beta_1} \SSE(\mb\beta)
    &amp;=&amp; 0.
 \end{eqnarray*}\]</span>
<p>This gives</p>
<span class="math display" id="eq:LSEEE1" id="eq:LSEEE0">\[\begin{eqnarray}
  \frac{\partial}{\partial \beta_0} \SSE(\mb\beta)
    &amp;=&amp; -2\sum_{i=1}^n (Y_i - \beta_0-\beta_1 x_i) =0 
    \tag{2.4} \\
  \frac{\partial}{\partial \beta_1} \SSE(\mb\beta)
    &amp;=&amp; -2\sum_{i=1}^n x_i(Y_i - \beta_0-\beta_1 x_i)=0.
    \tag{2.5}
 \end{eqnarray}\]</span>
<p>Equations <a href="#eq:LSEEE0">(2.4)</a> and <a href="#eq:LSEEE1">(2.5)</a> are called the <strong>estimating equations</strong>. In the context of LSE for linear regression models they are also known as the <strong>normal equations</strong>.</p>
</div>
<div id="example-galtons-height-data-1" class="section level2 unnumbered">
<h2>Example (Galton's height data)</h2>
<p>We estimate the parameters of the regression line for Galton's data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(son.cm<span class="op">~</span>father.cm,<span class="dt">data=</span>Galton.sons)
m</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = son.cm ~ father.cm, data = Galton.sons)
## 
## Coefficients:
## (Intercept)    father.cm  
##     89.8182       0.5077</code></pre>
<p>The estimated (or fitted) regression line is thus given by <span class="math display">\[
  \hat{m}(x) = 89.8 + 0.51 x.
\]</span></p>
<p>Figure <a href="#fig:GaltonFittedReg">2.6</a> shows the scatter plot and the fitted regression line. Sometimes the fitted regression line is represented as <span class="math display">\[
  \hat{y}_i = 89.8 + 0.51 x_i .
\]</span> Interpretation: if the height of the fathers increases with 1cm, then the average height of their sons is estimated to increase with <span class="math inline">\(0.5\)</span>cm.</p>
<p>The intercept, however, has no direct physical interpretation because there are no fathers of height 0cm. This issue can be resolved by first centering the regressor. This is illustrated next.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Galton.sons&lt;-Galton.sons <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">father.cm.centered=</span>father.cm<span class="op">-</span><span class="kw">mean</span>(father.cm))
<span class="kw">mean</span>(Galton.sons<span class="op">$</span>father.cm)</code></pre></div>
<pre><code>## [1] 175.4905</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(Galton.sons<span class="op">$</span>father.cm.centered)</code></pre></div>
<pre><code>## [1] 6.242341e-15</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m2&lt;-<span class="kw">lm</span>(son.cm<span class="op">~</span>father.cm.centered,<span class="dt">data=</span>Galton.sons)
m2</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = son.cm ~ father.cm.centered, data = Galton.sons)
## 
## Coefficients:
##        (Intercept)  father.cm.centered  
##           178.9070              0.5077</code></pre>
<p>First note that the estimate of the slope remains unchanged after centering the regressor. The intercept is now estimanted by <span class="math inline">\(\hat\beta_0=178.9\)</span>. Hence, when the centered regressor equals 0, i.e. when fathers have average height (175.5cm), their sons have an estimated average height of <span class="math inline">\(178.9\)</span>cm.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(Galton.sons,
       <span class="kw">aes</span>(<span class="dt">x=</span>father.cm, <span class="dt">y=</span>son.cm)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">color=</span><span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;length of father (cm)&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;length of son (cm)&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>), <span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept=</span>m<span class="op">$</span>coefficients[<span class="dv">1</span>],<span class="dt">slope=</span>m<span class="op">$</span>coefficients[<span class="dv">2</span>]) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept=</span><span class="dv">0</span>,<span class="dt">slope=</span><span class="dv">1</span>,<span class="dt">color=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:GaltonFittedReg"></span>
<img src="DASM2_files/figure-html/GaltonFittedReg-1.png" alt="Scatter plot of the Galton data and the fitted regression line (black). The red line is the diagonal line." width="672" />
<p class="caption">
Figure 2.6: Scatter plot of the Galton data and the fitted regression line (black). The red line is the diagonal line.
</p>
</div>
<p><strong>An historical note</strong>: Figure <a href="#fig:GaltonFittedReg">2.6</a> shows the fitted regression line, but also the diagonal line. If the fitted regression line would coincide with the diagonal, then this would indicate that the average height of sons equals the height of their fathers. However, this is obviously not the case for Galton's dataset. From the graph we see that the smaller fathers have sons that are on average taller than them. On the other hand, the taller fathers have sons that are on average smaller than them. In 1886 Galton also observed this phenomenon, which he called <em>regression towards mediocrity</em>. In his 1886 paper he develop the basis of modern regression analysis (without the statistical inference); the term <em>regression</em> comes from his paper on the analysis of heights of parents and children. Nowadays, <em>regression towards mediocrity</em> is known as <em>regression to the mean</em>.</p>
<p>Finally, note that the estimates computed for this example, do not give any appreciation of the (im)precision with which they were estimated. To what extent can we thrust these estimates? To answer that question, we need the sampling distribution of the estimators. This will be discussed later in this course.</p>
</div>
<div id="exercise-blood-pressure" class="section level2 unnumbered">
<h2>Exercise: blood pressure</h2>
<p>In a small dose-finding study of a blood pressure reducing drug, 40 high blood pressure patients (systolic blood pressure at least 150 mmHg) were randomised over 4 concentrations of the active compound (<em>arginine</em>) in the drug: 0, 2, 5 and 10 mg per day. The outcome is the systolic blood pressure reduction after 2 months, measured in mmHg. The data can be read as shown in the next chunck of R code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;Data/BloodPressure.RData&quot;</span>)</code></pre></div>
<p>Fit a linear regression to the data and interpret the regression coefficient.</p>
<p><details> <summary markdown="span">Try to make this exercise yourself. If you are ready you can expand this page and look at a solution</summary></p>
<p>First we explore the dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(BloodPressure,
       <span class="kw">aes</span>(<span class="dt">x=</span>dose, <span class="dt">y=</span>bp.reduction)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">color=</span><span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;dose (mg / day&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;systolic blood pressure reduction (mmHg)&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>), <span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>))</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Next we fit the linear regression model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.bloodpressure&lt;-<span class="kw">lm</span>(bp.reduction<span class="op">~</span>dose, <span class="dt">data=</span>BloodPressure)
m.bloodpressure</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = bp.reduction ~ dose, data = BloodPressure)
## 
## Coefficients:
## (Intercept)         dose  
##     0.03304      1.78634</code></pre>
<p>From the output we read <span class="math inline">\(\hat\beta_1=\)</span> 1.7863436. Hence, we conclude that we estimate that on average the systolic blood pressure reducses with 1.8mmHg over a period of two months, with an increase of the daily dose of 1mg.</p>
<p>The next graph shows the estimated regression line.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(BloodPressure,
       <span class="kw">aes</span>(<span class="dt">x=</span>dose, <span class="dt">y=</span>bp.reduction)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">color=</span><span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;dose (mg / day&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;systolic blood pressure reduction (mmHg)&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>), <span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept=</span>m.bloodpressure<span class="op">$</span>coefficients[<span class="dv">1</span>],
              <span class="dt">slope=</span>m.bloodpressure<span class="op">$</span>coefficients[<span class="dv">2</span>]) </code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p></details></p>
</div>
<div id="S:PropLSE" class="section level2">
<h2><span class="header-section-number">2.3</span> Properties of the Least Squares Estimator</h2>

<div id="mean-and-variance-of-the-lse" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Mean and variance of the LSE</h3>

<p>The next theorem gives two important properties of the LSE. </p>

<div class="theorem">
<p><span id="thm:LSEMeanVar" class="theorem"><strong>Theorem 2.2  (Mean and variance of the LSE)  </strong></span>Assume that model <a href="#eq:Mod3">(2.2)</a> is correct and that rank(<span class="math inline">\(\mb{X}\)</span>)=<span class="math inline">\(2\)</span> (<span class="math inline">\(2\leq n\)</span>). Then the following holds</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\E{\hat{\mb\beta}}=\mb\beta\)</span> (the LSE is an <strong>unbiased</strong> estimator of <span class="math inline">\(\mb\beta\)</span>)</p></li>
<li><span class="math inline">\(\var{\hat{\mb\beta}}= (\mb{X}^t\mb{X})^{-1}\sigma^2\)</span>.
</div>
</li>
</ol>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Part 1.</p>
<p>The unbiasedness of <span class="math inline">\(\hat{\mb\beta}\)</span> follows from <span class="math display">\[
   \E{\hat{\mb\beta}} = \E{(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}}=(\mb{X}^t\mb{X})^{-1}\mb{X}^t\E{\mb{Y}}
   =(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{X}\mb\beta=\mb\beta.
 \]</span></p>
<p>Part 2.</p>
<p>For the covariance matrix of <span class="math inline">\(\hat{\mb\beta}\)</span> we will need <span class="math inline">\(\var{\mb{Y}}\)</span>. On the diagonal of this matrix we find <span class="math inline">\(\var{Y_i}=\var{\eps_i}=\sigma^2\)</span> and on the off-diagonal positions we need the covariances <span class="math inline">\(\cov{Y_i,Y_j}\)</span> (<span class="math inline">\(i\neq j\)</span>). All these covariances are equal to zero because the independence between outcomes is assumed. Hence, the covariance matrix of <span class="math inline">\(\hat{\mb\beta}\)</span> becomes</p>
<span class="math display">\[\begin{eqnarray*}
 \var{\hat{\mb\beta}}
  &amp;=&amp; \var{(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}} \\
  &amp;=&amp; (\mb{X}^t\mb{X})^{-1}\mb{X}^t \var{\mb{Y}} \left[(\mb{X}^t\mb{X})^{-1}\mb{X}^t\right]^t \\
  &amp;=&amp; (\mb{X}^t\mb{X})^{-1}\mb{X}^t \sigma^2 \mb{I}_n \mb{X}(\mb{X}^t\mb{X})^{-1} \\
  &amp;=&amp; (\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{X} (\mb{X}^t\mb{X})^{-1} \sigma^2\\
  &amp;=&amp; (\mb{X}^t\mb{X})^{-1} \sigma^2.
\end{eqnarray*}\]</span>
</div>

We also give the explicit form of <span class="math inline">\(\var{\hat{\mb\beta}}= (\mb{X}^t\mb{X})^{-1} \sigma^2\)</span> (after working out the matrix multiplication and inversion):
<span class="math display" id="eq:SigmaBetaLSE">\[\begin{equation}
   \var{\hat{\mb\beta}} = \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \begin{pmatrix}
   \frac{1}{n}\sum_{i=1}^n x_i^2   &amp; - \bar{x} \\
   -\bar{x}                                      &amp; 1 \end{pmatrix}.
   \tag{2.6}
\end{equation}\]</span>
<p>To give you a good understanding of these two properties, we extend the simulation study of Section <a href="#S:RegSimStudy">2.1</a>. For every repeated experiment, we compute the LSE of the two <span class="math inline">\(\beta\)</span>-parameters. We repeat the experiment <span class="math inline">\(N=\)</span> 100 times and then we compute the mean and the variance of the <span class="math inline">\(N=\)</span> 100 parameter estimates. Recall that we set the (true) parameter values to <span class="math inline">\(\beta_0=90\)</span>, <span class="math inline">\(\beta_1=0.5\)</span> and <span class="math inline">\(\sigma=5\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">75286</span>)
N&lt;-N100 <span class="co"># number of repeated experiments</span>
x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span>
betaHat&lt;-<span class="kw">data.frame</span>(<span class="dt">beta0Hat=</span><span class="ot">NA</span>,<span class="dt">beta1Hat=</span><span class="ot">NA</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">5</span>,<span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span>
    m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
    betaHat[i,]&lt;-<span class="kw">coef</span>(m)
}

<span class="kw">colMeans</span>(betaHat)</code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 88.8751318  0.5051252</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(betaHat)</code></pre></div>
<pre><code>##          beta0Hat    beta1Hat
## beta0Hat 2209.750 -12.6339951
## beta1Hat  -12.634   0.0723973</code></pre>
<p>The averages of the <span class="math inline">\(N=\)</span> 100 estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are indeed close to the true values. If <span class="math inline">\(N\)</span> were larger, we expect the estimates to be even closer to the true values.</p>
<p>Theorem <a href="#thm:LSEMeanVar">2.2</a> tells us that the variance of <span class="math inline">\(\hat{\mb\beta}\)</span> equals <span class="math inline">\((\mb{X}^t\mb{X})^{-1}\sigma^2\)</span>. The design matrix <span class="math inline">\(\mb{X}\)</span> for our simulation experiment is constructed in the following chunck of R code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X&lt;-<span class="kw">matrix</span>(<span class="dt">nrow=</span><span class="dv">5</span>,<span class="dt">ncol=</span><span class="dv">2</span>)
X[,<span class="dv">1</span>]=<span class="dv">1</span>
X[,<span class="dv">2</span>]=x
X</code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]    1  165
## [2,]    1  170
## [3,]    1  175
## [4,]    1  180
## [5,]    1  185</code></pre>
<p>With this matrix and with <span class="math inline">\(\sigma^2=25\)</span> we find the covariance matrix of <span class="math inline">\(\hat{\mb\beta}\)</span>. See the next chunck of R code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X)<span class="op">*</span><span class="dv">25</span></code></pre></div>
<pre><code>##        [,1]  [,2]
## [1,] 3067.5 -17.5
## [2,]  -17.5   0.1</code></pre>
<p>The results of our simulation study give only a rough approximation to this true covariance matrix. A better approximation is obtained with a larger number of repeated experiments (<span class="math inline">\(N\)</span>). We illustrate this with <span class="math inline">\(N=\)</span> 10^{4}.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">6247467</span>)
N&lt;-N10000 <span class="co"># number of repeated experiments</span>
x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span>
betaHat5&lt;-<span class="kw">data.frame</span>(<span class="dt">beta0Hat=</span><span class="ot">NA</span>,<span class="dt">beta1Hat=</span><span class="ot">NA</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">5</span>,<span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span>
    m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
    betaHat5[i,]&lt;-<span class="kw">coef</span>(m)
}

<span class="kw">colMeans</span>(betaHat)</code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 88.8751318  0.5051252</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">EmpVar5&lt;-<span class="kw">var</span>(betaHat5)
Var5&lt;-<span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X)<span class="op">*</span><span class="dv">25</span>
EmpVar5</code></pre></div>
<pre><code>##            beta0Hat    beta1Hat
## beta0Hat 3080.48493 -17.5710114
## beta1Hat  -17.57101   0.1003883</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Var5</code></pre></div>
<pre><code>##        [,1]  [,2]
## [1,] 3067.5 -17.5
## [2,]  -17.5   0.1</code></pre>
<p>To get a better understanding of the concept of <span class="math inline">\(\var{\hat{\mb\beta}}\)</span>, we repeat the simulation study but with more observations for each repeated experiment. Before this was <span class="math inline">\(n=5\)</span>. We increase this to <span class="math inline">\(n=50\)</span>. We keep the 5 father's heights, but for each height we have now 10 observations (as if we have 10 fathers of the same height, and each of these fathers has a son).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">6247467</span>)
N&lt;-N10000 <span class="co"># number of repeated experiments</span>
x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span>
x&lt;-<span class="kw">rep</span>(x,<span class="dv">10</span>) <span class="co"># the five father&#39;s heights are replicated 10 times</span>
betaHat50&lt;-<span class="kw">data.frame</span>(<span class="dt">beta0Hat=</span><span class="ot">NA</span>,<span class="dt">beta1Hat=</span><span class="ot">NA</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">50</span>,<span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span>
    m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
    betaHat50[i,]&lt;-<span class="kw">coef</span>(m)
}

<span class="kw">colMeans</span>(betaHat50)</code></pre></div>
<pre><code>##  beta0Hat  beta1Hat 
## 90.178681  0.498924</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X&lt;-<span class="kw">matrix</span>(<span class="dt">nrow=</span><span class="dv">50</span>,<span class="dt">ncol=</span><span class="dv">2</span>)
X[,<span class="dv">1</span>]=<span class="dv">1</span>
X[,<span class="dv">2</span>]=x

EmpVar50&lt;-<span class="kw">var</span>(betaHat50)
Var50&lt;-<span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X)<span class="op">*</span><span class="dv">25</span>
EmpVar50</code></pre></div>
<pre><code>##            beta0Hat    beta1Hat
## beta0Hat 311.718416 -1.77841831
## beta1Hat  -1.778418  0.01016245</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Var50</code></pre></div>
<pre><code>##        [,1]  [,2]
## [1,] 306.75 -1.75
## [2,]  -1.75  0.01</code></pre>
<p>Again we see that the means and variances of the simulated estimates agree with the true means and variances as we found from theory. The results also show that the variances of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are smaller for <span class="math inline">\(n=50\)</span> than for <span class="math inline">\(n=5\)</span>. The former is a factor 10 smaller than the latter. This factor agrees with the factor with which we increased the sample size (from <span class="math inline">\(n=5\)</span> to <span class="math inline">\(n=50\)</span>). (Try to prove this yourself.)</p>
<p>The difference between the <span class="math inline">\(n=5\)</span> and the <span class="math inline">\(n=50\)</span> scenario is demonstrated in Figure <a href="#fig:SimReg3">2.7</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:SimReg3"></span>
<img src="DASM2_files/figure-html/SimReg3-1.png" alt="histograms of $N=10000$ repeated estimates of  $\beta_1$ for $n=5$ and $n=50$. These form approximations for the sampling distributions of the estimators." width="672" />
<p class="caption">
Figure 2.7: histograms of <span class="math inline">\(N=10000\)</span> repeated estimates of <span class="math inline">\(\beta_1\)</span> for <span class="math inline">\(n=5\)</span> and <span class="math inline">\(n=50\)</span>. These form approximations for the sampling distributions of the estimators.
</p>
</div>
<p>The figure shows the histograms of the <span class="math inline">\(N=\)</span> 10^{4} estimates of <span class="math inline">\(\beta_1\)</span> for sample sizes of <span class="math inline">\(n=5\)</span> and <span class="math inline">\(n=50\)</span>. This illustrates once more that the estimator <span class="math inline">\(\hat{\mb\beta}_1\)</span> is a stochastic (or random) variable. This is easy to understand: <span class="math inline">\(\hat\beta_1= \frac{\sum_{i=1}^n (Y_i-\bar{Y})(x_i-\bar{x})}{\sum_{i=1}^n (x_i-\bar{x})^2}\)</span> is a (linear) function of <span class="math inline">\(Y_1,\ldots, Y_1\)</span>, which are randomly sampled outcomes. Hence, <span class="math inline">\(\hat\beta_1\)</span> is also a random variable and it can this be described by a distribution. This distribution is referred to as the <strong>sampling distribution</strong>. It will be discussed in some more detail later in this course. In this section we only looked at the mean and the variance of the estimator.</p>
<p>Figure <a href="#fig:SimReg3">2.7</a> demonstrates that the variance of the sampling distribution of <span class="math inline">\(\hat\beta_1\)</span> descreases with increasing sample size <span class="math inline">\(n\)</span>. The variance, which is defined as <span class="math display">\[
  \var{\hat\beta_1} = \E{(\hat\beta_1-\E{\hat\beta_1})^2},
\]</span> quantifies how the estimates, over the repeated experiments, vary about the true parameter value <span class="math inline">\(\beta_1\)</span> (note that <span class="math inline">\(\beta_1=\E{\hat\beta_1}\)</span>; unbiased estimator). Thus, the smaller the variance, the more frequent (over repeated experiments) the estimate <span class="math inline">\(\hat\beta_1\)</span> is close to the true value <span class="math inline">\(\beta_1\)</span>. In other words (cfr. definition of variance): for a large sample size <span class="math inline">\(n\)</span> we expect the estimates <span class="math inline">\(\hat\beta_1\)</span> an average closer to the true value <span class="math inline">\(\beta_1\)</span> than for a small sample size <span class="math inline">\(n\)</span>.</p>
<p>If the goal of the study is to estimate the parameter with a great <strong>precision</strong>, then the variance of the estimator must be small. We therefore say that the variance of an estimator is to be considered as a measure of the <strong>imprecision</strong> of the estimator.</p>
</div>
</div>
<div id="exercise-simulation-study" class="section level2 unnumbered">
<h2>Exercise: simulation study</h2>
<p>Repeat the previous simulation study, but now with other values of the regressor and other numbers of replicates for each regressor value. In particular, consider the setting with only two different values of the regressor (<span class="math inline">\(x=165\)</span> and <span class="math inline">\(x=185\)</span>) and at each of these values, consider 25 replicates. This makes a total of <span class="math inline">\(n=50\)</span> observations, just like in the simulation study.</p>
<p>Check whether the LSEs of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are unbiased and compute the empirical variance of the estimators from the simulation study. How do these variances compare to the variances from the previous simulation study? Can you give an explanation?</p>
<p><details> <summary markdown="span">Try to first make the exercise yourself. You can expand this page to see a solution. </summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">6247467</span>)
N&lt;-N10000 <span class="co"># number of repeated experiments</span>
x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">185</span>) <span class="co"># two regressor values</span>
x&lt;-<span class="kw">rep</span>(x,<span class="dv">25</span>) <span class="co"># the two regressor values are replicated 25 times</span>
betaHat50&lt;-<span class="kw">data.frame</span>(<span class="dt">beta0Hat=</span><span class="ot">NA</span>,<span class="dt">beta1Hat=</span><span class="ot">NA</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">50</span>,<span class="dt">sd=</span><span class="dv">5</span>) 
    m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
    betaHat50[i,]&lt;-<span class="kw">coef</span>(m)
}

<span class="kw">colMeans</span>(betaHat50)</code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 89.8835261  0.5006106</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(betaHat50)</code></pre></div>
<pre><code>##             beta0Hat     beta1Hat
## beta0Hat 152.1525837 -0.865844097
## beta1Hat  -0.8658441  0.004943341</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(betaHat50<span class="op">$</span>beta1Hat,<span class="dt">main=</span><span class="st">&quot;n=50&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;beta1-hat&quot;</span>,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">abline</span>(<span class="dt">v=</span><span class="fl">0.5</span>,<span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>The simulation study demonstrates once more that the LSE of the estimators are unbiased. When looking at the empirical variances of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> we see that they are now equal to 152.1525837 and 0.0049433, respecitvely. These are smaller than the variances we obtained from the previous simulation study in which we considered 5 equally spaced values of the regressor. The minimum and maximum values of the regressor are still 165 and 185 and the total sample size is also still equal to <span class="math inline">\(n=50\)</span>. The differences in the variances can be explained by expression <a href="#eq:SigmaBetaLSE">(2.6)</a> for <span class="math inline">\(\var{\hat{\mb\beta}}\)</span>.</p>
<p>Let us first look at <span class="math inline">\(\var{\hat\beta_1}\)</span>, which is given by <span class="math display">\[
  \frac{\sigma^2}{\sum_{i=1}^n (x_i-\bar{x})^2}.
\]</span> Recall that <span class="math inline">\(n=50\)</span> and <span class="math inline">\(\sigma^2=25\)</span> are the same here as in the previous simulation study. Also <span class="math inline">\(\bar{x}=175\)</span> is the same as before (less relevant). It is easy to confirm that the denominator <span class="math inline">\(\sum_{i=1}^n (x_i-\bar{x})^2\)</span> is larger here (5000) than for the previous simulation study (2500). For <span class="math inline">\(\var{\hat\beta_0}\)</span> you may also check that the variance for the setting in this exercise is smaller than for the previous setting.</p>
<p>More generally it can be shown that for a given <span class="math inline">\(n\)</span>, a given <span class="math inline">\(\sigma^2\)</span> and a given interval for the regressor, i.e. <span class="math inline">\(x_i \in [x_\text{min}, x_\text{max}]\)</span>, the variances of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are the smallest when half of the regressors are set to <span class="math inline">\(x_\text{min}\)</span> and the other half is set to <span class="math inline">\(x_\text{max}\)</span>.</p>
<p></details></p>
<div id="best-linear-unbiased-estimator-blue" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Best Linear Unbiased Estimator (BLUE)</h3>
<p>In the two previous sections we have investigated the variance of the LSE. We have stressed that the variance is a measure for the imprecission of an estimator. So we like to have an estimator that has a small imprecission or variance. In this section we shall demonstrate that within a certain class of estimators, and for regression model <a href="#eq:Mod3">(2.2)</a>, the LSE has the smallest variance and hence from this perspective it is the best estimator.</p>
<p>Let's describe the class of estimators of <span class="math inline">\(\mb\beta\)</span> that can be written as <span class="math inline">\(\mb{AY}\)</span>, with <span class="math inline">\(\mb{A}\)</span> a <span class="math inline">\(p\times n\)</span> matrix that may depend on <span class="math inline">\(\mb{X}\)</span>, but that may not depend on stochastic variables (such as e.g. <span class="math inline">\(\mb{Y}\)</span>). We write <span class="math inline">\(\hat{\mb\beta}^*=\mb{AY}\)</span>. The class of <strong>linear unbiased estimators</strong> is then given by estimators of the form <span class="math inline">\(\hat{\mb\beta}^*=\mb{AY}\)</span>, for which it holds that <span class="math inline">\(\E{\hat{\mb\beta}^*}=\mb\beta\)</span> (i.e. the estimator is unbiased for <span class="math inline">\(\mb\beta\)</span>).</p>
<p>The LSE <span class="math inline">\(\hat{\mb{\beta}}\)</span> is an example of a linear unbiased estimator of <span class="math inline">\(\mb\beta\)</span>. It has the form <span class="math inline">\(\mb{AY}\)</span> with <span class="math inline">\(\mb{A}=(\mb{X}^t\mb{X})^{-1}\mb{X}^t\)</span>.</p>
<p>As an example, consider the LSE of <span class="math inline">\(\beta_1\)</span>, which can be written as <span class="math display">\[
  \hat\beta_1= \frac{\sum_{i=1}^n (Y_i-\bar{Y})(x_i-\bar{x})}{\sum_{i=1}^n (x_i-\bar{x})^2}.
\]</span> Also here you see that the estimator is a linear combination of the <span class="math inline">\(n\)</span> outcomes <span class="math inline">\(Y_i\)</span>. We consider now an alternative linear estimator: <span class="math display">\[
  \hat\beta_1^*= \frac{\sum_{i=1}^n w_i(Y_i-\bar{Y})(x_i-\bar{x})}{\sum_{i=1}^n (x_i-\bar{x})^2},
\]</span> where <span class="math inline">\(w_1,\ldots, w_n\)</span> are non-negative constants for which holds th at <span class="math inline">\(\sum_{i=1}^n w_i=1\)</span> (i.e. <span class="math inline">\(w_i\)</span> are weigths). For <span class="math inline">\(\hat\beta_1^*\)</span> it still holds that it is an unbiased estimator. The variance, however, is different.</p>
<p>Within the class of linear unbiased estimators, the estimator with the smallest variancs is the <strong>best linear unbiased estimator</strong> (BLUE). The next theorem is still more general. </p>

<div class="theorem">
<span id="thm:BLUE" class="theorem"><strong>Theorem 2.3  (The LSE is the BLUE)  </strong></span>Assume that model <a href="#eq:Mod3">(2.2)</a> is correct, and consider the LSE <span class="math inline">\(\hat{\mb\beta}\)</span>. Then it holds for all <span class="math inline">\(\hat{\mb\beta}^*=\mb{AY}\)</span> for which <span class="math inline">\(\E{\hat{\mb\beta}^*}=\mb\beta\)</span>, that <span class="math display">\[
   \var{\mb{c}^t\hat{\mb\beta}} \leq \var{\mb{c}^t\hat{\mb\beta}^*} \;\;\;\text{ voor alle } \mb{c}\in\mathbb{R}^p.
\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> The estimator <span class="math inline">\(\hat{\mb\beta}^*\)</span> can always be written as <span class="math display">\[
   \hat{\mb\beta}^* = \left[(\mb{X}^t\mb{X})^{-1}\mb{X}^t+\mb{D}\right]\mb{Y}
\]</span> with <span class="math inline">\(\mb{D}=\mb{A}-(\mb{X}^t\mb{X})^{-1}\mb{X}^t\)</span>. Thus, <span class="math display">\[
  \mb{c}^t\hat{\mb\beta}^* = \mb{c}^t\left[(\mb{X}^t\mb{X})^{-1}\mb{X}^t+\mb{D}\right]\mb{Y} = \mb{c}^t \mb{\hat\beta} + \mb{c}^t\mb{DY}.
\]</span></p>
<p>Next we find an expression for the variance of <span class="math inline">\(\mb{c}^t\hat{\mb\beta}^*\)</span>: <span class="math display">\[
  \var{\mb{c}^t\hat{\mb\beta}^*} = \var{\mb{c}^t\hat{\mb\beta}} + \var{\mb{c}^t\mb{DY}} + 2\cov{\mb{c}^t\hat{\mb\beta},\mb{c}^t\mb{DY}}.
\]</span> The covariance in this expression becomes <span class="math display">\[
  \cov{\mb{c}^t\hat{\mb\beta},\mb{c}^t\mb{DY}} = \mb{c}^t(\mb{X}^t\mb{X})^{-1}\mb{X}^t\var{\mb{Y}}\mb{D}^t\mb{c} = \mb{c}^t(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{D}^t\mb{c}\sigma^2.
\]</span></p>
<p>Since <span class="math inline">\(\hat{\mb\beta}^*\)</span> is an unbiased estimator, i.e. <span class="math inline">\(\E{\hat{\mb\beta}^*}=\mb\beta\)</span>, we find for all <span class="math inline">\(\mb\beta\)</span></p>
<span class="math display">\[\begin{eqnarray*}
  \E{\mb{DY}}
  &amp;=&amp; \mb{0} \\
  \E{\mb{D}(\mb{X\beta}+\mb{\eps})}
  &amp;=&amp; \mb{0} \\
  \mb{DX\beta}+\mb{D}\E{\mb{\eps}}
  &amp;=&amp; \mb{0} \\
  \mb{DX\beta}
  &amp;=&amp;\mb{0}.
\end{eqnarray*}\]</span>
<p>Since in general <span class="math inline">\(\mb\beta\)</span> is not equal to zero, the unbiasedness of <span class="math inline">\(\hat{\mb{\beta}}^*\)</span> implies that <span class="math inline">\(\mb{DX}=\mb{0}\)</span>. With this identity, we find that the covariance <span class="math inline">\(\cov{\mb{c}^t\hat{\mb\beta},\mb{c}^t\mb{DY}}\)</span> is identical to zero.</p>
<p>The variance of <span class="math inline">\(\mb{c}^t\hat{\mb\beta}^*\)</span> thus reduces to</p>
<p><span class="math display">\[
  \var{\mb{c}^t\hat{\mb\beta}^*} = \var{\mb{c}^t\hat{\mb\beta}} + \var{\mb{c}^t\mb{DY}}.
\]</span></p>
Finaly, since variances cannot be negative, we find, for all <span class="math inline">\(\mb{c}\in\mathbb{R}^p\)</span>, <span class="math display">\[
  \var{\mb{c}^t\hat{\mb\beta}} \leq \var{\mb{c}^t\hat{\mb\beta}^*} .
\]</span>
</div>

</div>
</div>
<div id="exercise-simulation-study-1" class="section level2 unnumbered">
<h2>Exercise: Simulation study</h2>
<p>We refer here to the simulation study in <a href="#S:PropLSE">2.3</a>. Repeat this simulation study with 10000 Monte Carlo simulation runs, but now with another estimator of <span class="math inline">\(\mb\beta\)</span>. With our conventional notation, define the following esimator, <span class="math display">\[
  \tilde{\mb\beta} = (\mb{X}^t\mb{X}+d\mb{I}_2)^{-1}\mb{X}^t\mb{Y},
\]</span> with <span class="math inline">\(d&gt;0\)</span> and <span class="math inline">\(\mb{I}_2\)</span> the <span class="math inline">\(2\times 2\)</span> identity matrix. In the simulation study you may set <span class="math inline">\(d=0.005\)</span>. Compare the LSE of <span class="math inline">\(\mb\beta\)</span> with this new estimator in terms of bias and variance. What do you conclude? How does this relate to the BLUE property of the LSE? \ Also compute the <em>Mean Squared Error</em> (MSE) of the estimators, defined here as <span class="math display">\[
  \E{(\hat\beta_j-\beta_j)^2} \;\;\text{ and }\;\; \E{(\tilde\beta_j-\beta_j)^2},
\]</span> for <span class="math inline">\(j=0,1\)</span>. Recall that the expectation can be approximated by the average over many Monte Carlo simulation runs. \ What do conclude from these Mean Squared Errors?</p>
<p><details> <summary markdown="span">Try to solve this problem and then you can expend this page to look at a solution. </summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">6247467</span>)
N&lt;-N10000 <span class="co"># number of repeated experiments</span>
x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span>
X&lt;-<span class="kw">cbind</span>(<span class="dv">1</span>,x) <span class="co"># design matrix</span>
betaHatLSE&lt;-<span class="kw">data.frame</span>(<span class="dt">beta0Hat=</span><span class="ot">NA</span>,<span class="dt">beta1Hat=</span><span class="ot">NA</span>)
betaHatNew&lt;-<span class="kw">data.frame</span>(<span class="dt">beta0Hat=</span><span class="ot">NA</span>,<span class="dt">beta1Hat=</span><span class="ot">NA</span>)
MSELSE&lt;-<span class="kw">matrix</span>(<span class="dt">nrow =</span> N,<span class="dt">ncol=</span><span class="dv">2</span>)
MSENew&lt;-<span class="kw">matrix</span>(<span class="dt">nrow =</span> N,<span class="dt">ncol=</span><span class="dv">2</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
  y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">5</span>,<span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span>
  m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
  betaHatLSE[i,]&lt;-<span class="kw">coef</span>(m)
  MSELSE[i,]&lt;-(<span class="kw">coef</span>(m)<span class="op">-</span><span class="kw">c</span>(<span class="dv">90</span>,<span class="fl">0.5</span>))<span class="op">^</span><span class="dv">2</span>
  betaTilde&lt;-<span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X<span class="op">+</span><span class="fl">0.005</span><span class="op">*</span><span class="kw">diag</span>(<span class="dv">2</span>))<span class="op">%*%</span><span class="kw">t</span>(X)<span class="op">%*%</span>y
  betaHatNew[i,]&lt;-betaTilde
  MSENew[i,]&lt;-(betaTilde<span class="op">-</span><span class="kw">c</span>(<span class="dv">90</span>,<span class="fl">0.5</span>))<span class="op">^</span><span class="dv">2</span>
}

<span class="kw">colMeans</span>(betaHatLSE)</code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 90.5853872  0.4966336</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">colMeans</span>(betaHatNew)</code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 56.1436710  0.6931226</code></pre>
<p>For these averages of the estimates over the Monte Carlo runs, we conclude that the LSEs are unbiased (as we already knew), but the new estimator is biased!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">diag</span>(<span class="kw">var</span>(betaHatLSE))</code></pre></div>
<pre><code>##     beta0Hat     beta1Hat 
## 3080.4849279    0.1003883</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">diag</span>(<span class="kw">var</span>(betaHatNew))</code></pre></div>
<pre><code>##     beta0Hat     beta1Hat 
## 1.183233e+03 3.865236e-02</code></pre>
<p>The new, but biased estimator, seems to give smaller variances than the LSE. \ Didn't we learn that the LSE has the smallest bias (BLUE property)? No, the BLUE property says that the LSE has the smallest variance among all <strong>unbiased</strong> estimators. Thus our new estimator, which is biased, does not belong to the class of unbiased estimators and hence it can theoretically have a smaller variance.</p>
<p>Let's now look at the Mean Squared errors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">colMeans</span>(MSELSE)</code></pre></div>
<pre><code>## [1] 3080.5195576    0.1003896</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">colMeans</span>(MSENew)</code></pre></div>
<pre><code>## [1] 2.329366e+03 7.594484e-02</code></pre>
<p>The MSEs of the LSEs are larger than the MSEs of the new estimators. This means that on average, over many repeated experiments, the new estimates are closer to the true value of the parameter! This is also a very desirable property!</p>
<p>The new estimator that was introduces here, is known as the <strong>ridge estimator</strong>. It will turn out to be a useful estimator in high dimensional prediction problems. </details></p>
<div id="sampling-distribution-of-the-lse" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Sampling distribution of the LSE</h3>
<p>Since <span class="math inline">\(\hat{\mb\beta}\)</span> is a function of the outcome vector <span class="math inline">\(\mb{Y}\)</span> and since the outcome vector is a random variable, the estimtor <span class="math inline">\(\hat{\mb\beta}\)</span> is also a random variable. Its distribution (<strong>sampling distribution</strong>) is determined by the distribution of <span class="math inline">\(\mb{Y}\)</span>. In model <a href="#eq:Mod3">(2.2)</a> we see that <span class="math inline">\(\mb{Y} = \mb{X}\mb\beta + \mb{\eps}\)</span>, but the distribution of <span class="math inline">\(\mb{\eps}\)</span> is not fully specified (only the mean is restricted to zero). This prohibits finding the sampling distribution of <span class="math inline">\(\hat{\mb\beta}\)</span>, unless asymptotically (see further).</p>
We shall introduce an extra distribution assumption in the statistical model, and this will allow for finding the sampling distribution. Model <a href="#eq:Mod3">(2.2)</a> is extended to (in matrix notation)
<span class="math display" id="eq:Mod4">\[\begin{equation}
  \mb{Y} = \mb{X}\mb\beta + \mb{\eps}
  \tag{2.7}
\end{equation}\]</span>
<p>with <span class="math inline">\(\mb{X}\)</span> an <span class="math inline">\(n\times p\)</span> (<span class="math inline">\(p\leq n\)</span>) matrix of rank <span class="math inline">\(p\)</span>, <span class="math inline">\(\mb{\eps}^t=(\eps_1,\ldots, \eps_n)\)</span> and <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>. In this model, the error terms are assumed to be normally distributed. The model will be referred to as the <strong>normal linear regression model</strong>. For this model, the next theorem gives the sampling distribution of the LSE. </p>

<div class="theorem">
<span id="thm:DistrMod4" class="theorem"><strong>Theorem 2.4  (Sampling distribution of the LSE in the normal linear regression model)  </strong></span>Assume that model <a href="#eq:Mod4">(2.7)</a> holds. This it holds that <span class="math display">\[
   \hat{\mb\beta} \sim \text{MVN}(\mb\beta,(\mb{X}^t\mb{X})^{-1}\sigma^2). 
\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Write <span class="math inline">\(\hat{\mb\beta} = (\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}\)</span> as <span class="math inline">\(\hat{\mb\beta} =\mb{CY}\)</span>. Matrix algebra tells us that <span class="math display">\[
   \text{rang}(\mb{C})=\text{rang}(\mb{X}^t)=\text{rang}(\mb{X})=p. 
\]</span> The estimator <span class="math inline">\(\hat{\mb\beta}=\mb{CY}\)</span> is thus a vector of linear combinations of the elements in <span class="math inline">\(\mb{Y}\)</span>, which are jointly multivariate normally distributed (see Lemma <a href="#lem:LinTransNorm">B.1</a> in Appendix <a href="#app:LinTrans">B</a>. The mean and the variance of <span class="math inline">\(\hat{\mb\beta}\)</span> were already given in Theorem <a href="#thm:LSEMeanVar">2.2</a>.</p>
Hence, <span class="math display">\[
   \hat{\mb\beta} \sim \text{MVN}(\mb\beta, (\mb{X}^t\mb{X})^{-1}\sigma^2).
\]</span>
</div>

<p>We now repeat the simulation study and this time we will use normal QQ-plots to check whether the sampling distribution is indeed a normal distribution. The results are shown in Figure <a href="#fig:SimReg4">2.8</a>. The QQ-plots clearly show that the sampling distribution of <span class="math inline">\(\hat\beta_1\)</span> is normal.</p>
<div class="figure" style="text-align: center"><span id="fig:SimReg4"></span>
<img src="DASM2_files/figure-html/SimReg4-1.png" alt="Normal QQ-plots of the $N=10000$ repeated estimates of $\beta_1$ for $n=5$ en $n=50$" width="672" />
<p class="caption">
Figure 2.8: Normal QQ-plots of the <span class="math inline">\(N=10000\)</span> repeated estimates of <span class="math inline">\(\beta_1\)</span> for <span class="math inline">\(n=5\)</span> en <span class="math inline">\(n=50\)</span>
</p>
</div>
<p>If the normality assumption of model <a href="#eq:Mod4">(2.7)</a> is violated, but the assumptions of model <a href="#eq:Mod3">(2.2)</a> do hold, then we can still find the sampling distribution of <span class="math inline">\(\hat{\mb\beta}\)</span>, but only for large sample sizes. Without proof, the result is stated in the following theorem. It is an <strong>asymptotic result</strong>, which means that it holds in the limit for sample sizes <span class="math inline">\(n\)</span> going to infinity. Fortunately, such asymptotic results often hold approximately for large, but finite sample sizes. </p>

<div class="theorem">
<span id="thm:DistrMod3" class="theorem"><strong>Theorem 2.5  (Asymptotic sampling distribution of the LSE)  </strong></span>Assume that (1) model <a href="#eq:Mod3">(2.2)</a> holds, (2) the regressor values are fixed by design and (3) that <span class="math inline">\(\lim_{n\rightarrow \infty} \frac{1}{n}\sum_{i=1}^n\mb{x}_i\mb{x}_i^t\)</span> has rank equal to 2. Then, as <span class="math inline">\(n\rightarrow \infty\)</span>, <span class="math display">\[
 \sqrt{n}(\hat{\mb\beta}-\mb\beta)\mb\Sigma_n^{-1} \convDistr \text{MVN}(\mb{0},\mb{I}_2)  
\]</span> with
<span class="math display" id="eq:SigmaBetaLSEAsymp">\[\begin{equation}
   \mb\Sigma_n = \frac{\sigma^2}{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2} \begin{pmatrix}
   \frac{1}{n}\sum_{i=1}^n x_i^2   &amp; - \bar{x} \\
   -\bar{x}                                      &amp; 1 \end{pmatrix}.
   \tag{2.8}
\end{equation}\]</span>
</div>

<p>Some notes regarding this theorem:</p>
<ul>
<li><p>the construction <span class="math inline">\(n\rightarrow \infty T_n \convDistr T\)</span> tells us that the distribution of the stochastic variance <span class="math inline">\(T_n\)</span>, which is based on a sample size of <span class="math inline">\(n\)</span>, converges to the distribution of the random variable <span class="math inline">\(T\)</span>, when the sample size goes to infinity. In the theorem, <span class="math inline">\(T_n\)</span> is the LSE based on a sample of size <span class="math inline">\(n\)</span>, and <span class="math inline">\(T\)</span> is a MVN random variable with mean <span class="math inline">\(\mb{0}\)</span> and covariance matrix <span class="math inline">\(\mb{I}_2\)</span> (<span class="math inline">\(2\times 2\)</span> identity matrix).</p></li>
<li><p>the distribution of <span class="math inline">\(T\)</span> (or the MVN distribution in the theorem) is referred to as the <strong>asymptotic sampling distribution</strong> of <span class="math inline">\(T_n\)</span>.</p></li>
<li><p>in contrast to the <em>asymptotic</em> sampling distribution, we use the term <strong>exact sampling distribution</strong> to refer to a sampling distribution that is correct even for small sample sizes <span class="math inline">\(n\)</span>. Such exact sampling distributions often require strong distributional assumptions.</p></li>
<li><p>the second condition in the theorem says that the regressor values must be <strong>fixed by design</strong>. This means that the regressor may not be a random variable (as it is in the Galton example). Here is an example of a <strong>fixed design</strong>: Randomly sample 10 subjects of each of the following ages: 20, 40, 60 and 70 years old (note: these ages are fixed prior to the executation of the study). For each of the <span class="math inline">\(10\times 4=40\)</span> subjects, measure the blood pressure. In this example, the blood pressure is the outcome (random variable) and the age is the regressor. However, since the ages were fixed by design, this is an example of a fixed design.</p></li>
<li><p>although the theorem only gives the asymptotic sampling distribution for fixed designs, a similar theorem exists for <strong>random designs</strong>. The second condition need to be reformulated such that it makes sense for random regressors (details not given here). The sampling distribution is the same as for fixed designs.</p></li>
</ul>
<p>We now demonstrate the practical meaning of the theorem in a simulation study. We repeat the same simulations as before, but now with error terms <span class="math inline">\(\eps_i\)</span> that are not normally distributed. We choose <span class="math inline">\(\eps_i\)</span> to be distributed as an <em>exponential distribution</em>. Figure <a href="#fig:ExpDistr">2.9</a> shows the shape of an exponential distribution with variance 1 en centered such that the mean is equal to zero (this is a requirement of error terms in our linear regression models).</p>
<div class="figure" style="text-align: center"><span id="fig:ExpDistr"></span>
<img src="DASM2_files/figure-html/ExpDistr-1.png" alt="Histogram of 10000 error terms from an exponential distribution (centered to make the mean equal to zero)" width="672" />
<p class="caption">
Figure 2.9: Histogram of 10000 error terms from an exponential distribution (centered to make the mean equal to zero)
</p>
</div>
<p>The next chunck or R code gives a simulation study in which we simulate <span class="math inline">\(n=5\)</span>, <span class="math inline">\(n=50\)</span> and <span class="math inline">\(n=200\)</span> outcomes according to Model <a href="#eq:Mod3">(2.2)</a> with a centered exponential distribution for the error term. The results show that the LSE is still unbiased (this property does not require the normality assumption). Figure <a href="#fig:SimReg5">2.10</a> shows three normal QQ-plots of the estimates <span class="math inline">\(\hat\beta_1\)</span> for the three sample sizes. For <span class="math inline">\(n=5\)</span> the sampling distribution of <span class="math inline">\(\hat\beta_1\)</span> is clearly not normal, but as the sample size <span class="math inline">\(n\)</span> increases, the approxiation to a normal distribution becomes better.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">6247467</span>)

<span class="co"># definieren een functie voor het simuleren</span>

simRegressionExp&lt;-<span class="cf">function</span>(<span class="dt">N=</span><span class="dv">1000</span>,<span class="dt">nRep=</span><span class="dv">1</span>) {
  <span class="co"># N: number of repeated samples</span>
  <span class="co"># nRep: number of replicated for each value of the regressor</span>
  
  x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># 5 father&#39;s heights</span>
  x&lt;-<span class="kw">rep</span>(x,nRep) <span class="co"># the five regrossor values are replaciated nRep times</span>
  x
  betaHat&lt;-<span class="kw">data.frame</span>(<span class="dt">beta0Hat=</span><span class="ot">NA</span>,<span class="dt">beta1Hat=</span><span class="ot">NA</span>)
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
      y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span>(<span class="kw">rexp</span>(<span class="dv">5</span><span class="op">*</span>nRep)<span class="op">-</span><span class="dv">1</span>) <span class="co"># random sample of outcomes</span>
      m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
      betaHat[i,]&lt;-<span class="kw">coef</span>(m)
  }

  <span class="kw">return</span>(betaHat)
}
  
<span class="co"># for n=5</span>
betaHat5&lt;-<span class="kw">simRegressionExp</span>(<span class="dt">N=</span>N1000,<span class="dt">nRep=</span><span class="dv">1</span>)
<span class="kw">colMeans</span>(betaHat5)</code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 89.6311933  0.5020559</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X&lt;-<span class="kw">matrix</span>(<span class="dt">nrow=</span><span class="dv">5</span>,<span class="dt">ncol=</span><span class="dv">2</span>)
X[,<span class="dv">1</span>]&lt;-<span class="dv">1</span>
X[,<span class="dv">2</span>]&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>)

EmpVar&lt;-<span class="kw">var</span>(betaHat5)
Var&lt;-<span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X)<span class="op">*</span><span class="dv">1</span> 
EmpVar</code></pre></div>
<pre><code>##             beta0Hat     beta1Hat
## beta0Hat 124.3967926 -0.712668233
## beta1Hat  -0.7126682  0.004089645</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Var</code></pre></div>
<pre><code>##       [,1]   [,2]
## [1,] 122.7 -0.700
## [2,]  -0.7  0.004</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># for n=50</span>
betaHat50&lt;-<span class="kw">simRegressionExp</span>(<span class="dt">N=</span>N1000,<span class="dt">nRep=</span><span class="dv">10</span>)
<span class="kw">colMeans</span>(betaHat50)</code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 90.0568710  0.4997006</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(betaHat50)</code></pre></div>
<pre><code>##             beta0Hat      beta1Hat
## beta0Hat 11.71799942 -0.0668749613
## beta1Hat -0.06687496  0.0003822612</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># for n=200</span>
betaHat200&lt;-<span class="kw">simRegressionExp</span>(<span class="dt">N=</span>N1000,<span class="dt">nRep=</span><span class="dv">40</span>)
<span class="kw">colMeans</span>(betaHat200)</code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 89.9934692  0.5000403</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(betaHat200)</code></pre></div>
<pre><code>##             beta0Hat     beta1Hat
## beta0Hat  3.08244132 -0.017619947
## beta1Hat -0.01761995  0.000100889</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))
<span class="kw">qqnorm</span>(betaHat5<span class="op">$</span>beta1Hat,<span class="dt">main=</span><span class="st">&quot;n=5&quot;</span>)
<span class="kw">qqline</span>(betaHat5<span class="op">$</span>beta1Hat)
<span class="kw">qqnorm</span>(betaHat50<span class="op">$</span>beta1Hat,<span class="dt">main=</span><span class="st">&quot;n=50&quot;</span>)
<span class="kw">qqline</span>(betaHat50<span class="op">$</span>beta1Hat)
<span class="kw">qqnorm</span>(betaHat200<span class="op">$</span>beta1Hat,<span class="dt">main=</span><span class="st">&quot;n=200&quot;</span>)
<span class="kw">qqline</span>(betaHat200<span class="op">$</span>beta1Hat)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:SimReg5"></span>
<img src="DASM2_files/figure-html/SimReg5-1.png" alt="Histograms of the $N=1000$ herhaalde estimates of $\beta_1$ for $n=5$, $n=50$ and $n=200$, with centered exponentially distributed error terms." width="672" />
<p class="caption">
Figure 2.10: Histograms of the <span class="math inline">\(N=1000\)</span> herhaalde estimates of <span class="math inline">\(\beta_1\)</span> for <span class="math inline">\(n=5\)</span>, <span class="math inline">\(n=50\)</span> and <span class="math inline">\(n=200\)</span>, with centered exponentially distributed error terms.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>

</div>
<div id="maximum-likelihood-estimator-of-hatmbbeta" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Maximum likelihood estimator of <span class="math inline">\(\hat{\mb\beta}\)</span></h3>
<p>The LSE does need distributional assumptions, such as normality. The method of least squares is thus a parameter estimation method that can be applied to <strong>semiparametric statistical models</strong> such as Model <a href="#eq:Mod3">(2.2)</a>.</p>
<p>For fully parametric statistical model, such as Model <a href="#eq:Mod4">(2.7)</a>, the method of <strong>maximum likelihood</strong> becomes applicable for parameter estimation. With normally distributed error terms, as in Model <a href="#eq:Mod4">(2.7)</a>, it can be shows that the LSE is equivalent to the <strong>maximum likelihood estimator</strong> (MLE). This is demonstrated in this section.</p>
<p>Model <a href="#eq:Mod4">(2.7)</a> is equivalent to <span class="math display">\[
  Y_i \mid x_i \sim N(\beta_0+\beta_1 x_i,\sigma^2).
\]</span> This normal distribution has density function <span class="math display">\[
  f(y;x,\beta_0,\beta_1,\sigma^2) = (2\pi\sigma^2)^{-1/2} \exp\left[-\frac{1}{2}\frac{(y-\beta_0-\beta_1 x )^2}{\sigma^2} \right].
\]</span></p>
<p>Since it is assumed that all <span class="math inline">\(n\)</span> outcomes are mutually independent, the likelihood function becomes <span class="math display">\[
L(\beta_0,\beta_1,\sigma^2) = \prod_{i=1}^n f(Y_i;x_i,\beta_0,\beta_1,\sigma^2) 
=(2\pi\sigma^2)^{-n/2} \exp\left[-\frac{1}{2} \sum_{i=1}^n\frac{(Y_i-\beta_0-\beta_1 x_i)^2}{\sigma^2} \right].
\]</span> Hence, the log-likelihood function is given by <span class="math display">\[
  l(\beta_0,\beta_1,\sigma^2) =\ln L(\beta_0,\beta_1,\sigma^2)
  = -\frac{n}{2}\ln(2\pi\sigma^2)-\frac{1}{2}\sum_{i=1}^n\frac{(Y_i-\beta_0-\beta_1 x_i)^2}{\sigma^2}.
\]</span> In matrix notation this becomes <span class="math display">\[
  l(\beta_0,\beta_1,\sigma^2) = -\frac{n}{2}\ln(2\pi\sigma^2)-\frac{1}{2\sigma^2} (\mb{Y}-\mb{X}\mb\beta)^t(\mb{Y}-\mb{X}\mb\beta).
\]</span> The MLE of <span class="math inline">\(\mb\beta\)</span> is defined as <span class="math display">\[
  \hat{\mb\beta} = \text{ArgMax}_{\mb\beta \in \mathbb{R}^2} l(\mb\beta,\sigma^2).
\]</span> We therefore need the partial derivative of the log-likelihood w.r.t. <span class="math inline">\(\mb\beta\)</span>, <span class="math display">\[
\frac{\partial}{\partial \mb\beta} l(\mb\beta,\sigma^2) = \frac{1}{\sigma^2}\mb{X}^t(\mb{Y}-\mb{X}\mb\beta).
\]</span> Setting this partial derivative equal to zero, gives exactly the normal equations of the LSE (see Theorem <a href="#thm:LSEReg1">2.1</a>).</p>
The MLE of the parameter <span class="math inline">\(\sigma^2\)</span> is the solution to the equation <span class="math display">\[
  \frac{\partial}{\partial \sigma^2} l(\mb\beta,\sigma^2) = 0.
\]</span> After some algebra, we find the MLE
<span class="math display" id="eq:MLESigma2">\[\begin{equation}
  \hat\sigma^2=\frac{1}{n}\sum_{i=1}^n (Y_i - \hat\beta_0-\hat\beta_1 x_i)^2.
  \tag{2.9}
\end{equation}\]</span>
<p>This estimator, however, is not unbiased (without proof), but it is <strong>asymptotically unbiased</strong>, i.e. <span class="math display">\[
  \lim_{n\rightarrow \infty} \E{\hat\sigma^2} = \sigma^2.
\]</span></p>
<p>In the next section we will develop an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
</div>
<div id="an-estimator-of-sigma2" class="section level2">
<h2><span class="header-section-number">2.4</span> An Estimator of <span class="math inline">\(\sigma^2\)</span></h2>
<p>We now know the sampling distribution of the LSE of <span class="math inline">\(\mb\beta\)</span>, but this distribution depends on the variance of the error term, <span class="math inline">\(\sigma^2\)</span>, and this variance is still unknown. To turn the sampling distribution of <span class="math inline">\(\hat{\mb\beta}\)</span> into an instrument that can be used with real data (e.g. for calculating confidence intervals and performing hypothesis tests), we will also need an estimator of <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The estimator (and its unbiasedness) are given in the following theorem. We give the result without a proof, but note that it does not require the normality assumption. Als note that the estimator is similar to the MLE of Equation <a href="#eq:MLESigma2">(2.9)</a>, except that the MLE has a factor <span class="math inline">\(1/n\)</span> instead of a factor <span class="math inline">\(1/(n-2)\)</span>. The estimator of <span class="math inline">\(\sigma^2\)</span> can be denoted by <span class="math inline">\(\hat\sigma^2\)</span>, but it is also known as MSE, which stands for the <strong>mean squared error</strong>. This terminology will become clear later. </p>

<div class="theorem">
<span id="thm:ExpectationMSE" class="theorem"><strong>Theorem 2.6  (An unbiased estimator of <span class="math inline">\(\sigma^2\)</span>)  </strong></span>Assume that model <a href="#eq:Mod3">(2.2)</a> holds true, and let <span class="math inline">\(\mb{x}_i^t\)</span> denote the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mb{X}\)</span> (<span class="math inline">\(i=1,\ldots, n\)</span>). Then <span class="math display">\[
  \MSE = \frac{\SSE}{n-2} = \frac{\sum_{i=1}^n (Y_i-\mb{x}_i^t\hat{\mb\beta})^2}{n-2} =
    \frac{(\mb{Y}-\mb{X}\hat{\mb\beta})^t (\mb{Y}-\mb{X}\hat{\mb\beta})}{n-2}
\]</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>.
</div>

<p>For further purposes, we will also need the sampling distribution of MSE. Note that for the property of unbiasedness the normality assumption was not required, but it will be for developping the sampling distribution. The next theorem is given without proof. </p>

<div class="theorem">
<span id="thm:DistrMSE" class="theorem"><strong>Theorem 2.7  (Sampling distribution of MSE)  </strong></span>Assume that Model <a href="#eq:Mod4">(2.7)</a> is correct. Then, <span class="math display">\[
 \frac{(n-2) \MSE}{\sigma^2} \sim \chi^2_{n-2}.
\]</span>
</div>

</div>
<div id="exercise-simulation-study-2" class="section level2 unnumbered">
<h2>Exercise: Simulation study</h2>
<p>Set up a simulation study to empirically demonstrate that the MLE of <span class="math inline">\(\sigma^2\)</span> is asymptotically unbiased. So we want you to repeat a simulation study for several choices of the sample size <span class="math inline">\(n\)</span> so as to show that the bias reduces as <span class="math inline">\(n\)</span> increases.</p>
<p><details> <summary markdown="span">Try to solve this problem and then you can expend this page to look at a solution. </summary></p>
<p>Since we have to repeat a simulation study for several choices of the sample size <span class="math inline">\(n\)</span>, I will write an R function to perform the simulation study. This is given in the next chunck of R code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">simulate.regression&lt;-<span class="cf">function</span>(<span class="dt">n=</span><span class="dv">10</span>,<span class="dt">sigma2=</span><span class="dv">1</span>,<span class="dt">beta0=</span><span class="dv">1</span>,<span class="dt">beta1=</span><span class="dv">1</span>,<span class="dt">N=</span><span class="dv">100</span>) {
  <span class="co"># function that simulates data from a regression model. </span>
  <span class="co"># The result of the function contains the averages of the MLE estimates of sigma^2, </span>
  <span class="co">#    as well as of the unbiased estimates MSE</span>
  
  <span class="co"># n: sample size</span>
  <span class="co"># sigma2: variance of the error term</span>
  <span class="co"># beta0 and beta1: regression parameters</span>
  <span class="co"># N: number of Monte Carlo simulations</span>
  
  sigma2.MLE&lt;-<span class="kw">c</span>() <span class="co"># initiation of vector that will contain the MLEs</span>
  sigma2.MSE&lt;-<span class="kw">c</span>() <span class="co"># initiation of vector that will contain the MSEs</span>
  x&lt;-<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">10</span>,<span class="dt">length.out =</span> n) <span class="co"># vector with n equally spaced regressor values between 1 and 10</span>
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    y&lt;-beta0<span class="op">+</span>beta1<span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(n,<span class="dt">sd=</span><span class="kw">sqrt</span>(sigma2)) <span class="co"># simulate outcome data</span>
    m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
    sigma2.MLE&lt;-<span class="kw">c</span>(sigma2.MLE,
                  <span class="kw">mean</span>(<span class="kw">residuals</span>(m)<span class="op">^</span><span class="dv">2</span>))
    sigma2.MSE&lt;-<span class="kw">c</span>(sigma2.MSE,
                  <span class="kw">sum</span>(<span class="kw">residuals</span>(m)<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>(n<span class="op">-</span><span class="dv">2</span>))
  }
  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">MLE=</span><span class="kw">mean</span>(sigma2.MLE),<span class="dt">MSE=</span><span class="kw">mean</span>(sigma2.MSE)))
}</code></pre></div>
<p>Now we will apply the function (i.e. perform the simulation study) for sample sizes ranging from <span class="math inline">\(n=3\)</span> to <span class="math inline">\(n=100\)</span> and plot the results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">91869</span>)
sample.sizes&lt;-<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">20</span>,<span class="dv">50</span>,<span class="dv">75</span>,<span class="dv">100</span>) 
sigma2.MLE&lt;-<span class="kw">c</span>()
sigma2.MSE&lt;-<span class="kw">c</span>()

<span class="cf">for</span>(n <span class="cf">in</span> sample.sizes) {
  s2&lt;-<span class="kw">simulate.regression</span>(<span class="dt">n=</span>n,<span class="dt">N=</span>N1000)
  sigma2.MLE&lt;-<span class="kw">c</span>(sigma2.MLE,s2<span class="op">$</span>MLE)
  sigma2.MSE&lt;-<span class="kw">c</span>(sigma2.MSE,s2<span class="op">$</span>MSE)
}

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(sample.sizes,sigma2.MLE,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">1.2</span>),
     <span class="dt">xlab=</span><span class="st">&quot;sample size&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;MLE estimate of sigma^2&quot;</span>,
     <span class="dt">main=</span><span class="st">&quot;MLE&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">1</span>,<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="dv">2</span>)

<span class="kw">plot</span>(sample.sizes,sigma2.MSE,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">1.2</span>),
     <span class="dt">xlab=</span><span class="st">&quot;sample size&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;MSE estimate of sigma^2&quot;</span>,
     <span class="dt">main=</span><span class="st">&quot;MSE&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">1</span>,<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p>The results illustrate that the MLE is biased for small sample sizes, but the bias disappears for larger sample sizes. The MSE, on the other hand, is unbiased even for very small sample sizes.</p>
<p></details></p>
</div>
<div id="sampling-distributions-of-the-standardised-and-the-studentised-lse" class="section level2">
<h2><span class="header-section-number">2.5</span> Sampling Distributions of the Standardised and the Studentised LSE</h2>

<p>In the previous sections we have developped the sampling distributions of the LSE and of the unbiased estimator of <span class="math inline">\(\sigma^2\)</span>. For the construction of confidence intervals and hypothesis tests we will often work with a transformation of the LSE such that the sampling distribution of the transformed LSE does no longer depend on parameters that need to be estimated.</p>
<p>We introduce the notation <span class="math inline">\(\sigma_{\beta_j}^2=\var{\hat\beta_j}\)</span> for the variance of <span class="math inline">\(\hat\beta_j\)</span> (<span class="math inline">\(j=0,1\)</span>). This is thus the appropriate diagonal element of <span class="math inline">\(\var{\hat{\mb\beta}}=(\mb{X}^t\mb{X})^{-1}\sigma^2\)</span>. The latter is often denoted by <span class="math inline">\(\mb\Sigma_{\mb\beta}\)</span>.</p>
<p>With this notation, the <strong>standardised</strong> parameter estimator of <span class="math inline">\(\beta_j\)</span> is then given by <span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{\sigma_{\beta_j}}.
\]</span> The following corollary is given without proof. </p>

<div class="corollary">
<span id="cor:DistrStandBeta" class="corollary"><strong>Corollary 2.1  (Sampling distribution of the standardised LSE)  </strong></span>Assume that Model <a href="#eq:Mod4">(2.7)</a> holds true. Then, <span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{\sigma_{\beta_j}} \sim N(0,1).
\]</span> Assume that Model <a href="#eq:Mod3">(2.2)</a> holds true. Then, as <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{\sigma_{\beta_j}} \convDistr N(0,1).
\]</span>
</div>

<p>When the variance <span class="math inline">\(\sigma^2\)</span> is not known, it can be replaced by its estimator MSE. The estimator of <span class="math inline">\(\var{\hat{\mb\beta}}=(\mb{X}^t\mb{X})^{-1}\sigma^2\)</span> is often denoted by <span class="math inline">\(\hat{\mb\Sigma}_{\mb\beta}=(\mb{X}^t\mb{X})^{-1}\MSE\)</span> and the estimator of <span class="math inline">\(\sigma^2_{\beta_j}\)</span> by <span class="math inline">\(\hat\sigma^2_{\beta_j}\)</span> or by <span class="math inline">\(S^2_{\beta_j}\)</span>. The square root of <span class="math inline">\(S^2_{\beta_j}\)</span>, i.e. <span class="math inline">\(S_{\beta_j}\)</span>, is also known as the <strong>standard error</strong> (SE or se) of the paramater estimator <span class="math inline">\(\hat\beta_j\)</span>.</p>
<p>The <strong>studentised</strong> estimator of <span class="math inline">\(\beta_j\)</span> is then defined as <span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{\hat\sigma_{\beta_j}}=\frac{\hat\beta_j-\beta_j}{S_{\beta_j}}.
\]</span> The following theory gives the (asymptotic) sampling distribution of the stundentised estimators (without proof). </p>

<div class="theorem">
<span id="thm:DistrStudBeta" class="theorem"><strong>Theorem 2.8  (Sampling Distribution of the studentised LSE)  </strong></span>Assume that Model <a href="#eq:Mod4">(2.7)</a> holds true. Then it holds that <span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{S_{\beta_j}} \sim t_{n-2}.
\]</span> Assume that Model <a href="#eq:Mod3">(2.2)</a> holds true. Then, as <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{S_{\beta_j}} \convDistr N(0,1).
\]</span>
</div>

</div>
<div id="S:BIReg1" class="section level2">
<h2><span class="header-section-number">2.6</span> Confidence Intervals</h2>
<p>Theorem <a href="#thm:DistrStudBeta">2.8</a> gives the (asymptotic) sampling distribution of the estimators <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>. This forms the basis for confidence intervals. In this section we give the result for the normal linear regression model <a href="#eq:Mod4">(2.7)</a>, for which we have developped the exact sampling distributions.</p>
<p>We will use the notation <span class="math inline">\(\sigma^2_{\beta_0}=\var{\hat\beta_0}\)</span> and <span class="math inline">\(\sigma^2_{\beta_1}=\var{\hat\beta_1}\)</span>. These are the diagonal elements of the covariance matrix <span class="math inline">\(\mb\Sigma_\beta\)</span>. Theorem <a href="#thm:DistrStudBeta">2.8</a> implies</p>
<span class="math display" id="eq:tmp998765615">\[\begin{equation}
  \frac{\hat\beta_1 - \beta_1}{\hat\sigma_{\beta_1}} \sim t_{n-2} .
  \tag{2.10}
\end{equation}\]</span>
<p>For a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom, say <span class="math inline">\(T\sim t_{n-2}\)</span>, it follows by definition that <span class="math display">\[
   \prob{-t_{n-2;1-\alpha/2} &lt; T &lt; t_{n-2;1-\alpha/2}} = 1-\alpha.
\]</span> Hence, with <span class="math inline">\(T=\frac{\hat\beta_1 - \beta_1}{\hat\sigma_{\beta_1}}\sim t_{n-2}\)</span>, the identity <span class="math display">\[
  \prob{-t_{n-2;1-\alpha/2} &lt; \frac{\hat\beta_1 - \beta_1}{\hat\sigma_{\beta_1}} &lt; t_{n-2;1-\alpha/2}} = 1-\alpha
\]</span> implies that <span class="math display">\[
  \prob{\hat\beta_1-t_{n-2;1-\alpha/2} \hat\sigma_{\beta_1}&lt;  \beta_1 &lt; \hat\beta_1+t_{n-2;1-\alpha/2} \hat\sigma_{\beta_1}} = 1-\alpha.
\]</span> From this equality, the <span class="math inline">\(1-\alpha\)</span> <strong>confidence interval</strong> (CI) of <span class="math inline">\(\beta_1\)</span> follows directly: <span class="math display">\[
 \left[\hat\beta_1-t_{n-2;1-\alpha/2} \hat\sigma_{\beta_1}, \hat\beta_1+t_{n-2;1-\alpha/2} \hat\sigma_{\beta_1}\right].
\]</span> The construction of the confidence interval of <span class="math inline">\(\beta_0\)</span> is analogous.</p>
<p>The interpretation of a confidence interval is now demonstrated by means of repeated sampling in a simulation study. We start with a small sample size of <span class="math inline">\(n=5\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">267213</span>)

simRegressionBI&lt;-<span class="cf">function</span>(<span class="dt">N=</span><span class="dv">10000</span>,<span class="dt">nRep=</span><span class="dv">1</span>) {
  <span class="co"># N: number of repeated samples</span>
  <span class="co"># nRep: number of replicated for each value of the regressor</span>
  
  x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># 5 father&#39;s heights</span>
  x&lt;-<span class="kw">rep</span>(x,nRep) <span class="co"># the five regrossor values are replaciated nRep times</span>
  x
  
  Results&lt;-<span class="kw">data.frame</span>(<span class="dt">beta1Hat=</span><span class="ot">NA</span>,<span class="dt">CI.lower=</span><span class="ot">NA</span>,<span class="dt">CI.upper=</span><span class="ot">NA</span>,
                      <span class="dt">cover=</span><span class="ot">NA</span>)
  
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
      y&lt;-<span class="dv">90</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">5</span><span class="op">*</span>nRep,<span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># random sample of outcomes</span>
      m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
      Results[i,<span class="dv">1</span>]&lt;-<span class="kw">coef</span>(m)[<span class="dv">2</span>]
      Results[i,<span class="dv">2</span><span class="op">:</span><span class="dv">3</span>]&lt;-<span class="kw">confint</span>(m)[<span class="dv">2</span>,] <span class="co"># default is a 95% CI</span>
      Results[i,<span class="dv">4</span>]&lt;-(<span class="fl">0.5</span><span class="op">&lt;</span>Results[i,<span class="dv">3</span>])<span class="op">&amp;</span>(Results[i,<span class="dv">2</span>]<span class="op">&lt;</span><span class="st"> </span><span class="fl">0.5</span>) <span class="co"># 0.5 is true parameter value</span>
  }

  <span class="kw">return</span>(Results)
}

plotBI&lt;-<span class="cf">function</span>(SimBI,<span class="dt">nPlot=</span><span class="kw">nrow</span>(SimBI),<span class="dt">mn=</span><span class="kw">min</span>(SimBI<span class="op">$</span>CI.lower)
                 ,<span class="dt">mx=</span><span class="kw">max</span>(SimBI<span class="op">$</span>CI.upper),...) {
  <span class="co"># SimBI: results of the function SimRegressionBI</span>
  <span class="co"># nPlot: number of repeated experiments that need to be plotted</span>
  <span class="co"># mn: lower limit horizontal axis</span>
  <span class="co"># mx: upper limit horizontal axis</span>
  
  <span class="kw">plot</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dt">xlim=</span><span class="kw">c</span>(mn,mx),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,nPlot),<span class="dt">xlab=</span><span class="st">&quot;&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;experiment&quot;</span>,
       <span class="dt">type=</span><span class="st">&quot;n&quot;</span>,...)
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nPlot) {
      <span class="kw">arrows</span>(SimBI<span class="op">$</span>CI.lower[i],i,SimBI<span class="op">$</span>CI.upper[i],i,
             <span class="dt">code=</span><span class="dv">0</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
             <span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">col=</span><span class="dv">2</span><span class="op">-</span>SimBI<span class="op">$</span>cover[i])
    <span class="kw">points</span>(SimBI<span class="op">$</span>beta1Hat[i],i)
  }
  <span class="kw">abline</span>(<span class="dt">v=</span><span class="fl">0.5</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">1</span>)
}

SimBI5&lt;-<span class="kw">simRegressionBI</span>(<span class="dt">N=</span>N1000,<span class="dt">nRep=</span><span class="dv">1</span>)
<span class="kw">mean</span>(SimBI5<span class="op">$</span>cover) <span class="co"># empirical coverage</span></code></pre></div>
<pre><code>## [1] 0.955</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plotBI</span>(SimBI5,<span class="dt">nPlot=</span><span class="dv">30</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:SimBI5"></span>
<img src="DASM2_files/figure-html/SimBI5-1.png" alt="95\% confidence intervals from repeated sampling ($n=5$). The points represent the point estimates $\hat\beta_1$. Only the results of the first 30 repeated experiments are shown. The vertical line indicates the true parameter value: $\beta_1=0.5$." width="672" />
<p class="caption">
Figure 2.11: 95% confidence intervals from repeated sampling (<span class="math inline">\(n=5\)</span>). The points represent the point estimates <span class="math inline">\(\hat\beta_1\)</span>. Only the results of the first 30 repeated experiments are shown. The vertical line indicates the true parameter value: <span class="math inline">\(\beta_1=0.5\)</span>.
</p>
</div>
<p>From the output we read the <strong>empirical coverage</strong> of the <span class="math inline">\(95\%\)</span> confidence interval: 0.955. This is based on 1000 repeated experiments. For a large number of repeated experiments, the empirical coverage is a good approximation of the true coverage probability. In our simulation study, the empirical coverage is (approximately) equal to the <strong>nominal</strong> <span class="math inline">\(95\%\)</span> confidence level, which (empirically) demonstrates that the theory is correct (the CI has its correct probabilistic interpretation). The results are visualised in Figure <a href="#fig:SimBI5">2.11</a>: the first 30 CIs are shown. Of these 30 intervals, 29 cover the true parameter value <span class="math inline">\(\beta_1=0.5\)</span>. That gives thus an empirical coverage of <span class="math inline">\(29/30=96.7\%\)</span>. Note that the R output shows the empirical coverage of all 1000 repeated experiments.</p>
<p>Figure <a href="#fig:SimBI5b">2.12</a> shows the results of the first 100 repeated experiments.</p>
<div class="figure" style="text-align: center"><span id="fig:SimBI5b"></span>
<img src="DASM2_files/figure-html/SimBI5b-1.png" alt="95\% confidence intervals from repeated sampling ($n=5$). The points represent the point estimates $\hat\beta_1$. Only the results of the first 100 repeated experiments are shown. The vertical line indicates the true parameter value: $\beta_1=0.5$." width="672" />
<p class="caption">
Figure 2.12: 95% confidence intervals from repeated sampling (<span class="math inline">\(n=5\)</span>). The points represent the point estimates <span class="math inline">\(\hat\beta_1\)</span>. Only the results of the first 100 repeated experiments are shown. The vertical line indicates the true parameter value: <span class="math inline">\(\beta_1=0.5\)</span>.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">267213</span>)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))
<span class="kw">plotBI</span>(SimBI5,<span class="dt">nPlot=</span><span class="dv">30</span>,<span class="dt">mn=</span><span class="op">-</span><span class="fl">0.2</span>,<span class="dt">mx=</span><span class="dv">1</span>,<span class="dt">main=</span><span class="st">&quot;n=5&quot;</span>)

SimBI10&lt;-<span class="kw">simRegressionBI</span>(<span class="dt">N=</span>N1000,<span class="dt">nRep=</span><span class="dv">2</span>)
<span class="kw">plotBI</span>(SimBI10,<span class="dt">nPlot=</span><span class="dv">30</span>,<span class="dt">mn=</span><span class="op">-</span><span class="fl">0.2</span>,<span class="dt">mx=</span><span class="dv">1</span>,<span class="dt">main=</span><span class="st">&quot;n=10&quot;</span>)

SimBI50&lt;-<span class="kw">simRegressionBI</span>(<span class="dt">N=</span>N1000,<span class="dt">nRep=</span><span class="dv">10</span>)
<span class="kw">plotBI</span>(SimBI50,<span class="dt">nPlot=</span><span class="dv">30</span>,<span class="dt">mn=</span><span class="op">-</span><span class="fl">0.2</span>,<span class="dt">mx=</span><span class="dv">1</span>,<span class="dt">main=</span><span class="st">&quot;n=50&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:SimBIAll"></span>
<img src="DASM2_files/figure-html/SimBIAll-1.png" alt="95\% confidence intervals from repeated sampling ($n=5$, $n=10$ and $n=50$). The points represent the point estimates $\hat\beta_1$. Only the results of the first 30 repeated experiments are shown. The vertical line indicates the true parameter value: $\beta_1=0.5$." width="672" />
<p class="caption">
Figure 2.13: 95% confidence intervals from repeated sampling (<span class="math inline">\(n=5\)</span>, <span class="math inline">\(n=10\)</span> and <span class="math inline">\(n=50\)</span>). The points represent the point estimates <span class="math inline">\(\hat\beta_1\)</span>. Only the results of the first 30 repeated experiments are shown. The vertical line indicates the true parameter value: <span class="math inline">\(\beta_1=0.5\)</span>.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p>We repeat the simulation study for larger sample sizes. The results are shown in Figure <a href="#fig:SimBIAll">2.13</a> (only for the first 30 repeated experiments). The empirical coverages based on 1000 repeated experiments are:</p>
<ul>
<li><p><span class="math inline">\(n=5\)</span>: 0.955</p></li>
<li><p><span class="math inline">\(n=10\)</span>: 0.958</p></li>
<li><p><span class="math inline">\(n=50\)</span>: 0.948</p></li>
</ul>
<p>For all sample sizes <span class="math inline">\(n\)</span> the empirical covarages are very close to the nominal confidence level of <span class="math inline">\(95\%\)</span>.</p>
<p>Figure <a href="#fig:SimBIAll">2.13</a> demonstrates that the lengths of the confidence intervals become smaller as the sample size increases. This follows directly from the theory: the length of a <span class="math inline">\(95\%\)</span> is <span class="math inline">\(2t_{n-2;1-0.05/2} \hat\sigma_{\beta_1}\)</span> and this decreases because <span class="math inline">\(\hat\sigma_{\beta_1}\)</span> decreases in expectation with increasing sample size (<span class="math inline">\(\sigma_{\beta_1} \propto \frac{1}{\sqrt{n}}\)</span>). Thus, on average the lenght of the intervals decreases as <span class="math inline">\(n\)</span> increases, and still the coverage remains <span class="math inline">\(95\%\)</span>. This phenomenon is related to the variability of the estimates: with increasing sample size <span class="math inline">\(n\)</span>, the variability of the estimates decreases and thus the CI can be smaller while still giving a coverage of <span class="math inline">\(95\%\)</span>.</p>
</div>
<div id="example-galtons-height-data-2" class="section level2 unnumbered">
<h2>Example (Galton's height data)</h2>
<p>We repeat the regression analysis for Galton's data. This time we look at the standard errors and the <span class="math inline">\(95\%\)</span> confidence interval of the regression coefficient <span class="math inline">\(\beta_1\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(son.cm<span class="op">~</span>father.cm,<span class="dt">data=</span>Galton.sons)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = son.cm ~ father.cm, data = Galton.sons)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.9406  -3.5300   0.2605   3.4064  20.5805 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 89.81819   11.73609   7.653 1.37e-12 ***
## father.cm    0.50766    0.06683   7.596 1.91e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.661 on 171 degrees of freedom
## Multiple R-squared:  0.2523, Adjusted R-squared:  0.2479 
## F-statistic:  57.7 on 1 and 171 DF,  p-value: 1.907e-12</code></pre>
<p>The standard errors of the parameter estimates are <span class="math display">\[
   \hat\sigma_{\beta_0} =  11.7 \;\;\text{ and }\;\; \hat\sigma_{\beta_1} =  0.0668.
 \]</span> From the R output we also read the MSE (square of the ``Residual standard error&quot;): <span class="math display">\[
   \MSE = 5.661^2 = 32.05
 \]</span> and the residual number of degrees of freedom is <span class="math inline">\(n-2=173-2=171\)</span>.</p>
<p>Next we use the standard error of <span class="math inline">\(\hat\beta_1\)</span> for computing the <span class="math inline">\(95\%\)</span> confidence interval of <span class="math inline">\(\beta_1\)</span>. We also need <span class="math inline">\(t_{n-2;1-\alpha/2}=t_{171;0.975}\)</span> nodig.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># quantile of t-distribution</span>
<span class="kw">qt</span>(<span class="fl">0.975</span>,<span class="dt">df=</span><span class="dv">171</span>)</code></pre></div>
<pre><code>## [1] 1.973934</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># lower bound of 95% CI</span>
<span class="fl">0.50766</span><span class="op">-</span><span class="kw">qt</span>(<span class="fl">0.975</span>,<span class="dt">df=</span><span class="dv">171</span>)<span class="op">*</span><span class="fl">0.0668</span></code></pre></div>
<pre><code>## [1] 0.3758012</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># upper bound of 95% CI</span>
<span class="fl">0.50766</span><span class="op">+</span><span class="kw">qt</span>(<span class="fl">0.975</span>,<span class="dt">df=</span><span class="dv">171</span>)<span class="op">*</span><span class="fl">0.0668</span></code></pre></div>
<pre><code>## [1] 0.6395188</code></pre>
<p>The <span class="math inline">\(95\%\)</span> confidence interval of <span class="math inline">\(\beta_1\)</span> thus becomes <span class="math display">\[
   [0.376 , 0.640] .
 \]</span> Hence, with a probability of 95% we expect that the regression coefficient <span class="math inline">\(\beta_1\)</span> is somewhere between <span class="math inline">\(0.376\)</span> and <span class="math inline">\(0.640\)</span>. Thus, if the father's height increases with 1cm, we expect with a probability of 95% that the average son's height increases with <span class="math inline">\(0.376\)</span> cm to <span class="math inline">\(0.640\)</span> cm. Equivalently, we could say that if the father's height increases with 5cm, we expect with a probability of 95% that the average son's height increases with <span class="math inline">\(5\times 0.376=1.88\)</span> cm to <span class="math inline">\(5 \times 0.640=3.2\)</span> cm.</p>
<p>Note that all values within the CI are positive. The data are thus consistent with a positive effect of father's height on the expected son's height.</p>
<p>The next R code can also be used for the computation of the CI.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m)</code></pre></div>
<pre><code>##                  2.5 %      97.5 %
## (Intercept) 66.6519190 112.9844701
## father.cm    0.3757362   0.6395761</code></pre>
<p>Finally we note that the correct probabilistic interpretation of the CI depends on the distributional assumption that is part of the statistical model. In particular: correct specification of the the conditional mean <span class="math inline">\(m(x)\)</span> as a linear function of <span class="math inline">\(x\)</span>; normality of the error term; homoskedasticity; mutual independence of the outcomes. Later we will introduce methods that can be used for assessing these assumptions.</p>
</div>
<div id="exercise-blood-pressure-1" class="section level2 unnumbered">
<h2>Exercise: Blood Pressure</h2>
<p>Consider again the blood pressure dataset and calculate and interpret a <span class="math inline">\(95\%\)</span> confidence interval of the regression slope.</p>
<p><details> <summary markdown="span">Try to make this exercise and expand the page to see the soluation.</summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load the data</span>
<span class="kw">load</span>(<span class="st">&quot;Data/BloodPressure.RData&quot;</span>)

<span class="co"># fit the model</span>
m&lt;-<span class="kw">lm</span>(bp.reduction<span class="op">~</span>dose,<span class="dt">data=</span>BloodPressure)

<span class="co"># 95% confidence intervals</span>
<span class="kw">confint</span>(m, <span class="dt">level=</span><span class="fl">0.95</span>)</code></pre></div>
<pre><code>##                 2.5 %   97.5 %
## (Intercept) -1.948188 2.014267
## dose         1.437469 2.135218</code></pre>
<p>Based on this output we conclude that with a probability of <span class="math inline">\(95\%\)</span> we expect the blood pressure to be reduced with 1.4 to 2.1 mmHg when the dose increases with 1mg per day.</p>
<p></details></p>
</div>
<div id="S:RegTests" class="section level2">
<h2><span class="header-section-number">2.7</span> Hypothesis Tests</h2>
<p>A frequent reason for performing a regression analysis, is to give an answer to the research question whether or not the average outcome is linearly associated with a regressor. In the Galton's data example, Galton wanted to know whether or not the average height of son's linearly depends on their father's height. In terms of regression model <a href="#eq:Mod4">(2.7)</a> this research question translates into the null hypothesis <span class="math display">\[
  H_0: \beta_1 = 0.
\]</span> The alternative hypothesis depends on the exact formulation of the research question, or on prior knowledge. In the Galton's data example there may be two sensible situations one could think of:</p>
<ul>
<li><p>Galton did not have a clue as to what the relation between father's and son's heights could be, because there is also the mother and environmental factors (<em>nature versus nurture</em>). In this setting, the alternative hypothesis becomes <span class="math inline">\(H_1: \beta_1\neq 0\)</span>. (<em>two-sided alternative</em>)</p></li>
<li><p>Galton had prior knowledge (from literature, from discussions with other scientists, or from other independent datasets) that a negative relation (i.e. <span class="math inline">\(\beta_1&lt;0\)</span>) is not plausible. In this setting, the alternative hypothesis would be formulated as <span class="math inline">\(H_1: \beta_1&gt; 0\)</span>. (<em>one-sided alternative to the right</em>)</p></li>
<li><p>For completeness, we also give this third option, which does probably make much sense for the Galton data example. Galton could not have been interested in detecting a negative association, in which case he would again have chosen for the alternative <span class="math inline">\(H_1: \beta_1&gt; 0\)</span>.</p></li>
</ul>
<p>We propose the test statistic <span class="math display">\[
  T =  \frac{\hat\beta_1}{\hat\sigma_{\beta_1}} .
\]</span> From this expression, it is evident that this statistic is sensitive for deviations from <span class="math inline">\(H_0\)</span> in the direction of the two-sided as well as the one-sided alternatives.</p>
<p>If we assume that the normal regression model <a href="#eq:Mod4">(2.7)</a> holds, then from Equation <a href="#eq:tmp998765615">(2.10)</a> it follows that <span class="math display">\[
 T \HSim t_{n-2}.
\]</span> (i.e. under the null hypothesis the test statistic <span class="math inline">\(T\)</span> has a <span class="math inline">\(t_{n-2}\)</span> <strong>null distribution</strong>). Based on this null distribution, <span class="math inline">\(p\)</span>-values and rejection regions can be computed. In particular:</p>
<ul>
<li><p>The one-sided alternative hypothesis <span class="math inline">\(H_1:\beta_1&gt;0\)</span>.<br />
We wish to reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_1\)</span> for large values of <span class="math inline">\(T\)</span>. Hence, <span class="math display">\[
   p = \probf{0}{T\geq t} = 1-\probf{0}{T\leq t} = 1-F_T(t;n-2),
 \]</span> with <span class="math inline">\(F_T(.;n-2)\)</span> the CDF of <span class="math inline">\(t_{n-2}\)</span>.<br />
The rejection region for the test at the <span class="math inline">\(\alpha\)</span> significance level follows from <span class="math display">\[
   \alpha= \prob{\text{type I error}}=\probf{0}{\text{reject }H_0}=\probf{0}{T&gt;t_{n-2;1-\alpha}}.
 \]</span> The rejection region is thus <span class="math inline">\([t_{n-2;1-\alpha},+\infty[\)</span>. We also say that <span class="math inline">\(t_{n-2;1-\alpha}\)</span> is the <em>critical value</em>.</p></li>
<li><p>The one-sided alternative hypothesis <span class="math inline">\(H_1:\beta_1&lt;0\)</span>.<br />
We wish to reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_1\)</span> for small values of <span class="math inline">\(T\)</span> (i.e. large, but negative values of <span class="math inline">\(T\)</span>). Hence, <span class="math display">\[
   p = \probf{0}{T\leq t} = F_T(t;n-2).
 \]</span> The rejection region for the test at the <span class="math inline">\(\alpha\)</span> significance level follows from <span class="math display">\[
   \alpha= \prob{\text{type I error}}=\probf{0}{\text{reject }H_0}=\probf{0}{T&lt;t_{n-2;\alpha}}=\probf{0}{T&lt;-t_{n-2;1-\alpha}},
 \]</span> and is thus given by <span class="math inline">\(]-\infty, -t_{n-2;1-\alpha}]\)</span>. Or, equivalently, the critical value is <span class="math inline">\(t_{n-2;\alpha}=-t_{n-2;1-\alpha}\)</span>.</p></li>
<li><p>The two-sided alternative hypothesis <span class="math inline">\(H_1:\beta_1\neq 0\)</span>.<br />
We wish to reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_1\)</span> for large and small values of <span class="math inline">\(T\)</span> (i.e. large positive and negative values of <span class="math inline">\(T\)</span>). Hence, <span class="math display">\[
   p = \probf{0}{|T|\geq |t|} = \probf{0}{T\leq -|t| \text{ or } T \geq |t|} = 2 \probf{0}{T\geq |t|}=2(1-F_T(|t|;n-2)).
 \]</span> The rejection region for the test at the <span class="math inline">\(\alpha\)</span> significance level follows from <span class="math display">\[
   \alpha= \prob{\text{type I error}}=\probf{0}{\text{reject }H_0}=\probf{0}{|T|&gt;t_{n-2;1-\alpha/2}},
 \]</span> and it is thus given by <span class="math inline">\(]-\infty,-t_{n-2;1-\alpha/2}] \cup [t_{n-2;1-\alpha/2},+\infty[\)</span>, or, equivalently, the critical value for the test based on <span class="math inline">\(|T|\)</span> is given by <span class="math inline">\(t_{n-2;1-\alpha/2}\)</span>.</p></li>
</ul>
</div>
<div id="example-galtons-height-data-3" class="section level2 unnumbered">
<h2>Example (Galton's height data)</h2>
<p>Let's look again at the results of the regression analysis in R.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(son.cm<span class="op">~</span>father.cm,<span class="dt">data=</span>Galton.sons)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = son.cm ~ father.cm, data = Galton.sons)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.9406  -3.5300   0.2605   3.4064  20.5805 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 89.81819   11.73609   7.653 1.37e-12 ***
## father.cm    0.50766    0.06683   7.596 1.91e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.661 on 171 degrees of freedom
## Multiple R-squared:  0.2523, Adjusted R-squared:  0.2479 
## F-statistic:  57.7 on 1 and 171 DF,  p-value: 1.907e-12</code></pre>
<p>We want to test the null hypothesis <span class="math inline">\(H_0: \beta_1=0\)</span> against the alternative hypothesis <span class="math inline">\(H_1: \beta_1\neq 0\)</span>. In the output we read on the line of <em>father.cm</em>: <span class="math inline">\(t=7.596\)</span> and <span class="math inline">\(p=1.91\times 10^{-12}\)</span>. We verify these results in the next chunck.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># observed test statistic</span>
t.obs&lt;-<span class="fl">0.50766</span> <span class="op">/</span><span class="st"> </span><span class="fl">0.06683</span> 
t.obs</code></pre></div>
<pre><code>## [1] 7.596289</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># two-sided p-value</span>
<span class="dv">2</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">pt</span>(<span class="kw">abs</span>(t.obs),<span class="dt">df=</span><span class="dv">171</span>))</code></pre></div>
<pre><code>## [1] 1.905587e-12</code></pre>
<p>Hence, at the 5% level of significance we conclude that there is a positive effect of the father's height on the average height of their sons (<span class="math inline">\(p&lt;0.0001\)</span>). From previous analyses we know that the effect is estimated as <span class="math inline">\(\hat\beta_1=0.51\)</span> with a 95% confidence interval of <span class="math inline">\(0.376\)</span> to <span class="math inline">\(0.640\)</span>.</p>
<p>Suppose that the researchers know that the effect of the father's height on the average son's height can never be negative. Then the alternative hypothesis becomes <span class="math inline">\(H_1: \beta_1&gt;0\)</span>. The R output from the <em>lm</em> function, however, always gives the <span class="math inline">\(p\)</span>-value for a two-sided alternative hypothesis. Upon using the symmetry of the <span class="math inline">\(t_{n-2}\)</span> null distribution we can easily convert the two-sided <span class="math inline">\(p\)</span>-value to a one-sided <span class="math inline">\(p\)</span>-value. In particular, the R output gives the two-sided <span class="math inline">\(p\)</span>-value, which is defined as <span class="math display">\[
   2\probf{0}{T\geq |t|} .
 \]</span> For the one-sided test we need <span class="math display">\[
   \probf{0}{T\geq t}.
\]</span> If <span class="math inline">\(t&gt;0\)</span>, then <span class="math inline">\(t=+|t|\)</span> and thus <span class="math display">\[
   p=\probf{0}{T\geq t} = \probf{0}{T\geq |t|} ,
\]</span> Consequently, the one-sided <span class="math inline">\(p\)</span>-value equals half of the two-sided <span class="math inline">\(p\)</span>-value if <span class="math inline">\(t\)</span> is positive. Thus,<br />
<span class="math display">\[
   p = \frac{1}{2}\times 1.9\times 10^{-12} = 9.5\times 10^{-13}.
\]</span> For this data example we come to the same conclusion as before.</p>
</div>
<div id="exercise-converting-two-sided-p-values" class="section level2 unnumbered">
<h2>Exercise: Converting two-sided p-values</h2>
<p>As an exercise, derive the formula to find the <span class="math inline">\(p\)</span>-value for the one-sided testing problem with <span class="math inline">\(H_1: \beta_1&lt;0\)</span>, starting from the <span class="math inline">\(p\)</span>-value for the two-sided alternative, <span class="math inline">\(2\probf{0}{T\geq |t|}\)</span>. Derive the conversion for both a positive and a negative observed test statistic <span class="math inline">\(t\)</span>. Calculate this one-sided <span class="math inline">\(p\)</span>-value for the Galton's height data.</p>
<p><details> <summary markdown="span">Try to make this exercise and expand the page to see the soluation.</summary> For the one-sided test we need <span class="math display">\[
   \probf{0}{T\leq t}.
\]</span></p>
<p>If <span class="math inline">\(t&lt;0\)</span>, then <span class="math inline">\(t=-|t|\)</span> and thus <span class="math display">\[
   p=\probf{0}{T\leq t} = \probf{0}{T\leq -|t|} = \probf{0}{-T\geq |t|} = \probf{0}{T\geq |t|} ,
\]</span> in which the last step made use of the symmetry of the null distrubution of <span class="math inline">\(T\)</span> (i.e. the distribution of <span class="math inline">\(-T\)</span> equals the distribution of <span class="math inline">\(T\)</span> under <span class="math inline">\(H_0\)</span>). Consequently, the one-sided <span class="math inline">\(p\)</span>-value equals half of the two-sided <span class="math inline">\(p\)</span>-value if <span class="math inline">\(t\)</span> is negative. With <span class="math inline">\(p_2=2\probf{0}{T\geq |t|}\)</span>, this one-sided <span class="math inline">\(p\)</span>-value thus equals <span class="math inline">\(p_2/2\)</span>.</p>
<p>If <span class="math inline">\(t&gt;0\)</span>, then <span class="math inline">\(t=|t|\)</span> and thus <span class="math display">\[
   p=\probf{0}{T\leq t} = \probf{0}{T\leq |t|} = 1-\probf{0}{T\geq |t|} .
\]</span> With <span class="math inline">\(p_2=2\probf{0}{T\geq |t|}\)</span>, the one-sided <span class="math inline">\(p\)</span>-value can thus be calculated as <span class="math display">\[
  p = 1- \frac{p_2}{2}.
\]</span></p>
<p>In the Galton's height example, the observed <span class="math inline">\(t\)</span>-value equals <span class="math inline">\(7.596&gt;0\)</span>, and hence the <span class="math inline">\(p\)</span>-value that corresponds to <span class="math inline">\(H_1:\beta_1&lt;0\)</span> becomes <span class="math display">\[
   p = 1-\frac{1}{2}\times 1.9\times 10^{-12} \approx 1.
\]</span><br />
</details></p>
</div>
<div id="exercise-muscle-mass" class="section level2 unnumbered">
<h2>Exercise: Muscle mass</h2>
<p>Scientists suspect that the muscle mass of people starts declining from a certain age onwards. To verify this research question, a nutritionist randomly sampled 59 women, aged between 41 and 78. For these women; also the muscle mass was measured (we actually only have a proxy based on bioelectrical impedance measurements).</p>
<p>Perform a regression analysis and formulate an answer to this research question (including parameter estimates, confidence interval and hypothesis test). You may use the next chunk of R code for reading the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">muscles&lt;-<span class="kw">read.csv</span>(<span class="st">&quot;Data/muscles.txt&quot;</span>, <span class="dt">sep=</span><span class="st">&quot; &quot;</span>)
<span class="kw">names</span>(muscles)&lt;-<span class="kw">c</span>(<span class="st">&quot;muscle.mass&quot;</span>,<span class="st">&quot;age&quot;</span>)
<span class="kw">skim</span>(muscles)</code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-40">Table 2.2: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">muscles</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">59</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">2</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">muscle.mass</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">84.61</td>
<td align="right">16.11</td>
<td align="right">52</td>
<td align="right">73.0</td>
<td align="right">84</td>
<td align="right">96.5</td>
<td align="right">119</td>
<td align="left">▃▇▇▆▃</td>
</tr>
<tr class="even">
<td align="left">age</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">60.27</td>
<td align="right">11.68</td>
<td align="right">41</td>
<td align="right">51.5</td>
<td align="right">60</td>
<td align="right">70.0</td>
<td align="right">78</td>
<td align="left">▇▃▇▆▇</td>
</tr>
</tbody>
</table>
<p><details> <summary markdown="span">Try to make this exercise and expand the page to see the soluation.</summary></p>
<p>First we fit the linear regression model and make a graph of the data and the fitted regression line.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(muscle.mass<span class="op">~</span>age, <span class="dt">data=</span>muscles)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = muscle.mass ~ age, data = muscles)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -16.121  -6.373  -0.674   6.968  23.455 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 156.22438    5.68612   27.48   &lt;2e-16 ***
## age          -1.18820    0.09265  -12.82   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.244 on 57 degrees of freedom
## Multiple R-squared:  0.7426, Adjusted R-squared:  0.7381 
## F-statistic: 164.5 on 1 and 57 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m)</code></pre></div>
<pre><code>##                  2.5 %     97.5 %
## (Intercept) 144.838119 167.610636
## age          -1.373721  -1.002678</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(muscles,
       <span class="kw">aes</span>(<span class="dt">x=</span>age, <span class="dt">y=</span>muscle.mass)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">color=</span><span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;age (years)&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;muscle mass&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>), <span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept=</span>m<span class="op">$</span>coefficients[<span class="dv">1</span>],<span class="dt">slope=</span>m<span class="op">$</span>coefficients[<span class="dv">2</span>]) </code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<p>From the output we can formulate the following conclusions.</p>
<p>The <span class="math inline">\(p\)</span>-value that is reported in the R output refers to the two-sided alternative. Here we translate the research question into the one-sided alternative <span class="math inline">\(H_1: \beta_1&lt;0\)</span>. Since the observed test statistic <span class="math inline">\(t\)</span>=-12.83 is negative the <span class="math inline">\(p\)</span>-value becomes 1.924970810^{-18}/2<span class="math inline">\(&lt;0.0001\)</span>. Hence, on average the muscle mass of women aged between 41 and 78 years is significantly negatively associated with the age (<span class="math inline">\(p&lt;0.001\)</span>) at the <span class="math inline">\(5\%\)</span> level of significance. We estimate that the mean muscle mass decreases with 1.19 (SE=0.09) units with an increase of age of 1 year. The <span class="math inline">\(95\%\)</span> confidence interval of this estimate is -1.37 to -1 units of muscle mass per increase of age with one year. </details></p>
</div>
<div id="exercise-choosing-h_1-after-looking-at-the-data" class="section level2 unnumbered">
<h2>Exercise: choosing <span class="math inline">\(H_1\)</span> after looking at the data</h2>
<p>Set up a simulation study to demonstrate that the following hypothesis testing procedure does not control the type I error rate at the nominal <span class="math inline">\(\alpha\)</span> level.</p>
<p>This is the procedure to consider in the context of the simple linear regression model:</p>
<ul>
<li><p>generate the data and estimate the regression parameters</p></li>
<li><p>if <span class="math inline">\(\hat\beta_1&lt;0\)</span>, set the alternative hypothesis to <span class="math inline">\(H_1: \beta_1&lt;0\)</span>, otherwise set the alternative hypothesis to <span class="math inline">\(H_1:\beta_1&gt;0\)</span></p></li>
<li><p>compute the <span class="math inline">\(p\)</span>-value for the test for testing <span class="math inline">\(H_0:\beta_1=0\)</span> versus the alternative selected in the previous step.</p></li>
</ul>
<p><details> <summary markdown="span">Try to make this exercise and expand the page to see the soluation.</summary></p>
<p>In the next chunck of R code data are simulated under the null hypothesis <span class="math inline">\(\beta_1=0\)</span>, because the type I error rate is the probability to reject the null hypothesis, given that the null hypothesis is true. For each repeated sample we compute the p-value for the one-sided alternative that is determined by the sign of the parameter estimate <span class="math inline">\(\hat\beta_1\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">87175</span>)
N&lt;-N10000 <span class="co"># number of repeated experiments</span>
x&lt;-<span class="kw">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span>
X&lt;-<span class="kw">cbind</span>(<span class="dv">1</span>,x) <span class="co"># design matrix</span>
pvalues&lt;-<span class="kw">c</span>()
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
  y&lt;-<span class="dv">90</span><span class="op">+</span><span class="dv">0</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">5</span>,<span class="dt">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes -- we simulate under H_0:beta1=0</span>
  m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
  beta1Hat&lt;-<span class="kw">coef</span>(m)[<span class="dv">2</span>]
  <span class="cf">if</span>(beta1Hat<span class="op">&lt;</span><span class="dv">0</span>) {
    p&lt;-<span class="kw">summary</span>(m)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">4</span>]<span class="op">/</span><span class="dv">2</span> <span class="co"># if t&lt;0, the p-value for H_1: beta1&lt;0 needs to be devided by 2</span>
  }
  <span class="cf">if</span>(beta1Hat<span class="op">&gt;</span><span class="dv">0</span>) {
    p&lt;-<span class="kw">summary</span>(m)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">4</span>]<span class="op">/</span><span class="dv">2</span> <span class="co"># if t&gt;0, the p-value for H_1: beta1&gt;0 needs to be devided by 2</span>
  }
  pvalues&lt;-<span class="kw">c</span>(pvalues,p)
}

<span class="co"># empirical type I error rate for alpha=0.05</span>
<span class="kw">mean</span>(pvalues<span class="op">&lt;</span><span class="fl">0.05</span>)</code></pre></div>
<pre><code>## [1] 0.0982</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># histogram </span>
<span class="kw">hist</span>(pvalues, <span class="dt">cex.lab=</span><span class="fl">1.5</span>, <span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<p>This simulation study shows that the type I error rate is approximated by 0.0982, which is larger than the nominal value of <span class="math inline">\(\alpha=0.05\)</span>. This testing procedure thus does not control the type I error rate at its nominal level. The histogram no longer shows a uniform distribution between 0 and 1 (which we would expect if the test was correctly constructed).</p>
<p></details></p>
</div>
<div id="S:AssessAssumptions" class="section level2">
<h2><span class="header-section-number">2.8</span> Assessment of the Model Assumptions</h2>
<p>The normal simple linear regression model <a href="#eq:Mod4">(2.7)</a> is the most restrictive than the semiparametric linear regression model <a href="#eq:Mod3">(2.2)</a> in the sense that it requires the largest numer of assumptions. In this section we discuss the importance of the model assumptions and how they can be verified based on the observed data. We will use Galton's height data to illustrate the methods.</p>
</div>
<div id="example-galtons-height-data-4" class="section level2 unnumbered">
<h2>Example (Galton's height data)</h2>
<p>We will verify one-by-one the model assumption of model <a href="#eq:Mod4">(2.7)</a>.</p>
<div id="linearity-of-the-regression-model" class="section level4 unnumbered">
<h4>Linearity of the regression model</h4>
<p>The conditional mean of the outcome must satisfy <span class="math display">\[
   \E{Y\mid x} = m(x;\mb\beta) = \beta_0+\beta_1 x.
 \]</span> If the parameters are known, then this is equivalent to the condition <span class="math display">\[
   0 = \E{\eps \mid x} = \E{Y-m(x;\mb\beta)\mid x}=\E{Y-\beta_0-\beta_1 x\mid x} .
 \]</span></p>
<p>If there are replicated outcomes available for a given <span class="math inline">\(x\)</span>, then <span class="math inline">\(\E{Y-\beta_0-\beta_1 x\mid x}\)</span> can be (unbiasedly) estimated as the sample mean of the residuals <span class="math inline">\(e_i=y_i-\hat\beta_0-\hat\beta_1x_i\)</span> for which <span class="math inline">\(x_i=x\)</span>. These avarage residuals can be computed for all <span class="math inline">\(x\in \{x_1,\ldots, x_n\}\)</span>. Note that for Galton's data, for some of the regressor values (father's heights) there is only one observed outcome. For these regressor values, the sample mean of the residuals equals the residual. Figure <a href="#fig:GaltonRes1">2.14</a> shows the result for the Galton data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(son.cm<span class="op">~</span>father.cm,<span class="dt">data=</span>Galton.sons)
e&lt;-m<span class="op">$</span>residuals
x.all&lt;-<span class="kw">unique</span>(Galton.sons<span class="op">$</span>father.cm)
ave.e&lt;-<span class="kw">c</span>()
<span class="cf">for</span>(x <span class="cf">in</span> x.all) {
    ave.e&lt;-<span class="kw">c</span>(ave.e,<span class="kw">mean</span>(e[Galton.sons<span class="op">$</span>father.cm<span class="op">==</span>x]))
}
<span class="kw">plot</span>(Galton.sons<span class="op">$</span>father.cm,e,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Father&#39;s height (cm)&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;residuals&quot;</span>)
<span class="kw">points</span>(x.all,ave.e,<span class="dt">col=</span><span class="dv">2</span>,<span class="dt">pch=</span><span class="dv">8</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:GaltonRes1"></span>
<img src="DASM2_files/figure-html/GaltonRes1-1.png" alt="Scatterplot of the residuals against the father's heights (Galton's height example). The red stars represent the sample means of the residuals for a given father's height." width="672" />
<p class="caption">
Figure 2.14: Scatterplot of the residuals against the father's heights (Galton's height example). The red stars represent the sample means of the residuals for a given father's height.
</p>
</div>
<p>Since the sample means of the residuals are only estimates of <span class="math inline">\(\E{Y-\beta_0-\beta_1 x\mid x}\)</span>, we cannot expect that these sample means are exactly equal to zero (i.e. the sample means also show sampling variability). The larger the number of replicates on which such a sample mean is computed, the smaller the sampling variability and the closer we expect the sample mean to be close to zero.</p>
<p>We hope that the average residuals do not show a systematic pattern as a function of the regressor. No systematic pattern would agree with the model assumption <span class="math inline">\(\E{\eps \mid x}=0\)</span>. Figure <a href="#fig:GaltonRes1">2.14</a> shows no such systematic pattern, and therefore we conclude that the linear relation between the regressor (father's height) and the mean response (son's height) is linear.</p>
<p>If there are no replicates at the regressor values, then the plot can be constructed, but with no sample means of the residuals. Such graphs (with or without the average residuals) are known as <strong>residual plots</strong>.</p>
</div>
<div id="normality-of-the-error-term" class="section level3 unnumbered">
<h3>Normality of the error term</h3>
<p>The model implies that <span class="math display">\[ 
  \eps_i=Y_i-m(x_i;\mb\beta) \mid x_i \sim N(0,\sigma^2). 
\]</span></p>
<p>To some extent the residuals <span class="math inline">\(e_i=Y_i-m(x_i;\hat{\mb\beta})\)</span> can be considered as ``estimates'' of <span class="math inline">\(\eps_i\)</span>. Therefore we will use the residuals for assessing the normality assumption. We could use histograms, boxplots and normal QQ-plots for this purpose. Particularly the normal QQ-plots are informative, because they are specifically developped for this assessing normality.</p>
<p>Figure <a href="#fig:GaltonQQResid">2.15</a> shows the normal QQ-plot of the residuals of the Galton example. Most of the points in the QQ-plot are close to the straight line with no systematic pattern, and with only a few larger deviations in the right hand tail of the distribution. However, the number of outliers (2 or 3) is very small as compared to the sample size (173). The plot does also not reveal systematic deviations from the straight line.</p>
<p>Finally, note that the normality assumption is not very important for this example, because the rather large sample size of <span class="math inline">\(n=173\)</span> tells us that the parameter estimators will be approximately normally distributed thanks to the central limit theorem. So only a very strong deviation from normality would have been worrisome. Also recall that the <span class="math inline">\(p\)</span>-value for the two-sided test for <span class="math inline">\(H_0:\beta_1=0\)</span> was very small (<span class="math inline">\(p&lt;0.001\)</span>); thus even a small deviation from normality would not have caused doubt over the conclusion of the statistical test.</p>
<p>The R code for the QQ-plot in Figure <a href="#fig:GaltonQQResid">2.15</a> is shown below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qqnorm</span>(m<span class="op">$</span>resid,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
       <span class="dt">xlab=</span><span class="st">&quot;expected quantiles&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;residuals&quot;</span>,<span class="dt">main=</span><span class="st">&quot;&quot;</span>)
<span class="kw">qqline</span>(m<span class="op">$</span>resid)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:GaltonQQResid"></span>
<img src="DASM2_files/figure-html/GaltonQQResid-1.png" alt="Normal QQ-plot of the residuals of the Galton example." width="672" />
<p class="caption">
Figure 2.15: Normal QQ-plot of the residuals of the Galton example.
</p>
</div>
</div>
<div id="homoskedasticity" class="section level3 unnumbered">
<h3>Homoskedasticity</h3>
<p>Model <a href="#eq:Mod4">(2.7)</a> implies that <span class="math display">\[
  \var{\eps_i}=\var{\eps_i\mid x_i} =\var{Y_i \mid x_i}=\sigma^2,
\]</span> i.e. the variance of the outcomes (and of the error terms) is constant and does not depend on the values of the regressor.</p>
<p>If there are replicated observations for each value of the regressor in the dataset, then the sample variance of the outcome can be calculated for unique observed value of the regressor. The sample variances can then be plotted against the regressor <span class="math inline">\(x\)</span>. If this graph shows no clear pattern that deviates from the assumption of a constant variance, then the graph suggests that the constant-variance assumption is satisfied.</p>
<p>If there are no replicated observations for the unique regressor values, then one may plot <span class="math inline">\(e_i^2\)</span> versus <span class="math inline">\(x_i\)</span>. If we reason that the residuals <span class="math inline">\(e_i\)</span> are (approximately) estimates for the error terms <span class="math inline">\(\eps_i\)</span>, and because <span class="math inline">\(\var{\eps_i} = \E{(\eps_i-\E{\eps_i})^2} = \E{\eps_i^2}\)</span>, we expect that <span class="math inline">\(\E{e_i^2} \approx \E{\eps_i}\)</span> and hence the plot of <span class="math inline">\(e_i^2\)</span> versus <span class="math inline">\(x_i\)</span> should not indicate a systematic pattern under the assumption of constant-variance.</p>
<p>If there are regressor values with and without replicates, then one could either choose to simply plot <span class="math inline">\(e_i^2\)</span> for all <span class="math inline">\(i=1,\ldots, n\)</span>, or for the replicated observations one may plot the average of the <span class="math inline">\(e_i^2\)</span> (note that this is different from the sample variances because of the factor <span class="math inline">\(1/(n-1)\)</span> in the calculation of the sample variance, and the factor <span class="math inline">\(1/n\)</span> in the calculation of the average). The reason for using <span class="math inline">\(1/n\)</span> in this case, is to make these averages more comparable to the individually plotted <span class="math inline">\(e_i^2\)</span>.</p>
<p>Figure <a href="#fig:GaltonResidVar">2.16</a> shows the plot for the Galton example. No systematic pattern can be observed, except for some outliers, particularly in the plot in the left panel (variance). The plot in the right panel is less extreme because he square-root transformation is applied to the (average of the) <span class="math inline">\(e_i^2\)</span> (this is this plotted at the scale of the standard deviation and the scale of the outcomes). Note that particularly in the graph in the left panel we expect strong skewness to the right, because the sampling distribution of the sample variance is related to a <span class="math inline">\(\chi^2\)</span> distribution.</p>
<p>We conclude that there is no indication for a violation of the constant-variance assumption.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
heights&lt;-<span class="kw">unique</span>(Galton.sons<span class="op">$</span>father.cm)
var.y&lt;-<span class="kw">c</span>()
<span class="cf">for</span>(x <span class="cf">in</span> heights) {
    <span class="co">#var.y&lt;-c(var.y,var(Galton.sons$son.cm[Galton.sons$father.cm==x]))</span>
  var.y&lt;-<span class="kw">c</span>(var.y,<span class="kw">sum</span>(m<span class="op">$</span>residuals[Galton.sons<span class="op">$</span>father.cm<span class="op">==</span>x]<span class="op">^</span><span class="dv">2</span>))
}
<span class="kw">plot</span>(heights,var.y,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Father&#39;s height (cm)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;sample variance&quot;</span>,<span class="dt">col=</span><span class="dv">2</span>,<span class="dt">pch=</span><span class="dv">8</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">sum</span>(m<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="dv">173</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="co"># note that the reference line is at MSE (n-2)/n</span>

<span class="kw">plot</span>(heights,<span class="kw">sqrt</span>(var.y),<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Father&#39;s height (cm)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;sample standard deviation&quot;</span>,
     <span class="dt">col=</span><span class="dv">2</span>,<span class="dt">pch=</span><span class="dv">8</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">sqrt</span>(<span class="kw">sum</span>(m<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="dv">173</span>),<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:GaltonResidVar"></span>
<img src="DASM2_files/figure-html/GaltonResidVar-1.png" alt="Sample variances (left) en standard deviations (right) against the  regressor (father's height). The horizontal reference line corresponds to the MSE (left) and en $\sqrt{\MSE}$ (right)." width="672" />
<p class="caption">
Figure 2.16: Sample variances (left) en standard deviations (right) against the regressor (father's height). The horizontal reference line corresponds to the MSE (left) and en <span class="math inline">\(\sqrt{\MSE}\)</span> (right).
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># note that the reference line is at MSE (n-2)/n</span>
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p>The next two graphs in Figure <a href="#fig:GaltonResid2">2.17</a> show the squared residuals <span class="math inline">\(e_i^2\)</span> versus <span class="math inline">\(x_i\)</span> (thus no averaging, even at regressor values with replicated observations) and the absolute values <span class="math inline">\(\vert e_i \vert\)</span> against <span class="math inline">\(x_i\)</span>. The latter graph is also supposed to give no systematic pattern if the assumption of constant-variance holds, and it is less sensitive to outliers as compared to the plot with the squared residuals. We also expect no skewness in the plot of the absolute values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
e&lt;-m<span class="op">$</span>residuals
<span class="kw">plot</span>(Galton.sons<span class="op">$</span>father.cm,e<span class="op">^</span><span class="dv">2</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Father&#39;s height (cm)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;Squared residuals&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">sum</span>(m<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>(<span class="dv">173</span><span class="op">-</span><span class="dv">2</span>),<span class="dt">lty=</span><span class="dv">2</span>)

e&lt;-m<span class="op">$</span>residuals
<span class="kw">plot</span>(Galton.sons<span class="op">$</span>father.cm,<span class="kw">abs</span>(e),<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Father&#39;s height (cm)&quot;</span>
     ,<span class="dt">ylab=</span><span class="st">&quot;absolute value of the residuals&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">sum</span>(<span class="kw">abs</span>(m<span class="op">$</span>residuals))<span class="op">/</span><span class="dv">173</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:GaltonResid2"></span>
<img src="DASM2_files/figure-html/GaltonResid2-1.png" alt="Scatter plots of $e_i^2$ against $x_i$ and of $| e_i |$ against $x_i$ for the Galton example. The horizontal reference lines correspond to MSE (left) and $\frac{1}{n}\sum_{i=1}^n | e_i |$ (right)." width="672" />
<p class="caption">
Figure 2.17: Scatter plots of <span class="math inline">\(e_i^2\)</span> against <span class="math inline">\(x_i\)</span> and of <span class="math inline">\(| e_i |\)</span> against <span class="math inline">\(x_i\)</span> for the Galton example. The horizontal reference lines correspond to MSE (left) and <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n | e_i |\)</span> (right).
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
</div>
</div>
<div id="exercise-muscle-mass-1" class="section level2 unnumbered">
<h2>Exercise: Muscle mass</h2>
<p>Assess the assumption for the regression analysis of the muscle mass example.</p>
<p><details> <summary markdown="span">Try to make this exercise and expand the page to see the soluation.</summary></p>
<p>First we fit the regression model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(muscle.mass<span class="op">~</span>age, <span class="dt">data=</span>muscles)</code></pre></div>
<p><strong>Linearity of the regression model</strong></p>
<p>We can assess the linearity by simple plotting the residuals versus the regressor (age). If there are multiple observations for an age, we can also compute and plot the average residuals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">e&lt;-m<span class="op">$</span>residuals
x.all&lt;-<span class="kw">unique</span>(muscles<span class="op">$</span>age)
ave.e&lt;-<span class="kw">c</span>()
<span class="cf">for</span>(x <span class="cf">in</span> x.all) {
    ave.e&lt;-<span class="kw">c</span>(ave.e,<span class="kw">mean</span>(e[muscles<span class="op">$</span>age<span class="op">==</span>x]))
}
<span class="kw">plot</span>(muscles<span class="op">$</span>age,e,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Age (years)&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;residuals&quot;</span>)
<span class="kw">points</span>(x.all,ave.e,<span class="dt">col=</span><span class="dv">2</span>,<span class="dt">pch=</span><span class="dv">8</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>This graph does not suggest any deviation from linearity because we do not observe a systematic pattern of the (average) residuals versus the age.</p>
<p>One may also add a <em>nonparametric smoother</em> to the residual plot. Although you may perhaps still do not know what is a nonparametric smoother (outside of the scope of this course), you may simply interpret it as a nonparametric estimate of <span class="math inline">\(\E{E \mid x}\)</span>, with <span class="math inline">\(E\)</span> the residual. This is illustrated next.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(muscles<span class="op">$</span>age,e,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Age (years)&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;residuals&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="dv">2</span>)
<span class="kw">lines</span>(<span class="kw">lowess</span>(muscles<span class="op">$</span>age,e))</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<p>The smooth line again does not suggest a deviation from linearity.</p>
<p><strong>Normality of the error terms</strong></p>
<p>We make a normal QQ-plot of the residuals, as well as a boxplot. The normal QQ-plot shows small systematic deviations in the two tails of the distribution, but the shape of the distribution is still quite symmetric (see also the boxplot). Given the moderately large sample size of <span class="math inline">\(n=59\)</span>, we do consider this a problematic deviation from the normality assumption and we can still trust the results for confidence intervals and the hypothesis test (moreover, the <span class="math inline">\(p\)</span>-value is extremely small).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">qqnorm</span>(m<span class="op">$</span>resid,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
       <span class="dt">xlab=</span><span class="st">&quot;expected quantiles&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;residuals&quot;</span>,<span class="dt">main=</span><span class="st">&quot;&quot;</span>)
<span class="kw">qqline</span>(m<span class="op">$</span>resid)

<span class="kw">boxplot</span>(m<span class="op">$</span>residuals, <span class="dt">ylab=</span><span class="st">&quot;residuals&quot;</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p><strong>Homoskedasticity</strong></p>
<p>In the next chunck of R code two plots are produced (squared residuals against regressor and absolute value against regressor). These graphs indicate no violation against the constant-variance assumption.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
e&lt;-m<span class="op">$</span>residuals
<span class="kw">plot</span>(muscles<span class="op">$</span>age,e<span class="op">^</span><span class="dv">2</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Age (years)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;Squared residuals&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">sum</span>(m<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>(<span class="dv">173</span><span class="op">-</span><span class="dv">2</span>),<span class="dt">lty=</span><span class="dv">2</span>)

e&lt;-m<span class="op">$</span>residuals
<span class="kw">plot</span>(muscles<span class="op">$</span>age,<span class="kw">abs</span>(e),<span class="dt">cex.lab=</span><span class="fl">1.5</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Age (years)&quot;</span>
     ,<span class="dt">ylab=</span><span class="st">&quot;absolute value of the residuals&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">sum</span>(<span class="kw">abs</span>(m<span class="op">$</span>residuals))<span class="op">/</span>(<span class="dv">173</span><span class="op">-</span><span class="dv">2</span>),<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p></details></p>
</div>
<div id="binary-dummy-regressors" class="section level2">
<h2><span class="header-section-number">2.9</span> Binary Dummy Regressors</h2>
<p>In this section we will demonstrate how the <strong>two-sample problem</strong> is a special case of simple linear regression. In the two sample problem, we are interested in comparing two means. For example, consider the dose finding study, but we only look at the patients that received 0mg (control or placebo group) and 2mg per day. We want to know the mean difference in blood pressure reduction between these two doses. Thus we basically have two samples of patients: a sample of 10 patients receiving placebo and a sample of 10 patients receiving 2mg/day. We are not interested in assessing a linear effect of the dose. We now consider the 0mg/day and 2mg/day as two treatments (placebo and active treatment).</p>
<p>More formally, we consider two groups, populations or treatments (whatever term you prefer), say treatment U and treatment V. The interest is in estimating <span class="math display">\[
  \mu_U = \E{Y \mid U} \;\; \text{ and } \;\; \mu_V=\E{Y \mid V}
\]</span> and the treatment <em>effect size</em> <span class="math display">\[
 \delta = \mu_V-\mu_U.
\]</span></p>
<p>Of course we know the solution: <span class="math display">\[
  \hat\mu_U = \bar{Y}_U \;\;\text{ and } \;\; \hat\mu_V=\bar{Y}_V \;\;\text{ and }\;\; \hat\delta = \hat\mu_V-\hat\mu_V,
\]</span> in which we used the obvious notation of <span class="math inline">\(\bar{Y}_U\)</span> and <span class="math inline">\(\bar{Y}_V\)</span> representing the sample means of the outcomes in the <span class="math inline">\(U\)</span> and the <span class="math inline">\(V\)</span> sample, respectively. Confidence intervals can be easily obtained as well as hypothesis tests for testing <span class="math inline">\(H_0: \delta=0\)</span> against one-sided or two-sided alternatives (this is the well known two-sample <span class="math inline">\(t\)</span>-test).</p>
<p>For the confidence intervals and two-sample hypothesis test, we often assume normality in the two treatment groups (unless the samples sizes are large), i.e. <span class="math display">\[
  Y_i \mid U \sim N(\mu_U, \sigma^2) \;\;\text{ and }\;\; Y_i \mid V \sim N(\mu_V, \sigma^2).
\]</span></p>
<p>This two-sample problem can also be formulated as a regression model, which will allow us to apply all theory that we have seen before.</p>
<p>Define a <strong>dummy regressor</strong> <span class="math inline">\(x_i\)</span> as</p>
<span class="math display">\[\begin{eqnarray*}
  x_i 
    &amp;=&amp; 1 \text{ if observation i belongs to treatment V} \\
    &amp;=&amp; 0 \text{ if observation i belongs to treatment U} .
\end{eqnarray*}\]</span>
<p>With this definition, we build the conventional regression model, <span class="math display">\[
  Y_i = \beta_0 + \beta_1 x_i + \eps_i
\]</span> with <span class="math inline">\(\eps_i\)</span> i.i.d. <span class="math inline">\(N(0,\sigma^2)\)</span>. Or, equivalently, <span class="math display">\[
 Y_i \mid x_i \sim N(\beta_0+\beta_1 x_i , \sigma^2).
\]</span> Since <span class="math inline">\(x_i\)</span> can take only two values we can explicitely look at the <span class="math inline">\(x_i=1\)</span> and <span class="math inline">\(x_i=0\)</span> possibilities: <span class="math display">\[
  Y_i \mid x_i=0 \sim N(\beta_0 , \sigma^2)
\]</span> and <span class="math display">\[
  Y_i \mid x_i=1 \sim N(\beta_0 + \beta_1 , \sigma^2).
\]</span> Comparing these expressions with our earlier formulation of the two-sample problem, we find for outcomes in treatment group U: <span class="math display">\[
  \mu_U = \E{Y\mid U} = \E{Y \mid x=0} = \beta_0 
\]</span> and for for outcomes in group V: <span class="math display">\[
  \mu_V = \E{Y\mid V} = \E{Y \mid x=1} = \beta_0 +\beta_1.
\]</span> This immediately gives <span class="math display">\[
  \delta=\mu_V-\mu_U = \beta_1.
\]</span> Thus the regression parameter <span class="math inline">\(\beta_1\)</span> in the linear regression model with the 0/1 binary dummy variable can be directly interpreted as the effect size <span class="math inline">\(\delta=\mu_V-\mu_U\)</span>. The methods for confidence intervals and hypothesis testing for the parameter <span class="math inline">\(\beta_1\)</span> can now be directly applied.</p>
<p>Also note that the regression parameter <span class="math inline">\(\beta_0=\mu_U\)</span>.</p>
</div>
<div id="example" class="section level2 unnumbered">
<h2>Example</h2>
Consider now the blood pressure example with only the 0mg/day and 2mg/day observations, and define
<span class="math display">\[\begin{eqnarray*}
  x_i 
    &amp;=&amp; 1 \text{ if observation i belongs to the 2mg/day group} \\
    &amp;=&amp; 0 \text{ if observation i belongs to the 0mg/day group (placebo)} .
\end{eqnarray*}\]</span>
<p>The next chunck of R code shows the analysis with R. We start with subsetting the blood pressure dataset and with defining the binary dummy regressor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># extract the 0mg/day and 2mg/day observations</span>
BloodPressure2&lt;-BloodPressure <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(dose<span class="op">==</span><span class="dv">0</span><span class="op">|</span>dose<span class="op">==</span><span class="dv">2</span>)

<span class="co"># define binary dummy</span>
BloodPressure2<span class="op">$</span>x&lt;-<span class="kw">ifelse</span>(BloodPressure2<span class="op">$</span>dose<span class="op">==</span><span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">0</span>)
<span class="kw">table</span>(BloodPressure2<span class="op">$</span>dose,BloodPressure2<span class="op">$</span>x)</code></pre></div>
<pre><code>##    
##      0  1
##   0 10  0
##   2  0 10</code></pre>
<p>The table demonstrates that we correctly defined the regressor. Now we fit the linear regression model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(bp.reduction<span class="op">~</span>x,<span class="dt">data=</span>BloodPressure2)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = bp.reduction ~ x, data = BloodPressure2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -7.10  -1.75   0.20   1.50   5.90 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)   -0.900      1.168  -0.771  0.45083   
## x              5.400      1.651   3.270  0.00425 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.692 on 18 degrees of freedom
## Multiple R-squared:  0.3727, Adjusted R-squared:  0.3378 
## F-statistic: 10.69 on 1 and 18 DF,  p-value: 0.004252</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m)</code></pre></div>
<pre><code>##                 2.5 %   97.5 %
## (Intercept) -3.353076 1.553076
## x            1.930827 8.869173</code></pre>
<p>And now the same analysis but with the t.test function in R.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(bp.reduction<span class="op">~</span>dose,<span class="dt">data=</span>BloodPressure2,
       <span class="dt">var.equal=</span><span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  bp.reduction by dose
## t = -3.2702, df = 18, p-value = 0.004252
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -8.869173 -1.930827
## sample estimates:
## mean in group 0 mean in group 2 
##            -0.9             4.5</code></pre>
<p>The agreement between the results can be seen directly.</p>
</div>
<div id="exercise-smoking" class="section level2 unnumbered">
<h2>Exercise: Smoking</h2>
<p>We are presented with a sample of 654 youths, aged 3 to 19 years, in the area of East Boston during middle to late 1970's. Interest concerns the relationship between smoking and FEV (forced expiratory volume; it measures how much air a person can exhale during a forced breath, measured in liters). In the dataset, the smoke variable is already coded as a dummy variable: smoke=0 refers to non-smokers and smoke=1 refers to smokers.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fev&lt;-<span class="kw">read.csv</span>(<span class="st">&quot;data/fevdata.txt&quot;</span>,<span class="dt">sep=</span><span class="st">&quot; &quot;</span>)
<span class="kw">names</span>(fev)&lt;-<span class="kw">c</span>(<span class="st">&quot;age&quot;</span>,<span class="st">&quot;fev&quot;</span>,<span class="st">&quot;height&quot;</span>,<span class="st">&quot;sex&quot;</span>,<span class="st">&quot;smoke&quot;</span>)</code></pre></div>
<p>Fit a linear regression model with the dummy <em>smoke</em> as regressor and interpret the model fit (estimated regression coefficient, <span class="math inline">\(95\%\)</span> confidence interval, and hypohtesis test for <span class="math inline">\(H_0:\beta_1=0\)</span> versus <span class="math inline">\(H_1: \beta_1\neq 0\)</span> at the <span class="math inline">\(5\%\)</span> level of significance).</p>
<p><details> <summary markdown="span">Try to make this exercise yourself. If you are ready you can expand this page and look at a solution</summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(fev<span class="op">$</span>fev<span class="op">~</span>fev<span class="op">$</span>smoke, <span class="dt">ylab=</span><span class="st">&quot;FEV (liters)&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;smoke&quot;</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(fev<span class="op">~</span>smoke,<span class="dt">data=</span>fev)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = fev ~ smoke, data = fev)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -10.140  -3.491  -2.093   4.467  13.467 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   5.5329     0.2022   27.36   &lt;2e-16 ***
## smoke         6.5597     0.3820   17.17   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.384 on 651 degrees of freedom
##   (345 observations deleted due to missingness)
## Multiple R-squared:  0.3117, Adjusted R-squared:  0.3107 
## F-statistic: 294.9 on 1 and 651 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m)</code></pre></div>
<pre><code>##                2.5 %   97.5 %
## (Intercept) 5.135845 5.930036
## smoke       5.809588 7.309810</code></pre>
<p>We conclude at the <span class="math inline">\(5\%\)</span> level of significance that smokers have on average a significant larger FEV (<span class="math inline">\(p&lt;0.001\)</span>). We estimate that on average the FEV of smokers is 6.56 (SE=0.38) liters larger than among the non-smokers. The corresponding <span class="math inline">\(95\%\)</span> confidence interval ranges from <span class="math inline">\(5.81\)</span> to <span class="math inline">\(7.31\)</span> liters.</p>
<p></details></p>
</div>
<div id="S:Causality" class="section level2">
<h2><span class="header-section-number">2.10</span> Association versus Causation</h2>
<div id="introduction" class="section level3">
<h3><span class="header-section-number">2.10.1</span> Introduction</h3>
<p>Recall some of the examples that we have seen before:</p>
<ul>
<li><p>Galton's data: we found a significant positive effect of the height of the father on the average height of their sons <span class="math inline">\((p&lt;0.001\)</span>).</p></li>
<li><p>Blood pressure data: we found a significant positive effect of the dose on the average blood pressure reduction (<span class="math inline">\(p=0.003\)</span>).</p></li>
<li><p>Muscle mass data: we found a significant negative effect of the age on the mean muscle mass (<span class="math inline">\(p&lt;0.001\)</span>).</p></li>
<li><p>FEV data: we found a significant larger FEV among the smokers as compared to the non-smokers (<span class="math inline">\(p&lt;0.001\)</span>)</p></li>
</ul>
<p>The conclusions all refer to <strong>associations</strong> between a regressor and the (mean) outcome, but they do not necessarily imply a <strong>causation</strong>. Think about the following questions:</p>
<ul>
<li><p>Can we conclude that increasing fathers' heights (e.g. by means of a better diet in the fathers' youth) causes the average height of their sons to become larger?</p></li>
<li><p>Can we conclude that increasing the daily dose of the active compound in the blood pressure lowering medication causes the average blood pressure reduction to increase?</p></li>
<li><p>Can we conclude that smoking causes the FEV to become larger on average (i.e. smoking causes the lung function to become better)?</p></li>
</ul>
<p>In this section we will argue that for the blood pressure reduction example, we do have established a causal relationship, but for the other examples we do not have demonstrated causality (only association).</p>
<p>All examples listed above, except for the blood pressure study, are <strong>observational studies</strong>. This means that the regressor <span class="math inline">\(X_i\)</span> and the outcome <span class="math inline">\(Y_i\)</span> variables are sampled together, without the researcher having control over the decision which subject <span class="math inline">\(i\)</span> is assigned to what value of <span class="math inline">\(X_i\)</span>. In the blood pressure study, on the other hand, the research planned how the subjects would be assigned to what daily dose. In particular, the subjects in the study were <strong>randomised</strong> over the doses. This is an example of an <strong>experimental study</strong>. We will demonstrate that in the latter case, associations can be interpreted as causal effects.</p>
</div>
<div id="causal-inference-and-counterfactuals" class="section level3">
<h3><span class="header-section-number">2.10.2</span> Causal inference and counterfactuals</h3>
<p><strong>Causal inference</strong> is a discipline in statistics that aims to develop methods that can be used to assess causal relationships. Although it is only rather recent (last 20 years) that it has become a very active research area, concepts of causality were already proposed by Sir Ronald Fisher and Jerzy Neyman in the 1920s. The modern revival started in the early 1980 with e.g. the work of Donald Rubin.</p>
<p>In this section we explain some of the basic concepts of causal inference. As an example we will consider the blood pressure reduction study with only the 0mg/day (placebo) (<span class="math inline">\(X_i=0\)</span>) and the 2mg/day treatment groups (<span class="math inline">\(X_i=1\)</span>) (two-sample problem).</p>
<p>We now define two <strong>counterfactual</strong> outcomes:</p>
<ul>
<li><p><span class="math inline">\(Y_i(1)\)</span> is the outcome of subject <span class="math inline">\(i\)</span> if subject <span class="math inline">\(i\)</span> would receive treatment <span class="math inline">\(X_i=1\)</span></p></li>
<li><p><span class="math inline">\(Y_i(0)\)</span> is the outcome of subject <span class="math inline">\(i\)</span> if subject <span class="math inline">\(i\)</span> would receive treatment <span class="math inline">\(X_i=0\)</span>.</p></li>
</ul>
<p>In reality a subject can only receice a single treatment, either <span class="math inline">\(X_i=1\)</span> or <span class="math inline">\(X_i=0\)</span>, and hence both counterfactuals can never be observed simultaneously. Still we can think of <span class="math inline">\(Y_i(1)\)</span> and <span class="math inline">\(Y_i(0)\)</span> as inherent properties of subject <span class="math inline">\(i\)</span>.</p>
<p>Thus, what we observe is <span class="math inline">\((X_i,Y_i)\)</span>, with <span class="math inline">\(X_i\)</span> (0 or 1) the actual treatment that is received by subject <span class="math inline">\(i\)</span> and <span class="math inline">\(Y_i\)</span> the observed outcome under treatment <span class="math inline">\(X_i\)</span>. Note that the observed <span class="math inline">\(Y_i\)</span> can be written as <span class="math display">\[
  Y_i = X_i Y_i(1) + (1-X_i) Y_i(0).
\]</span></p>
<p>In the context of the blood pressure example:</p>
<ul>
<li><p><span class="math inline">\(Y_i(0)\)</span> is the blood pressure reduction if subject <span class="math inline">\(i\)</span> would have received placebo</p></li>
<li><p><span class="math inline">\(Y_i(1)\)</span> is the blood pressure reduction if subject <span class="math inline">\(i\)</span> would have received 2mg/day.</p></li>
</ul>
<p>Thus conceptually we can also think of <span class="math inline">\(Y_i(1)-Y_i(0)\)</span> as the <strong>causal effect</strong> of the treatment for subject <span class="math inline">\(i\)</span>. This could then serve as the basis for testing the following null hypothesis: <span class="math display">\[
  H_0: Y_i(1)=Y_i(0) \;\;\text{ for all subjects i in the population}.
\]</span> This is known as the <strong>sharp causal null hypothesis</strong>. However, we will be interested in the <strong>average causal effect</strong>, defined in terms of population averages: <span class="math display">\[
  \E{Y(1)-Y(0)}=\E{Y(1)}- \E{Y(0)}.
\]</span> In the remainder of this course, we will simply call this the <strong>causal effect</strong>. In the context of the two-sample problem, it is also known as the <strong>average treatment effect</strong> (ATE).</p>
<p>The question is now: do we have unbiased estimators of <span class="math inline">\(\E{Y(1)}\)</span> and <span class="math inline">\(\E{Y(0)}\)</span>? If yes, we also have an unbiased estimator of the causal effect. It may be tempting to consider <span class="math inline">\(\bar{Y}_1\)</span> (sample mean of observed outcomes in the <span class="math inline">\(X=1\)</span> group) as an estimator of <span class="math inline">\(\E{Y(1)}\)</span>, and <span class="math inline">\(\bar{Y}_0\)</span> (sample mean of observed outcomes in the <span class="math inline">\(X=0\)</span> group) as an estimator of <span class="math inline">\(\E{Y(0)}\)</span>. We will show that this holds true for studies that involve complete randomisation, but it does not hold in general.</p>
<p>Suppose we would use <span class="math inline">\(\bar{Y}_a\)</span> (<span class="math inline">\(a=0,1\)</span>). What is the <em>estimand</em> of this estimator (i.e. what population parameter is estimated by the estimator). Thus we need to look at <span class="math inline">\(\E{\bar{Y}_a}\)</span>. To be more precise, we write <span class="math inline">\(\Ef{XY}{\bar{Y}_a}\)</span> to stress that both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> may be random.</p>
<p>Let <span class="math inline">\(N_a\)</span> denote the number of sample observations in treatment group <span class="math inline">\(a\)</span> (<span class="math inline">\(a=0,1\)</span>), which can be written as</p>
<span class="math display">\[
  N_1 = \sum_{i=1}^n X_i  \;\;\text{ and }\;\; N_0 = \sum_{i=1}^n (1-X_i).
\]</span> Then,
<span class="math display">\[\begin{eqnarray*}
  \Ef{XY}{\bar{Y}_a}
    &amp;=&amp; \Ef{XY}{\frac{1}{N_a}\sum_{i: X_i=a} Y_i} \\
    &amp;=&amp; \Ef{X}{\Ef{Y\mid X}{\frac{1}{N_a}\sum_{i: X_i=a} Y_i}} \\
    &amp;=&amp; \Ef{X}{\frac{1}{N_a} \sum_{i: X_i=a} \Ef{Y\mid X}{Y_i \mid X_i=a}} \\
    &amp;=&amp; \Ef{X}{\frac{1}{N_a} N_a\Ef{Y\mid X}{Y \mid X=a}} \\
    &amp;=&amp; \Ef{Y\mid X}{Y \mid X=a}.
\end{eqnarray*}\]</span>
<p>Hence, for <span class="math inline">\(\bar{Y}_a\)</span> to be an unbiased estimator of <span class="math inline">\(\E{Y(a)}\)</span>, we need to demonstrate that <span class="math display">\[
  \E{Y \mid X=a} = \E{Y(a)}.
\]</span> The conceptual differnce between these two expectation is illustrated in Figure <a href="#fig:Causal1">2.18</a></p>
<div class="figure" style="text-align: center"><span id="fig:Causal1"></span>
<img src="figures/Causal1.png" alt="Difference between means of counterfactuals and conditional means (Figure 1.1. from Hern\'an and Robins, Causal Inference: What if, 2020)."  />
<p class="caption">
Figure 2.18: Difference between means of counterfactuals and conditional means (Figure 1.1. from Hern'an and Robins, Causal Inference: What if, 2020).
</p>
</div>
<p>We will need the following assumptions:</p>
<ul>
<li><p><strong>consistency</strong>: For all subjects <span class="math inline">\(i\)</span>, it must hold that <span class="math inline">\(X_i=a\)</span> implies that <span class="math inline">\(Y_i(a)=Y_i\)</span>. In other words, when a subject receives treatment a, the observed outcome of the subject must be equal to the counterfacual for that treatment.</p></li>
<li><p><strong>mean exchangeability</strong>: <span class="math inline">\(\E{Y(a) \mid X=0} = \E{Y(a) \mid X=1} = \E{Y(a)}\)</span>, for <span class="math inline">\(a=0/1\)</span>. This condition tells us that the populations of subjects that receive placebo (<span class="math inline">\(X=0\)</span>) and that receive treatment (<span class="math inline">\(X=1\)</span>), have the same mean value for the counterfactual <span class="math inline">\(Y(a)\)</span> for treatment <span class="math inline">\(a\)</span> (<span class="math inline">\(a=0/1\)</span>), and, as a consequence these two conditional means are equal to the marginal mean <span class="math inline">\(\E{Y(a)}\)</span>. This means that the treatment allocation <span class="math inline">\(X\)</span> is not associated with the mean of the counterfactual outcomes.</p></li>
</ul>
<p>A property that is even stronger than mean exchangeability, is <strong>full exchangeability</strong>. This states that the treatment assignment <span class="math inline">\(X\)</span> is stochastically independent of the vector of counterfactual outcomes of a subject, <span class="math inline">\((Y(0),Y(1))\)</span>, i.e. <span class="math display">\[
  (Y(0), Y(1)) \ind X.
\]</span> This implies that the treatment <span class="math inline">\(X_i\)</span> given to subject <span class="math inline">\(i\)</span>, is jointly independent of the counterfactual outcomes <span class="math inline">\(Y_i(0)\)</span> and <span class="math inline">\(Y_i(1)\)</span>. Full exchangeability implies mean exchangeability.</p>
<p>Now we are ready to prove the identity <span class="math inline">\(\E{Y \mid X=a} = \E{Y(a)}\)</span>. For <span class="math inline">\(a=0,1\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\E{Y(a)}=\E{Y(a) \mid X=a}\)</span> (implied by mean exchangeability)</p></li>
<li><p><span class="math inline">\(\E{Y(a) \mid X=a} = \E{Y \mid X=a}\)</span> (implied by consistency).</p></li>
</ol>
</div>
<div id="randomised-studies" class="section level3">
<h3><span class="header-section-number">2.10.3</span> Randomised studies</h3>
<p>Full exchangeability is implied by a <strong>completely randomised study</strong>, i.e. a study in which subjects are randomly assigned to the treatment groups. For example, the binary treatment indicator can be Bernoulli distributed with <span class="math display">\[
  \prob{X_i=1}=\frac{1}{2}.
\]</span> This may come with or without the extra condition <span class="math inline">\(\sum_{i=1}^n X_i=n_1\)</span> (fixed group sample sizes). This randomisation scheme is independent of the counterfactual outcomes and hence it implies full exchangeability.</p>
<p>Recall that the blood pressure reduction study is a randomised study: 20 subjects were randomised over the placebo and 2mg/day treatment. The conclusions that we have reached in our previous statistical analysis can thus be causally interpreted. The difference in sample means is thus an estimate of the ATE.</p>
<p>The reasoning that we have build in this section does not only apply to 2 treatment groups. It can easily be extended to more than 2 treatment groups or to the setting of linear regression.</p>
<p>Let us reconsider a few earlier examples:</p>
<ul>
<li><p>The full blood pressure reduction study, with the 4 treatment groups, that was previously analysed with a linear regression analysis also allows for causal conclusions, because all 40 students were randomised over the 4 treatment groups (doses). It is still a randomised study.</p></li>
<li><p>The study on the effect of smoking on the FEV consisted of a sample of 654 subjects, and for these subjects the smoking status (<span class="math inline">\(X_i\)</span>) and the FEV outcome (<span class="math inline">\(Y_i\)</span>) were observed. At best the sample may be a random sample from a larger population, but the <em>treatment</em> `smoking' was not randomly assigned to the subjects. This would of course not be ethical! So this is an <strong>observational study</strong>, and the full or mean exchangeability condition is not satisfied. Our conclusion that the average FEV is larger among the smokers than among the non-smokers (suggesting that smoking is beneficial for the lung fuction) is thus not a causal conclusion. Modern causal inference methods provide other data analysis methods that could allow for causal conclusions in observational studies (outside the scope of this course).</p></li>
<li><p>The study designs of the Galton and the muscle mass example are also observational, and hence the conclusions for these studies are also only in terms of association rather than in terms of causation.</p></li>
</ul>
<p>To conclude we give an example that demonstrates how the full exchangeability condition can be violated, even in experimental studies (but without proper randomisation).</p>
</div>
</div>
<div id="S:Reg1BIPI" class="section level2">
<h2><span class="header-section-number">2.11</span> Estimation of the Conditional Mean Outcome</h2>
</div>
<div id="example-blood-pressure" class="section level2 unnumbered">
<h2>Example (Blood Pressure)</h2>
<p>Now that we have fitted the regression model for the blood pressure example, we could be interested in estimating the avere blood pressure reduction for patients that would receive 3mg/day of the medication. We are thus interested in estimating <span class="math display">\[
  \E{Y \mid x} = m(x) = \beta_0 + \beta_1 x
\]</span> with <span class="math inline">\(x=3\)</span> mg/day.</p>
<p>With the estimates of the regression parameters, we can estimate this conditional mean as <span class="math display">\[
  \hat{m}(x) = \hat\beta_0 +\hat\beta_1 x.
\]</span> With <span class="math inline">\(x=3\)</span> and with <span class="math inline">\(\hat\beta_0=0.033\)</span> and <span class="math inline">\(\hat\beta_1=1.79\)</span>, we thus estimate the average blood reduction for patients that take 3 mg/day as <span class="math display">\[
  0.033 + 1.79 \times 3 = 5.4 \text{ mmHg}.
\]</span></p>
<p>Just as before, we need to realise that this estimate does not give any information on the (im)precision of this estimate. So there is again need for a standard error and a confidence interval. So again we need to know the sampling distribution of our estimator <span class="math inline">\(\hat{m}(x)\)</span>. Note that the estimator is a function of <span class="math inline">\(x\)</span>, and hence also the standard error and the sampling distribution of the estimator may depend on <span class="math inline">\(x\)</span>.</p>
<div id="sampling-distribution" class="section level3 unnumbered">
<h3>Sampling Distribution</h3>
<p>Here we give results for the normal linear regression model <a href="#eq:Mod4">(2.7)</a> for which we can have the exact sampling distributions of the parameter estimators, and hence also the exact sampling distribution of the estimator of the conditional mean <span class="math inline">\(\E{Y\mid x}\)</span>. </p>

<div class="theorem">
<span id="thm:DistrMHat" class="theorem"><strong>Theorem 2.9  (Sampling distribution of <span class="math inline">\(\hat{m}(x)\)</span> with normal error terms)  </strong></span> Assume that model <a href="#eq:Mod4">(2.7)</a> holds. Then, for a given <span class="math inline">\(x\)</span>, <span class="math display">\[
   \hat{m}(x) \sim N\left(\beta_0 + \beta_1 x, \sigma_{m}^2(x)\right)
 \]</span> with <span class="math display">\[
    \sigma^2_{{m}}(x) = \begin{pmatrix} 1 &amp; x \end{pmatrix}
    \mb\Sigma_\beta \begin{pmatrix} 1\\ x \end{pmatrix} = \sigma_{\beta_0}^2 + x^2 \sigma_{\beta_1}^2 + 2x\cov{\hat\beta_0,\hat\beta_1}
 \]</span> with (see Equation <a href="#eq:SigmaBetaLSE">(2.6)</a>) <span class="math display">\[ 
   \mb\Sigma_\beta = \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \begin{pmatrix}
   \frac{1}{n}\sum_{i=1}^n x_i^2   &amp; - \bar{x} \\
   -\bar{x}                                      &amp; 1 \end{pmatrix} .
 \]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span>  We write <span class="math display">\[
   \hat{m}(x) = \hat\beta_0+\hat\beta_1 x= \hat{\mb{\beta}}^t \mb{x}
 \]</span> with <span class="math display">\[
    \hat{\mb{\beta}}^t = (\hat\beta_0, \hat\beta_1) \;\;\text{ and }\;\; \mb{x}^t=(1, x).
 \]</span> Theorem <a href="#thm:DistrMod4">2.4</a> now gives <span class="math display">\[
     \hat{\mb{\beta}} \sim N({\mb{\beta}},\mb\Sigma_\beta) .
 \]</span> Since <span class="math inline">\(\hat{m}(x)=\hat{\mb{\beta}}^t \mb{x}\)</span> is a linear combination of the elements of <span class="math inline">\(\hat{\mb{\beta}}\)</span>, the estimator <span class="math inline">\(\hat{m}(x)\)</span> is also normally distributed with mean <span class="math display">\[
   \E{\hat{m}(x)}=\E{\hat{\mb{\beta}}^t \mb{x}} = \E{\hat{\mb{\beta}}}^t\mb{x}=\mb\beta^t\mb{x} = \beta_0+\beta_1x
 \]</span> and variance <span class="math display">\[
   \var{\hat{m}(x)} = \var{\hat{\mb{\beta}}^t \mb{x}} = \mb{x}^t \var{\hat{\mb{\beta}}}\mb{x}=\mb{x}^t \mb\Sigma_\beta\mb{x}=\sigma_m^2(x).
 \]</span>
</div>

<hr />
<p>Without the normality assumption (i.e. model <a href="#eq:Mod3">(2.2)</a>) we become the next theorem (without proof). Since the theorem expresses an asymptotic result, it would have been better to use notation that stresses the dependence on the sample size <span class="math inline">\(n\)</span>, but we ignore this for now. As before we will use asymptotic results for approximate inference for <em>large</em> samples. </p>

<div class="theorem">
<span id="thm:DistrMHatAsym" class="theorem"><strong>Theorem 2.10  (Sampling distribution of <span class="math inline">\(\hat{m}(x)\)</span>)  </strong></span> Assume that model <a href="#eq:Mod3">(2.2)</a> holds. Then, for a given <span class="math inline">\(x\)</span>, and as <span class="math inline">\(n \rightarrow \infty\)</span>, it holds that <span class="math display">\[
   \sqrt{n}\frac{\hat{m}(x)-m(x)}{\sigma_{m*}(x)} \convDistr N\left(0, 1\right)
 \]</span> with <span class="math display">\[
    \sigma^2_{m*}(x) = \begin{pmatrix} 1 &amp; x \end{pmatrix}
    \mb\Sigma_n
    \begin{pmatrix} 1\\ x \end{pmatrix}.
 \]</span> and <span class="math display">\[
   \mb\Sigma_n = \frac{\sigma^2}{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2} 
   \begin{pmatrix}
   \frac{1}{n}\sum_{i=1}^n x_i^2   &amp; - \bar{x} \\
   -\bar{x}                                      &amp; 1 \end{pmatrix} .
 \]</span> Or (approximately for large <span class="math inline">\(n\)</span>) <span class="math display">\[
   \hat{m}(x) \ApproxSim N(m(x),\sigma_m^2(x))
 \]</span> with <span class="math inline">\(\sigma_m^2(x) = \sigma_{m*}^2(x)/n\)</span>.
</div>

</div>
<div id="confidence-interval-ci" class="section level3 unnumbered">
<h3>Confidence Interval (CI)</h3>
<p>The <span class="math inline">\(1-\alpha\)</span> confidence interval (CI) of the conditional mean <span class="math inline">\(m(x)=\E{Y\mid x}\)</span> (for a given <span class="math inline">\(x\)</span>) in model <a href="#eq:Mod4">(2.7)</a> follows from theorem <a href="#thm:DistrMHat">2.9</a>. First we write <span class="math display">\[
  \frac{\hat{m}(x)-m(x)}{\sigma_m(x)} \sim N(0,1).
\]</span> The only unknown in <span class="math inline">\(\sigma^2_m(x)\)</span> is the residual variance <span class="math inline">\(\sigma^2\)</span> that can be estimated by the MSE. Upon using MSE we obtain the estimator <span class="math inline">\(\hat\sigma_m^2(x)\)</span>.</p>
<p>Theorem <a href="#thm:DistrMSE">2.7</a> relates MSE to a chi-squared distribution with <span class="math inline">\(n-2\)</span> degrees of freedom, and hence (similar to Theorem <a href="#thm:DistrStudBeta">2.8</a>) <span class="math display">\[
  \frac{\hat{m}(x)-m(x)}{\hat\sigma_m(x)} \sim t_{n-2}.
\]</span> Form the identify <span class="math display">\[
  \prob{-t_{n-2;1-\alpha/2} &lt; \frac{\hat{m}(x) - m(x)}{\hat\sigma_m(x)} &lt; t_{n-2;1-\alpha/2}} = 1-\alpha
\]</span> it follows that <span class="math display">\[
  \prob{\hat{m}(x)-t_{n-2;1-\alpha/2} \hat\sigma_m(x)&lt;  m(x) &lt; \hat{m}(x)+t_{n-2;1-\alpha/2} \hat\sigma_m(x)} = 1-\alpha
\]</span> from which the <span class="math inline">\(1-\alpha\)</span> confindence interval of <span class="math inline">\(m(x)=\E{Y\mid x}\)</span> follows: <span class="math display">\[
 \left[\hat{m}(x)-t_{n-2;1-\alpha/2} \hat\sigma_m(x), \hat{m}(x)+t_{n-2;1-\alpha/2} \hat\sigma_m(x)\right].
\]</span></p>
</div>
</div>
<div id="example-blood-pressure-1" class="section level2 unnumbered">
<h2>Example (Blood Pressure)</h2>
<p>The following R code gives the estimate of the average blood pressure reduction of patients that recieved a dose of 3 mg/day. We also give the <span class="math inline">\(95\%\)</span> confidence interval.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(bp.reduction<span class="op">~</span>dose,<span class="dt">data=</span>BloodPressure)
<span class="kw">predict</span>(m,<span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">dose=</span><span class="dv">3</span>))</code></pre></div>
<pre><code>##       1 
## 5.39207</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(m,<span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">dose=</span><span class="dv">3</span>),<span class="dt">interval=</span><span class="st">&quot;confidence&quot;</span>,
    <span class="dt">level=</span><span class="fl">0.95</span>)</code></pre></div>
<pre><code>##       fit      lwr      upr
## 1 5.39207 4.007517 6.776624</code></pre>
<p>So we estimate that patients receiving a daily dose of 3mg will show on average a reduction in their diastolic blood pressure of <span class="math inline">\(5.4\)</span> mmHg. The <span class="math inline">\(95\%\)</span> confidence interval goes from 4 to <span class="math inline">\(6.8\)</span> mmHg. In other words, with a probability of <span class="math inline">\(95\%\)</span> we expect that the diastolic blood pressure reduces on average with some value between 4 and <span class="math inline">\(6.8\)</span> mmHg.</p>
<p>The R code shown below illustrates how the lower and upper bounds of <span class="math inline">\(95\%\)</span> confindence intervals of the mean blood pressure reduction for patients can be calculated for a series of values for the daily dose. Based on these calculations we obtain Figure <a href="#fig:RegCI">2.19</a>. The plot shows that the lower and upper bounds of the confidence interval, as a function of <span class="math inline">\(x\)</span>, describe a hyperbole.</p>
<p>We may not overinterpret this graph: although the lower and upper bounds are visually presented as hyperboles, we can only interpret them in a <strong>pointwise</strong> fashion. Only for a given value of the regressor, the corresponding lower and upper bounds can be interpreted as a 95% CI of the conditional mean outcome. The hyperboles are <em>not</em> a confidence envelope that tells us that with a probability of 95% we may expect the true regression line to be in between the hyperbole lines.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xs&lt;-<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="fl">0.2</span>)
m.CI&lt;-<span class="kw">predict</span>(m,<span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">dose=</span>xs),
    <span class="dt">interval=</span><span class="st">&quot;confidence&quot;</span>)
<span class="kw">plot</span>(bp.reduction<span class="op">~</span>dose,<span class="dt">data=</span>BloodPressure,<span class="dt">xlab=</span><span class="st">&quot;dose (mg / day)&quot;</span>,
    <span class="dt">ylab=</span><span class="st">&quot;diastolic blood pressure reduction (mmHg)&quot;</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>)
<span class="kw">lines</span>(xs,m.CI[,<span class="dv">2</span>],<span class="dt">col=</span><span class="dv">4</span>)
<span class="kw">lines</span>(xs,m.CI[,<span class="dv">3</span>],<span class="dt">col=</span><span class="dv">4</span>)
<span class="kw">abline</span>(<span class="kw">coef</span>(<span class="kw">lm</span>(bp.reduction<span class="op">~</span>dose,<span class="dt">data=</span>BloodPressure)),<span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:RegCI"></span>
<img src="DASM2_files/figure-html/RegCI-1.png" alt="Scatter plot of the blood pressure reduction data with the estimated regression line (red line). The graph also shows the under and upper bounds (blue lines) of the 95\% confidence intervals of the conditional means  $\E{Y\mid x}$ as a function of $x$ (dose)." width="672" />
<p class="caption">
Figure 2.19: Scatter plot of the blood pressure reduction data with the estimated regression line (red line). The graph also shows the under and upper bounds (blue lines) of the 95% confidence intervals of the conditional means <span class="math inline">\(\E{Y\mid x}\)</span> as a function of <span class="math inline">\(x\)</span> (dose).
</p>
</div>
</div>
<div id="S:PI" class="section level2">
<h2><span class="header-section-number">2.12</span> Predictions and Prediction Intervals (PI)</h2>
</div>
<div id="example-blood-pressure-2" class="section level2 unnumbered">
<h2>Example (Blood Pressure)</h2>
<p>From previous analyses we know that the fitted regression model is given by <span class="math display">\[
  \hat{m}(x) = 0.033 - 1.79 x.
\]</span></p>
<p>We now want to use this fitted regression model to make a prediction of the diastolic blood pressure reduction for <em>a</em> patient that receives <span class="math inline">\(x=3\)</span> mg/day. So we are now concerned with the <strong>prediction of an individual outcome</strong> for a given value of the regressor.</p>
<p>Prediction only makes sense if the to-be-predicted outcome is not yet observed. This <em>future</em> outcome, conditional on some <span class="math inline">\(x\)</span>, is denoted as <span class="math inline">\(Y^*\)</span>. Since <span class="math inline">\(Y^*\)</span> is not yet observed, it is not part of the sample observations used for fitting the regression model.</p>
<p>We will denote the prediction as <span class="math inline">\(\hat{Y}\)</span> or <span class="math inline">\(\hat{Y}(x)\)</span>, or as <span class="math inline">\(\hat{y}\)</span> or <span class="math inline">\(\hat{y}(x)\)</span>.</p>
<p>So far we know that <span class="math inline">\(\hat{m}(x)\)</span> is an estimate of the mean outcome (blood pressure reduction) at regressor value <span class="math inline">\(x\)</span> (daily dose), i.e. <span class="math inline">\(\hat{m}(x)\)</span> is an estimate of a conditional mean. I will now argue that this is also a good prediction for an individual outcome at regressor value <span class="math inline">\(x\)</span>:</p>
<ul>
<li><p>we know that <span class="math inline">\(\hat{m}(x)\)</span> is an estimate of the conditional mean <span class="math inline">\(\E{Y\mid x}\)</span>, which is the point on the regression line at <span class="math inline">\(x\)</span>.</p></li>
<li><p>regression model <a href="#eq:Mod4">(2.7)</a> states that for a given <span class="math inline">\(x\)</span>, the outcomes <span class="math inline">\(Y\)</span> are normally distributed with mean <span class="math inline">\(\E{Y\mid x}\)</span></p></li>
<li><p>since this normal distribution is symmetric about its mean <span class="math inline">\(\E{Y\mid x}\)</span>, it is equally likely to observe an outcome smaller than <span class="math inline">\(\E{Y\mid x}\)</span> than it is to observe an outcome larger than <span class="math inline">\(\E{Y\mid x}\)</span></p></li>
<li><p>we do not have information that allows us to suspect that an individual (to-be-predicted) outcome is smaller or larger than the conditional mean <span class="math inline">\(\E{Y\mid x}\)</span></p></li>
</ul>
<p>Because of these reasons, the point on the estimated regression line, i.e. the estimate <span class="math inline">\(\hat{m}(x)\)</span> of <span class="math inline">\(\E{Y\mid x}\)</span>, is our best prediction of an outcome at regressor value <span class="math inline">\(x\)</span>.</p>
<p>Back to the blood pressure example. So we predict that the diastolic blood pressure reduction of a patient that is on the 3mg/day regime is given by <span class="math display">\[
  \hat{y}(3) = 0.033 + 1.79 \times 3 = 5.4 \text{ mmHg}.
\]</span></p>
<p>Note that <span class="math inline">\(\hat{y}(3)\)</span> is numerically identical to <span class="math inline">\(\hat{m}(3)\)</span>, but because of the difference in interpretation we use different notation.</p>
<p>The prediction is comparable to a point estimate in the sense that it is only a single numerical result, and that it does not contain any information about how well it succeeds in predicting the outcome. Later in this section we will construct prediction intervals that can serve this purpose, but first we dig a little deeper into the concept of prediction.</p>
<p>So we aim at predicting an individual outcome for a given value of <span class="math inline">\(x\)</span>. We will assume in this section that this outcome behaves according the same statistical model as the sample observations. Thus we assume that model <a href="#eq:Mod4">(2.7)</a> applies, <span class="math display">\[
  Y^* = m(x) + \eps^* = \beta_0 + \beta_1 x + \eps^*
\]</span> with <span class="math inline">\(\eps^* \sim N(0,\sigma^2)\)</span>.</p>
<p>Moreover, we will assume that this outcome is independent from the <span class="math inline">\(n\)</span> sample observations, i.e. <span class="math inline">\(Y^* \ind (Y_1,\ldots, Y_n)\)</span>, or, equivalently, <span class="math inline">\(\eps^* \ind (Y_1,\ldots, Y_n)\)</span>. Hence, the parameter estimator <span class="math inline">\(\hat{\mb\beta}\)</span> is independent of <span class="math inline">\(Y^*\)</span> (or <span class="math inline">\(\eps^*\)</span>).</p>
<p>Now that we know the prediction <span class="math inline">\(\hat{Y}(x)\)</span> and the to-be-predicted outcome <span class="math inline">\(Y^*\)</span>, we can define a measure to quantify the qualilty of the prediction. The <strong>prediction error</strong> is given by <span class="math display">\[
  \hat{Y}(x)-Y^*,
\]</span> but this can never be observed, because, by definition, <span class="math inline">\(Y^*\)</span> is not observed (yet). Moreover, since both <span class="math inline">\(\hat{Y}(x)\)</span> and <span class="math inline">\(Y^*\)</span> show sampling variability (i.e. they are both random variables), it makes more sense to investigate the quality of the predictions <em>on average</em> over many repeated experiments. In each repeated experiment, we both get new sample data <span class="math inline">\((X_1,Y_1), \ldots, (X_n,Y_n)\)</span>, as well as a new to-be-predicted outcome <span class="math inline">\(Y^*\)</span>. Here we define the <strong>expected conditional test error</strong> (sometimes also referred to as the <strong>mean squared error</strong>) as <span class="math display">\[
   \text{Err}(x) = \Ef{\mb{Y} Y^*}{(\hat{Y}(x)-Y^*)^2} 
\]</span> in which the expectation is with respect to the the distribution of the sample data and the to-be-predicted outcome. In a later chapter we will come back to this measure.</p>
<p>Both the mean squared error and prediction intervals are based on the <strong>sampling distribution of the prediction error</strong>, which, under model <a href="#eq:Mod4">(2.7)</a>, is given by <span class="math display">\[
  \hat{Y}(x) - Y^* \sim N(0,\sigma^2_m(x)+\sigma^2).
\]</span> This is a direct consequence of the sampling distribution of <span class="math inline">\(\hat{Y}(x)=\hat{m}(x)\)</span>, as given by Theorem <a href="#thm:DistrMHat">2.9</a>, and the independence between the sample observations and <span class="math inline">\(Y^*\)</span>. When the normality assumption does not hold, the result is approximately valid for large sample sizes.</p>
<p>The <strong>prediction interval</strong> (PI) is given in the next theorem.</p>

<div class="theorem">
<span id="thm:PIMod4" class="theorem"><strong>Theorem 2.11  (Prediciton interval for a given <span class="math inline">\(x\)</span> and with normal error terms)  </strong></span>Assume that model <a href="#eq:Mod4">(2.7)</a> holds. Then, for a given <span class="math inline">\(x\)</span> and a confidence level <span class="math inline">\(1-\alpha\)</span>,<br />

<span class="math display" id="eq:tmp23676268725">\[\begin{equation}
   \prob{\hat{Y}(x) - t_{n-2;1-\alpha/2}\sqrt{\hat\sigma_m^2(x)+\MSE} \leq Y^* \leq \hat{Y}(x) + t_{n-2;1-\alpha/2}\sqrt{\hat\sigma_m^2(x)+\MSE}} = 1-\alpha.
    \tag{2.11}
 \end{equation}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> For the standardised prediction error we have <span class="math display">\[
   \frac{\hat{Y}(x)-Y^*}{\sqrt{\sigma_m^2(x)+\sigma^2}}\sim N(0,1).
 \]</span> From theorem <a href="#thm:DistrMHat">2.9</a> we know that <span class="math inline">\(\sigma_m^2(x)\)</span> is proportional to <span class="math inline">\(\sigma^2\)</span>. Hence, we can write <span class="math inline">\(\sigma_m^2(x)=\sigma_m^{\prime 2}(x)\sigma^2\)</span>, with <span class="math inline">\(\sigma_m^{\prime 2}(x)\)</span> a proportionality factor. The only unknown in <span class="math inline">\(\sigma^2_m(x)\)</span> is the residual variance <span class="math inline">\(\sigma^2\)</span> that can be estimated by MSE. Upon using MSE we become the estimator <span class="math inline">\(\hat\sigma_m^2(x)=\sigma_m^{\prime 2}(x) \MSE\)</span>. Hence, the variance <span class="math inline">\(\sigma_m^2(x)+\sigma^2\)</span> is estimated by <span class="math inline">\(MSE(\sigma_m^{\prime 2}(x)+1)\)</span>. We further know that <span class="math inline">\((n-2)\MSE / \sigma^2 \sim \chi^2_{n-2}\)</span>. Hence, <span class="math display">\[
  \frac{\hat{Y}(x)-Y^*}{\sqrt{MSE(\sigma_m^{\prime 2}(x)+1)}}\sim t_{n-2},
\]</span> and thus <span class="math display">\[
  \prob{-t_{n-2;1-\alpha/2} \leq \frac{\hat{Y}(x)-Y^*}{\sqrt{MSE(\sigma_m^\prime(x)^2+1)}} \leq t_{n+2;1-\alpha/2}}=1-\alpha.
\]</span> Form this probability statement the prediction interval follows immediately.
</div>

<p>Note that the probability expression in <a href="#eq:tmp23676268725">(2.11)</a> has another structure than for confidence intervals. In the prediction interval there is a random variable (<span class="math inline">\(Y^*\)</span>) in between the two inequality signs, whereas in confidence intervals it is a population parameter.</p>
</div>
<div id="example-blood-pressure-3" class="section level2 unnumbered">
<h2>Example (Blood Pressure)</h2>
<p>The next R code calculates the predicted blood pressure reduction for a patient on the 3 mg/day regime, as well as a 95% prediction interval.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(bp.reduction<span class="op">~</span>dose,<span class="dt">data=</span>BloodPressure)
<span class="kw">predict</span>(m,<span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">dose=</span><span class="dv">3</span>),
    <span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>,<span class="dt">level=</span><span class="fl">0.95</span>)</code></pre></div>
<pre><code>##       fit       lwr      upr
## 1 5.39207 -3.033452 13.81759</code></pre>
<p>Conclusion: we predict that a patient with a daily dose of 3mg will show a reduction in diastolic blood pressure of <span class="math inline">\(5.4\)</span> mmHg after a period of 2 months. With a probability of 95% we expect for a patient on the 3 mg/day regime for a period of 2 months that his/her diastolic blood pressure will reduce with up to 13.8 mmHg or increase with up to 3 mmHg.</p>
<p>The next chunck of R code makes a graph with lower and upper limits of 95% prediction intervals of the blood pressure reduction for a sequence of daily doses; see Figure <a href="#fig:RegPI">2.20</a>. The under and upper bounds describe a hyperbole. Note that these bounds are furhter away from the fitted regression line than the bounds of confidence intervals (for the same confidence level <span class="math inline">\(1-\alpha\)</span>). Again only a pointwise interpretation is correct.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xs&lt;-<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="fl">0.01</span>)
m.PI&lt;-<span class="kw">predict</span>(m,<span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">dose=</span>xs),<span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>)
<span class="kw">plot</span>(bp.reduction<span class="op">~</span>dose,<span class="dt">data=</span>BloodPressure,<span class="dt">xlab=</span><span class="st">&quot;dose (mg/day)&quot;</span>,
    <span class="dt">ylab=</span><span class="st">&quot;diastolic blood pressure reduction (mmHg)&quot;</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>)
<span class="kw">lines</span>(xs,m.PI[,<span class="dv">2</span>],<span class="dt">col=</span><span class="dv">4</span>)
<span class="kw">lines</span>(xs,m.PI[,<span class="dv">3</span>],<span class="dt">col=</span><span class="dv">4</span>)
<span class="kw">abline</span>(<span class="kw">coef</span>(<span class="kw">lm</span>(bp.reduction<span class="op">~</span>dose,<span class="dt">data=</span>BloodPressure)),<span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:RegPI"></span>
<img src="DASM2_files/figure-html/RegPI-1.png" alt="Scatter plot of the blood pressure data and the fitted regression line (red line). The graph shows the lower and upper bounst of 95\% prediction intervals as a function of the dose." width="672" />
<p class="caption">
Figure 2.20: Scatter plot of the blood pressure data and the fitted regression line (red line). The graph shows the lower and upper bounst of 95% prediction intervals as a function of the dose.
</p>
</div>
<div id="simulation-study" class="section level3">
<h3><span class="header-section-number">2.12.1</span> Simulation Study</h3>
<p>To better understand the difference between confidence intervals and prediction intervals, their interpretations are here illustrated with a simulation study.</p>
<p>The following R code simulates repeated sampling from a linear regression model. For each sample, <span class="math inline">\(\hat{m}(x)=\hat{y}(x)\)</span> is computed for <span class="math inline">\(x=3\)</span>, together with its 95% confidence interal and prediction interval. For each repeated experiment we check whether the true conditional mean <span class="math inline">\(\E{Y\mid x}\)</span> is within the CI and whether a new independently sampled outcome <span class="math inline">\(Y^*\)</span> is within the PI.</p>
<p>The settings in this simulation study are inspired by the blood reduction example.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">126778</span>)
N&lt;-N10000 <span class="co"># number of repeated samples</span>
dose&lt;-<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">10</span>) <span class="co"># 4 doses</span>
dose&lt;-<span class="kw">rep</span>(dose,<span class="dv">10</span>) <span class="co"># 10 replicates for each dose</span>
x&lt;-<span class="dv">3</span> <span class="co"># dose for which the conditional mean must be estimated and an outcome must be predicted</span>
mx&lt;-<span class="fl">0.03</span><span class="op">+</span><span class="fl">1.79</span><span class="op">*</span>x <span class="co"># true conditional mean </span>
m.hat&lt;-<span class="kw">c</span>() 
cnt.CI&lt;-<span class="dv">0</span>  
cnt.PI&lt;-<span class="dv">0</span>  
CI.all&lt;-<span class="kw">matrix</span>(<span class="dt">nrow=</span>N,<span class="dt">ncol=</span><span class="dv">2</span>) 
PI.all&lt;-<span class="kw">matrix</span>(<span class="dt">nrow=</span>N,<span class="dt">ncol=</span><span class="dv">2</span>) 
YStars&lt;-<span class="kw">c</span>() 
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    y&lt;-<span class="fl">0.03</span><span class="op">+</span><span class="fl">1.79</span><span class="op">*</span>dose<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">40</span>,<span class="dt">sd=</span><span class="dv">4</span>) <span class="co"># random sample van uitkomsten</span>
    m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>dose) <span class="co"># model fit</span>
    CI&lt;-<span class="kw">predict</span>(m,<span class="kw">data.frame</span>(<span class="dt">dose=</span>x),<span class="dt">interval=</span><span class="st">&quot;confidence&quot;</span>)
    <span class="cf">if</span>((mx<span class="op">&gt;</span>CI[<span class="dv">2</span>])<span class="op">&amp;</span>(mx<span class="op">&lt;</span>CI[<span class="dv">3</span>])) cnt.CI&lt;-cnt.CI<span class="op">+</span><span class="dv">1</span> <span class="co"># if mx is within CI then counter is increased with one</span>
    yStar&lt;-<span class="fl">0.03</span><span class="op">+</span><span class="fl">1.79</span><span class="op">*</span>x<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">1</span>,<span class="dt">sd=</span><span class="dv">4</span>) <span class="co"># a new outcome (to be predicted)</span>
    PI&lt;-<span class="kw">predict</span>(m,<span class="kw">data.frame</span>(<span class="dt">dose=</span>x),<span class="dt">interval=</span><span class="st">&quot;predict&quot;</span>)
    <span class="cf">if</span>((yStar<span class="op">&gt;</span>PI[<span class="dv">2</span>])<span class="op">&amp;</span>(yStar<span class="op">&lt;</span>PI[<span class="dv">3</span>])) cnt.PI&lt;-cnt.PI<span class="op">+</span><span class="dv">1</span> <span class="co"># if yStar is within PI then counter is increased with one</span>
    CI.all[i,]&lt;-CI[<span class="dv">2</span><span class="op">:</span><span class="dv">3</span>]
    PI.all[i,]&lt;-PI[<span class="dv">2</span><span class="op">:</span><span class="dv">3</span>]
    YStars&lt;-<span class="kw">c</span>(YStars,yStar)
}
CI.coverage&lt;-cnt.CI<span class="op">/</span>N
PI.coverage&lt;-cnt.PI<span class="op">/</span>N

CI.coverage</code></pre></div>
<pre><code>## [1] 0.95</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">PI.coverage</code></pre></div>
<pre><code>## [1] 0.9493</code></pre>
<p>Based on <span class="math inline">\(N=\)</span> 10^{4} repeated experimetns, we find an empirical coverage of 0.95 for the CI and 0.9493 for the PI. Both are very close to the nominal lavel of <span class="math inline">\(95\%\)</span>.</p>
<p>Figure <a href="#fig:BIPIReg">2.21</a> visualises the results of the first 50 repeated samples.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(<span class="ot">NULL</span>,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">11</span>,<span class="dv">21</span>),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">50</span>),<span class="dt">xlab=</span><span class="st">&quot;repeated experiment&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;diastolic blood pressure reduction (mmHg)&quot;</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">50</span>) {
    colour&lt;-<span class="dv">2</span><span class="op">-</span><span class="kw">as.numeric</span>((mx<span class="op">&gt;</span>CI.all[i,<span class="dv">1</span>])<span class="op">&amp;</span>(mx<span class="op">&lt;</span>CI.all[i,<span class="dv">2</span>]))
    <span class="kw">lines</span>(<span class="kw">c</span>(i,i),<span class="kw">c</span>(CI.all[i,<span class="dv">1</span>],CI.all[i,<span class="dv">2</span>]),<span class="dt">col=</span>colour)
}
<span class="kw">points</span>(YStars[<span class="dv">1</span><span class="op">:</span><span class="dv">50</span>],<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">cex=</span><span class="fl">0.5</span>)
<span class="kw">abline</span>(<span class="dt">h=</span>mx,<span class="dt">col=</span><span class="dv">3</span>)

<span class="kw">plot</span>(<span class="ot">NULL</span>,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">11</span>,<span class="dv">21</span>),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">50</span>),<span class="dt">xlab=</span><span class="st">&quot;repeated experiment&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;diastolic blood pressure reduction (mmHg)&quot;</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">50</span>) {
    colour&lt;-<span class="dv">2</span><span class="op">-</span><span class="kw">as.numeric</span>((YStars[i]<span class="op">&gt;</span>PI.all[i,<span class="dv">1</span>])<span class="op">&amp;</span>(YStars[i]<span class="op">&lt;</span>PI.all[i,<span class="dv">2</span>]))
    <span class="kw">lines</span>(<span class="kw">c</span>(i,i),<span class="kw">c</span>(PI.all[i,<span class="dv">1</span>],PI.all[i,<span class="dv">2</span>]),<span class="dt">col=</span>colour)
}
<span class="kw">points</span>(YStars[<span class="dv">1</span><span class="op">:</span><span class="dv">50</span>],<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">cex=</span><span class="fl">0.5</span>)
<span class="kw">abline</span>(<span class="dt">h=</span>mx,<span class="dt">col=</span><span class="dv">3</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:BIPIReg"></span>
<img src="DASM2_files/figure-html/BIPIReg-1.png" alt="95\% confidencen intervals (left) en prediction intervals (right) of 50 repeated experiments. Black lines cover the true conditional mean (left) or the new to-be-predicted outcome (right), the red lines do not. The green horizontal reference line corresponds to $m(3)=0.03+1.79*3=5.4$. The blue points are the to-be-predicted outcomes." width="672" />
<p class="caption">
Figure 2.21: 95% confidencen intervals (left) en prediction intervals (right) of 50 repeated experiments. Black lines cover the true conditional mean (left) or the new to-be-predicted outcome (right), the red lines do not. The green horizontal reference line corresponds to <span class="math inline">\(m(3)=0.03+1.79*3=5.4\)</span>. The blue points are the to-be-predicted outcomes.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
</div>
</div>
<div id="exercise-galtons-data" class="section level2 unnumbered">
<h2>Exercise: Galton's data</h2>
<p>Consider again the Galton's height example. Give predictions of a son's heights with a father of 160 cm and for a son's height with a father of 175 cm, as wel the corresponding 90% prediction intervals.</p>
<p><details> <summary markdown="span">Try to make this exercise yourself. If you are ready you can expand this page and look at a solution</summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(son.cm<span class="op">~</span>father.cm, <span class="dt">data=</span>Galton.sons)
<span class="kw">predict</span>(m, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">father.cm=</span><span class="kw">c</span>(<span class="dv">160</span>,<span class="dv">175</span>)),
        <span class="dt">interval =</span> <span class="st">&quot;prediction&quot;</span>,
        <span class="dt">level =</span> <span class="fl">0.90</span>)</code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 171.0432 161.4991 180.5872
## 2 178.6580 169.2686 188.0474</code></pre>
<p>We predict the heigths of a son of a father of 160 cm and 175 cm as 171 cm and 179 cm, respectively.</p>
<p>With a probability of 90% we excpect that height of a son of a father of 160 cm to be in between 162 and 181 cm.</p>
<p>With a probability of 90% we excpect that height of a son of a father of 175 cm to be in between 169 and 188 cm. </details></p>
</div>
<div id="decomposition-of-the-total-sum-of-squares" class="section level2">
<h2><span class="header-section-number">2.13</span> Decomposition of the Total Sum of Squares</h2>
<p>In this section we discuss the construction of <strong>sum of squares</strong>. The convention is to present these sum of squares in a table which is known as the <strong>analysis of variance table</strong> or the <strong>anova table</strong>. <em>Anova</em> stands for <strong>analysis of variance</strong>.</p>

<div class="definition">
<span id="def:unnamed-chunk-59" class="definition"><strong>Definition 2.2  (Total sum of squares)  </strong></span> The total sum of squares is given by <span class="math display">\[
   \SSTot = \sum_{i=1}^n (Y_i-\bar{Y})^2 .
 \]</span>
</div>

<p>SSTot measures the total variabilty of the outcome in the sample. The statistic <span class="math display">\[
  \frac{\SSTot}{n-1} 
\]</span> is the sample variance of the <strong>marginal distribution</strong> of the outcome. The marginal distribution of <span class="math inline">\(Y\)</span> has mean <span class="math inline">\(\E{Y}\)</span>, which is estimated by the sample mean <span class="math inline">\(\bar{Y}\)</span>. The statistic <span class="math inline">\(\frac{\SSTot}{n-1}\)</span> is thus an estimator of the variance of the marginal distribution.</p>
<p>In this chapter the focus is mainly on the conditional distribution of the outcome <span class="math inline">\(Y\)</span> given the regressor <span class="math inline">\(x\)</span>, and particularly on the conditional mean <span class="math inline">\(\E{Y\mid x}\)</span>. We already know that MSE is an estimator of the variance of <span class="math inline">\(Y\)</span> given <span class="math inline">\(\mb{x}\)</span> (i.e. the variance of the error term <span class="math inline">\(\eps\)</span>).</p>
<p>Figure <a href="#fig:RegMod3Marginal">2.22</a> gives an illustration based on two conditional distributions and the marginal distribution.</p>
<div class="figure" style="text-align: center"><span id="fig:RegMod3Marginal"></span>
<img src="DASM2_files/figure-html/RegMod3Marginal-1.png" alt="Illustration of the normal linear regression model. The black line represents the regression line $m(x;\beta)$. The red points are the outcomes from the conditional distribution of $Y$ given $x=2$ and the blue plots come from the conditional distribution of $Y$ given $x=5$. The red and the bleu lines show the corresponding density functions of these conditional distributions. The black curve at the right side of the graph represents the density function of the marginal distribution of $Y$." width="672" />
<p class="caption">
Figure 2.22: Illustration of the normal linear regression model. The black line represents the regression line <span class="math inline">\(m(x;\beta)\)</span>. The red points are the outcomes from the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x=2\)</span> and the blue plots come from the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x=5\)</span>. The red and the bleu lines show the corresponding density functions of these conditional distributions. The black curve at the right side of the graph represents the density function of the marginal distribution of <span class="math inline">\(Y\)</span>.
</p>
</div>

<div class="definition">
<span id="def:unnamed-chunk-60" class="definition"><strong>Definition 2.3  (Sum of squares of the regression)  </strong></span>The sum of squares of the regression is defined as <span class="math display">\[
  \SSR = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2 = \sum_{i=1}^n (\hat{m}(x_i) - \bar{Y})^2.
\]</span>
</div>

<p>SSR is a measure for the deviation between the fitted regression line (<span class="math inline">\(\hat{Y}_i = \hat{m}(x_i)=\hat\beta_0+\hat\beta_1 x_i\)</span>) and a regression line with intercept only. Note that for the latter regression model (<span class="math inline">\(\E{Y \mid x}=\beta_0\)</span>) the intercept is estimated as <span class="math inline">\(\hat\beta_0=\bar{Y}\)</span> and hence the fitted regression line is a flat line at <span class="math inline">\(\hat{m}(x)=\bar{Y}\)</span>. In other words, SSE measures the size of the regressor effect in the sense that <span class="math inline">\(\SSR \approx 0\)</span> indicates that the regressor has hardly an effect on the conditional outcome mean, and <span class="math inline">\(\SSR &gt;0\)</span> indicates an effect of the regressor.</p>
<p>Finally, we repeat the definition of the sum of squared errors (SEE) as given in Equation <a href="#eq:SSEReg1">(2.3)</a>: <span class="math display">\[
  \SSE = \sum_{i=1}^n (\hat{Y}_i -Y_i)^2 = \sum_{i=1}^n (\hat{m}(x_i) -Y_i)^2.
\]</span> We know already that SSE is a measure for the deviation between the sample outcomes and the predictions (points on the fitted regression model) at the observed regressor values <span class="math inline">\(x_i\)</span>. The smaller SSE, the better the fit of the regression line. </p>

<div class="theorem">
<span id="thm:unnamed-chunk-61" class="theorem"><strong>Theorem 2.12  (Decomposition of the Total sum of Squares)  </strong></span> <span class="math display">\[
   \SSTot = \SSR + \SSE
 \]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> 
<span class="math display">\[\begin{eqnarray*}
  \SSTot 
    &amp;=&amp;  \sum_{i=1}^n (Y_i-\bar{Y})^2 \\
    &amp;=&amp;  \sum_{i=1}^n (Y_i-\hat{Y}_i+\hat{Y}_i-\bar{Y})^2 \\
    &amp;=&amp;  \sum_{i=1}^n (Y_i-\hat{Y}_i)^2+\sum_{i=1}^n(\hat{Y}_i-\bar{Y})^2  + 2 \sum_{i=1}^n (Y_i-\hat{Y}_i)(\hat{Y}_i-\bar{Y})\\
    &amp;=&amp;  \SSE+\SSR  + 2 \sum_{i=1}^n (Y_i-\hat{Y}_i)(\hat{Y}_i-\bar{Y}).
  \end{eqnarray*}\]</span>
Upon using <span class="math inline">\(\hat{Y}_i=\hat\beta_0+\hat\beta_1x_i\)</span> and the parameter estimates as given in Theorem <a href="#thm:LSEReg1">2.1</a>, we rewrite the last term as
<span class="math display">\[\begin{eqnarray*}
    \sum_{i=1}^n (Y_i-\hat{Y}_i)(\hat{Y}_i-\bar{Y}) 
       &amp;=&amp; \sum_{i=1}^n (Y_i-\hat{Y})(\hat\beta_0+\hat\beta_1 x_i - \bar{Y}) \\
       &amp;=&amp; \sum_{i=1}^n (Y_i-\hat{Y}_i)(\bar{Y}-\hat\beta_1\bar{x}+\hat\beta_1x_i-\bar{Y}) \\
       &amp;=&amp; \sum_{i=1}^n (Y_i-\hat{Y}_i)(\hat\beta_1(x_i-\bar{x})) \\
       &amp;=&amp; \hat\beta_1\sum_{i=1}^n (Y_i-\hat{Y}_i)(x_i-\bar{x}) \\
       &amp;=&amp; \hat\beta_1\sum_{i=1}^n (Y_i-\hat{Y}_i)x_i -\hat\beta_1\bar{x}\sum_{i=1}^n (Y_i-\hat{Y}_i) .
  \end{eqnarray*}\]</span>
The first term is eqal to zero because the parameter estimates in <span class="math inline">\(\hat{Y}_i\)</span> are the solutions of the estimating equations <a href="#eq:LSEEE1">(2.5)</a>. Similarly, the second term is also equal to zero (estimating equation <a href="#eq:LSEEE0">(2.4)</a>).
</div>

<p>The meaning of the decomposition of SSTot is the following. The total variability in the outcome data (SSTot) is partly explained by the regression relationship (SSR). The variability that is not explained by the fitted regression model, is the residual variability (SSE).</p>

<div class="definition">
<span id="def:unnamed-chunk-63" class="definition"><strong>Definition 2.4  (Coefficient of Determination)  </strong></span>The coefficient of determination is defined as <span class="math display">\[
   R^2 = 1-\frac{\SSE}{\SSTot}.
 \]</span>
</div>

<p>The coefficient of determination can also be written as <span class="math display">\[
   R^2 = 1-\frac{\SSE}{\SSTot}=1-\frac{\SSTot-\SSR}{\SSTot}=\frac{\SSR}{\SSTot}.
\]</span> So it is the fraction of the total variability in the sample outcomes that is explained by the fitted regression line. We can also say that it is the proportion of the total variability in the observed outcomes that is predictable by the fitted regression line.</p>
</div>
<div id="example-galtons-height" class="section level2 unnumbered">
<h2>Example (Galton's height)</h2>
<p>We look again at the output of the analysis of Galton's data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(son.cm<span class="op">~</span>father.cm,<span class="dt">data=</span>Galton.sons)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = son.cm ~ father.cm, data = Galton.sons)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.9406  -3.5300   0.2605   3.4064  20.5805 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 89.81819   11.73609   7.653 1.37e-12 ***
## father.cm    0.50766    0.06683   7.596 1.91e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.661 on 171 degrees of freedom
## Multiple R-squared:  0.2523, Adjusted R-squared:  0.2479 
## F-statistic:  57.7 on 1 and 171 DF,  p-value: 1.907e-12</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(m)</code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: son.cm
##            Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## father.cm   1 1849.1 1849.13  57.701 1.907e-12 ***
## Residuals 171 5480.0   32.05                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>In the output we read <span class="math inline">\(R^2=0.2523\)</span>. Hence, only 25% of the variability in the observed heights of sons can be explained by the linear regression relationship with their father's height.</p>
<p>The anova table gives <span class="math inline">\(\SSR=1849.1\)</span> and <span class="math inline">\(\SSE=5480\)</span>. Hence, <span class="math inline">\(\SSTot=\SSR+\SSE=7329.1\)</span>. With these numbers we also obtain <span class="math inline">\(R^2=\SSR / \SSTot=0.2523\)</span>.</p>
<p>This data analysis is also an example where the <span class="math inline">\(R^2\)</span> is small, but the effect of the regressor on the mean outcome is highly significant (<span class="math inline">\(p&lt;0.001\)</span>). This may be explained by the sample size: the <span class="math inline">\(p\)</span>-value is sensitive to the sample size; the <span class="math inline">\(p\)</span>-value measures evidence in the data against the null in favor of the alternative hypothesis, and with increasing sample size the evidence can accumulate. The coefficient of determination, however, is not sensitive to the sample size.</p>
<p>And, finally, we may not forget that the <span class="math inline">\(p\)</span>-value refers to hypotheses in terms of a <span class="math inline">\(\beta\)</span>-parameter that has an interpretation related to the (conditional) <em>mean</em> outcome, whereas prediction refer to individual outcomes.</p>
</div>
<div id="exercise-r2-and-prediction" class="section level2 unnumbered">
<h2>Exercise: <span class="math inline">\(R^2\)</span> and prediction</h2>
<p>A large <span class="math inline">\(R^2\)</span> is sometimes considered as an indication that the model can potentially be used as a good prediction model. However, this is generally not true.</p>
<p>You are asked to empirically demonstrate this statement. Simulate two datasets, both with the same residual variance <span class="math inline">\(\sigma^2\)</span>, but one with a small non-zero value for <span class="math inline">\(\beta_1\)</span> and another with a large value for <span class="math inline">\(\beta_1\)</span>. Discuss your findings.</p>
<p><details> <summary markdown="span">Try to make this exercise yourself. If you are ready you can expand this page and look at a solution</summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2627</span>)
x&lt;-<span class="dv">0</span><span class="op">:</span><span class="dv">10</span>  <span class="co"># regressor values</span>
sigma&lt;-<span class="dv">1</span> <span class="co"># residual standard deviation</span>
beta0&lt;-<span class="dv">1</span> <span class="co"># intercept parameter (the same for the two simulated data sets)</span>
eps&lt;-<span class="kw">rnorm</span>(<span class="dv">11</span>,<span class="dt">sd=</span>sigma) <span class="co"># simulation of the error terms (these will be used for the two simulated datasets)</span>

<span class="co"># simulation of first data set</span>
beta1&lt;-<span class="dv">1</span>
y&lt;-beta0<span class="op">+</span>beta1<span class="op">*</span>x<span class="op">+</span>eps
<span class="kw">plot</span>(x,y,
     <span class="dt">cex.lab=</span><span class="fl">1.5</span>, <span class="dt">cex.axis=</span><span class="fl">1.5</span>)
m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
<span class="kw">abline</span>(<span class="kw">coef</span>(m),<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.44430 -0.75415  0.04751  0.71378  1.26962 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.22797    0.58101   0.392    0.704    
## x            1.15065    0.09821  11.716 9.44e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.03 on 9 degrees of freedom
## Multiple R-squared:  0.9385, Adjusted R-squared:  0.9316 
## F-statistic: 137.3 on 1 and 9 DF,  p-value: 9.438e-07</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(m)</code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: y
##           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## x          1 145.640 145.640  137.27 9.438e-07 ***
## Residuals  9   9.549   1.061                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># simulation of first data set</span>
beta1&lt;-<span class="fl">0.2</span>
y&lt;-beta0<span class="op">+</span>beta1<span class="op">*</span>x<span class="op">+</span>eps
<span class="kw">plot</span>(x,y,
     <span class="dt">cex.lab=</span><span class="fl">1.5</span>, <span class="dt">cex.axis=</span><span class="fl">1.5</span>)
m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x)
<span class="kw">abline</span>(<span class="kw">coef</span>(m),<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-66-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.44430 -0.75415  0.04751  0.71378  1.26962 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)  0.22797    0.58101   0.392  0.70392   
## x            0.35065    0.09821   3.570  0.00602 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.03 on 9 degrees of freedom
## Multiple R-squared:  0.5862, Adjusted R-squared:  0.5402 
## F-statistic: 12.75 on 1 and 9 DF,  p-value: 0.00602</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(m)</code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: y
##           Df  Sum Sq Mean Sq F value  Pr(&gt;F)   
## x          1 13.5252  13.525  12.748 0.00602 **
## Residuals  9  9.5486   1.061                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>These two analyses illustrate that <span class="math inline">\(R^2\)</span> is not fully informative about the predictability of outcomes. Both settings give exactly the same MSE (<span class="math inline">\(\MSE=1.061\)</span>), because I used twice the same vector of simulated error terms. The MSE estimates the variability about the true regression line and it is the variability of to-be-predicted outcomes about this regression line.</p>
<p>The <span class="math inline">\(R^2\)</span> is larger for the larger value of <span class="math inline">\(\beta_1\)</span>, because the larger <span class="math inline">\(\beta_1\)</span> causes the range of sample outcomes to be larger, which in terms results in a larger SSTot. Thus, <span class="math inline">\(R^2=1-\SSE / \SSTot\)</span> is larger for the setting with the larger <span class="math inline">\(\beta_1\)</span> (larger SSTot).</p>
<p></details></p>
<hr />

</div>
</div>
<div id="Ch:Reg2" class="section level1">
<h1><span class="header-section-number">Hoofdstuk 3</span> Multiple Linear Regression Analysis</h1>
<div id="example-lead-concentration" class="section level2 unnumbered">
<h2>Example (Lead concentration)</h2>
<p>In the 1970s several studies focused on the environmental and health risks of lead smelters. The dataset considered here involves children who lived near a lead smelter in El Paso, Texas, USA. Children who were exposed to the lead smelter were included in this study, as well as a control group of children. outcomes of interest are</p>
<ul>
<li><p>finger-wrist tapping score (variable name: <em>MAXFWT</em>). It is the number of finger taps in a period of 10 second. It is considered a proxy for the integrity of the neuromuscular system. The higher the score the better. It can be measured for the left and the right hand. We will use the maximum of the two as outcome variable.</p></li>
<li><p>full-scale IQ score (variable name: <em>Iqf</em>). The higher the score the better.</p></li>
</ul>
<p>One of the research questions was how the MAXWT is related to the lead concentrations in the blood (<em>Ld72</em> and <em>Ld73</em> for measurements in 1972 and 1973 ) and to the total number of years the child lived in the proxy of the lead smelter (<em>Totyrs</em>). However, we should perhaps account for other variables in the dataset, such as the age (<em>Age</em>) and the gender (<em>Sex</em>) of the children.</p>
<p>This dataset was downloaded from <a href="https://www2.stat.duke.edu/courses/Fall00/sta102/HW/lab1.html" class="uri">https://www2.stat.duke.edu/courses/Fall00/sta102/HW/lab1.html</a>.</p>
<p>Before we introduce the multiple linear regression model, we will start with some data exploration of the dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;Data/lead.RData&quot;</span>)
<span class="kw">skim</span>(lead)</code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-67">Table 3.1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">lead</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">102</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">11</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">11</td>
</tr>
<tr class="odd">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Id</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">205.30</td>
<td align="right">118.57</td>
<td align="right">101.00</td>
<td align="right">126.25</td>
<td align="right">151.50</td>
<td align="right">213.75</td>
<td align="right">505.00</td>
<td align="left">▇▂▁▂▁</td>
</tr>
<tr class="even">
<td align="left">Area</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">1.64</td>
<td align="right">0.73</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">2.00</td>
<td align="right">3.00</td>
<td align="left">▇▁▆▁▂</td>
</tr>
<tr class="odd">
<td align="left">Age</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">9.05</td>
<td align="right">3.52</td>
<td align="right">3.75</td>
<td align="right">6.38</td>
<td align="right">8.67</td>
<td align="right">12.06</td>
<td align="right">15.92</td>
<td align="left">▇▇▅▆▅</td>
</tr>
<tr class="even">
<td align="left">Sex</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">1.38</td>
<td align="right">0.49</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">2.00</td>
<td align="right">2.00</td>
<td align="left">▇▁▁▁▅</td>
</tr>
<tr class="odd">
<td align="left">Iqf</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">91.91</td>
<td align="right">14.36</td>
<td align="right">50.00</td>
<td align="right">82.25</td>
<td align="right">91.50</td>
<td align="right">100.75</td>
<td align="right">141.00</td>
<td align="left">▁▆▇▂▁</td>
</tr>
<tr class="even">
<td align="left">Lead_type</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">1.24</td>
<td align="right">0.43</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">2.00</td>
<td align="left">▇▁▁▁▂</td>
</tr>
<tr class="odd">
<td align="left">Ld72</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">34.17</td>
<td align="right">17.28</td>
<td align="right">1.00</td>
<td align="right">24.00</td>
<td align="right">31.00</td>
<td align="right">38.00</td>
<td align="right">99.00</td>
<td align="left">▂▇▂▁▁</td>
</tr>
<tr class="even">
<td align="left">Ld73</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">31.58</td>
<td align="right">10.77</td>
<td align="right">15.00</td>
<td align="right">24.00</td>
<td align="right">28.00</td>
<td align="right">38.00</td>
<td align="right">58.00</td>
<td align="left">▅▇▃▃▂</td>
</tr>
<tr class="odd">
<td align="left">Totyrs</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">6.73</td>
<td align="right">3.37</td>
<td align="right">1.00</td>
<td align="right">4.00</td>
<td align="right">6.00</td>
<td align="right">9.00</td>
<td align="right">15.00</td>
<td align="left">▃▇▃▃▁</td>
</tr>
<tr class="even">
<td align="left">MAXFWT</td>
<td align="right">19</td>
<td align="right">0.81</td>
<td align="right">52.05</td>
<td align="right">12.90</td>
<td align="right">13.00</td>
<td align="right">47.00</td>
<td align="right">52.00</td>
<td align="right">59.00</td>
<td align="right">84.00</td>
<td align="left">▁▁▇▅▁</td>
</tr>
<tr class="odd">
<td align="left">Exposed</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">0.24</td>
<td align="right">0.43</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">1.00</td>
<td align="left">▇▁▁▁▂</td>
</tr>
</tbody>
</table>
<p>We look at scatter plots to explore the relation between the FWT and the blood lead concentrations (in 1973) and the total number of years the children lived in the neighborhood of the lead smelter. For these two settings, we will fit simple linear regression models and plot the fitted regression lines.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Ld73&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Ld73,<span class="dt">data=</span>lead)
<span class="kw">summary</span>(m.Ld73)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -42.308  -5.808   0.993   7.584  30.661 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  64.3660     4.0020  16.083   &lt;2e-16 ***
## Ld73         -0.3938     0.1206  -3.266   0.0016 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.2 on 81 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.1164, Adjusted R-squared:  0.1055 
## F-statistic: 10.67 on 1 and 81 DF,  p-value: 0.0016</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m.Ld73)</code></pre></div>
<pre><code>##                  2.5 %     97.5 %
## (Intercept) 56.4032516 72.3287437
## Ld73        -0.6337485 -0.1539078</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Totyrs&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Totyrs,<span class="dt">data=</span>lead)
<span class="kw">summary</span>(m.Totyrs)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Totyrs, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -37.341  -5.070  -0.584   7.552  29.902 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  42.8836     3.2174  13.329  &lt; 2e-16 ***
## Totyrs        1.2429     0.3964   3.136  0.00239 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.25 on 81 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.1082, Adjusted R-squared:  0.09723 
## F-statistic: 9.832 on 1 and 81 DF,  p-value: 0.00239</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m.Totyrs)</code></pre></div>
<pre><code>##                  2.5 %    97.5 %
## (Intercept) 36.4820525 49.285110
## Totyrs       0.4542191  2.031607</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(lead,
       <span class="kw">aes</span>(<span class="dt">x=</span>Ld73, <span class="dt">y=</span>MAXFWT)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">color=</span><span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;blood lead concentration (microgram/100ml)&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;FWT (taps per 10 seconds)&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>), <span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept=</span>m.Ld73<span class="op">$</span>coefficients[<span class="dv">1</span>],<span class="dt">slope=</span>m.Ld73<span class="op">$</span>coefficients[<span class="dv">2</span>])</code></pre></div>
<pre><code>## Warning: Removed 19 rows containing missing values (geom_point).</code></pre>
<p><img src="DASM2_files/figure-html/unnamed-chunk-69-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(lead,
       <span class="kw">aes</span>(<span class="dt">x=</span>Totyrs, <span class="dt">y=</span>MAXFWT)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">color=</span><span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Number of years living close to smelter&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;FWT (taps per 10 seconds)&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>), <span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept=</span>m.Totyrs<span class="op">$</span>coefficients[<span class="dv">1</span>],<span class="dt">slope=</span>m.Totyrs<span class="op">$</span>coefficients[<span class="dv">2</span>])</code></pre></div>
<pre><code>## Warning: Removed 19 rows containing missing values (geom_point).</code></pre>
<p><img src="DASM2_files/figure-html/unnamed-chunk-69-2.png" width="672" /></p>
<p>From these two regression analyses, we conclude at the 5% level of significance that there is a significant negative effect of the blood lead concentration on the expected FWT (<span class="math inline">\(p=0.0016\)</span>) and that there is a significant positive effect of the total number of years the child lived close to the lead smelter and the expected blood lead concentration (<span class="math inline">\(p=0.0024\)</span>). Particularly, this latter result is counter-intuitive. It suggests that the longer children are exposed, the better are their FWT score on average. This effect is estimated as an average increase in FWT of 1.2 taps/10seconds for each extra year living near the lead smelter. The 95% confidence of this effect goes from 0.5 to 2 taps/10seconds for each extra year. An explanation will be give later.</p>
</div>
<div id="S:AddMeervoudigModel" class="section level2">
<h2><span class="header-section-number">3.1</span> The Additive Multiple Linear Regression Model</h2>
<div id="the-statistical-model" class="section level3 unnumbered">
<h3>The Statistical Model</h3>
<p>We will use the following notation:</p>
<ul>
<li><p><span class="math inline">\(n\)</span>: total number of sample observations</p></li>
<li><p><span class="math inline">\(p-1\)</span>: number of regressors</p></li>
<li><p><span class="math inline">\(Y_i\)</span>: outcome of observation <span class="math inline">\(i=1,\ldots, n\)</span></p></li>
<li><p><span class="math inline">\(x_{ij}\)</span>: value of regressor <span class="math inline">\(j=1,\ldots, p-1\)</span> for observation <span class="math inline">\(i=1,\ldots, n\)</span>.</p></li>
</ul>
<p>We use the notation <span class="math inline">\(x_j\)</span> for referring to regressor <span class="math inline">\(j\)</span>, and <span class="math inline">\(x_{ij}\)</span> for referring to the value of regressor <span class="math inline">\(j\)</span> for observation <span class="math inline">\(i\)</span>.</p>
We now introduce the additive <strong>normal multiple linear regression model</strong>,
<span class="math display" id="eq:Mod5">\[\begin{equation}
 Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{p-1} x_{ip-1} + \eps_ i
 \tag{3.1}
\end{equation}\]</span>
<p>with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>.</p>
<p>This model can be equivalently represented as <span class="math display">\[
  Y_i \mid x_{i1}, x_{i2}, \ldots, x_{ip-1} \iid N(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{p-1} x_{ip-1},\sigma^2) ,
\]</span> which better stresses that it is a model for the conditional distribution of the outcome, given the regressors.</p>
<p>Research questions can often be translated into a statistical problem by making use of the regression coefficients (i.e. the <span class="math inline">\(\beta\)</span>-parameters). These have an interpretation in terms of the conditional mean of the outcome, i.e.</p>
<p><span class="math display">\[
  m(x_1,x_2,\ldots, x_{p-1}) = \E{Y \mid x_1,x_2,\ldots, x_{p-1}} = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \cdots + \beta_{p-1} x_{p-1}.
\]</span></p>
<p>The interpretation of the <span class="math inline">\(\beta\)</span>-parameters is similar as for the simple regression model, but still there is a very important difference. For parameter <span class="math inline">\(\beta_1\)</span> this follows from the following calculation:</p>
<span class="math display">\[\begin{eqnarray*}
&amp; &amp; 
 \E{Y \mid x_1+1, x_2, \ldots, x_{p-1}} - \E{Y \mid x_1, x_2, \ldots, x_{p-1}} \\
&amp;=&amp;  m(x_1+1, x_2, \ldots, x_{p-1}) - m(x_1, x_2, \ldots, x_{p-1}) \\
&amp;=&amp; \left(\beta_0 + \beta_1 (x_{1}+1) + \beta_2 x_{2} + \cdots + \beta_{p-1} x_{p-1}\right) \\
&amp; &amp; - \left(\beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \cdots + \beta_{p-1} x_{p-1}\right) \\
&amp;=&amp; \beta_1.
\end{eqnarray*}\]</span>
<p>Hence, the parameter <span class="math inline">\(\beta_1\)</span> quantifies the increase in expected outcome when the regressor <span class="math inline">\(x_1\)</span> increases with one unit, while the other regressors in the model remain constant. The expression <em>while the other regressors in the model remain constant</em> is very important for a correct interpretation of the parameter <span class="math inline">\(\beta_1\)</span>.</p>
<p>More generally, for parameter <span class="math inline">\(\beta_j\)</span>,</p>
<span class="math display">\[\begin{eqnarray*}
&amp; &amp; 
 \E{Y \mid x_1, \ldots, x_j+1,\ldots, x_{p-1}} - \E{Y \mid x_1, \ldots, x_j,\ldots, x_{p-1}} \\
&amp;=&amp;  m(x_1, \ldots, x_j+1,\ldots, x_{p-1}) - m(x_1, \ldots,  x_j, \ldots, x_{p-1}) \\
&amp;=&amp; \left(\beta_0 + \beta_1 x_{1} + \cdots + \beta_j (x_{j}+1) + \cdots + \beta_{p-1} x_{p-1}\right) \\
&amp; &amp; - \left(\beta_0 + \beta_1 x_{1} + \cdots + \beta_j x_{j} + \cdots + \beta_{p-1} x_{p-1}\right) \\
&amp;=&amp; \beta_j.
\end{eqnarray*}\]</span>
<p>The parameter <span class="math inline">\(\beta_j\)</span> thus quantifies the expected increase of the outcome when the regressor <span class="math inline">\(x_j\)</span> increases with one unit while the other regressors in the model remain constant.</p>
<p>Although model <a href="#eq:Mod5">(3.1)</a> is not much more complicated that the simple linear model, the notation is more demanding (because of the multple regressors). A more efficient and compact notation is provided by the matrix notation. We will use the following notation.</p>

With the matrix notation, model <a href="#eq:Mod5">(3.1)</a> can be written as
<span class="math display" id="eq:Mod6">\[\begin{equation}
 \tag{3.2}
 \mb{Y} = \mb{X}\mb{\beta} + \mb{\eps}
\end{equation}\]</span>
<p>with <span class="math inline">\(\mb{\eps} \sim \text{MVN}(\mb{0},\mb{I}_n\sigma^2)\)</span>. With this notation we also write <span class="math inline">\(m(\mb{x})\)</span> instead of <span class="math inline">\(m(x_1, \ldots, x_{p-1})\)</span>.</p>
<p>The parameters of models <a href="#eq:Mod5">(3.1)</a> and <a href="#eq:Mod6">(3.2)</a> can be estimated with the least squares estimation procedure. The method is identical to the method described in Section <a href="#S:LSE1">2.2</a>.</p>
<p>All the results that were provided in Chapter <a href="#Ch:Reg1">2</a> and that made use of the matrix notation, are still valid here. In the previous chapter the vector <span class="math inline">\(\mb\beta\)</span> had two elements (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>). Now the vector <span class="math inline">\(\mb\beta\)</span> has <span class="math inline">\(p\)</span> elements (<span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_{p-1}\)</span>). The design matrix had 2 columns, whereas it now has <span class="math inline">\(p\)</span> columns. All theory related to the sampling distributions, confidence and prediction intervals and hypothesis tests remain valid for the additive multiple linear regression model. This will now be illustrated for the lead concentration example.</p>
</div>
</div>
<div id="example-lead-concentration-1" class="section level2 unnumbered">
<h2>Example (Lead concentration)</h2>
<p>We will focus here on the research question that aims to assess the effects of the blood lead concentration and the total number of years living near the smelter on the FWT score. We thus consider the model</p>
<p><span class="math display">\[
   Y_i = \beta_0 + \beta_1 \text{Ld73}_i + \beta_2 \text{Totyrs}_i +\eps_i
 \]</span> with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>.</p>
<p>The next chunck of R code demonstrates how the parameters can be estimated with the least squares method, making use of the matrix notation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># indicator for the rows without missing values for MAXFWT</span>
indNA&lt;-<span class="op">!</span><span class="kw">is.na</span>(lead<span class="op">$</span>MAXFWT)

Y&lt;-lead<span class="op">$</span>MAXFWT[indNA]
XReg&lt;-<span class="kw">as.matrix</span>(lead[indNA,<span class="kw">c</span>(<span class="st">&quot;Ld73&quot;</span>,<span class="st">&quot;Totyrs&quot;</span>)])
<span class="kw">head</span>(XReg)</code></pre></div>
<pre><code>##   Ld73 Totyrs
## 1   18     11
## 2   28      6
## 3   29      5
## 4   30      5
## 5   34     11
## 6   25      6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X&lt;-<span class="kw">cbind</span>(<span class="dv">1</span>,XReg)
<span class="kw">head</span>(X)</code></pre></div>
<pre><code>##     Ld73 Totyrs
## 1 1   18     11
## 2 1   28      6
## 3 1   29      5
## 4 1   30      5
## 5 1   34     11
## 6 1   25      6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">beta.hat&lt;-<span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X)<span class="op">%*%</span><span class="kw">t</span>(X)<span class="op">%*%</span>Y
beta.hat</code></pre></div>
<pre><code>##              [,1]
##        55.3138948
## Ld73   -0.4115833
## Totyrs  1.3029689</code></pre>
From these calculations we find
<span class="math display">\[\begin{eqnarray*}
  \hat\beta_0 &amp;=&amp; 55.31 \\
  \hat\beta_1 &amp;=&amp; -0.41 \\
  \hat\beta_2 &amp;=&amp; 1.30 .
 \end{eqnarray*}\]</span>
<p>The same estimates can be obtained with the <em>lm</em> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.LdTot&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Ld73<span class="op">+</span>Totyrs,<span class="dt">data=</span>lead)
<span class="kw">summary</span>(m.LdTot)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73 + Totyrs, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.665  -4.731  -0.464   7.645  26.638 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  55.3139     4.5428  12.176  &lt; 2e-16 ***
## Ld73         -0.4116     0.1130  -3.642 0.000478 ***
## Totyrs        1.3030     0.3698   3.524 0.000707 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 11.42 on 80 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.2351, Adjusted R-squared:  0.216 
## F-statistic: 12.29 on 2 and 80 DF,  p-value: 2.21e-05</code></pre>
<p>Interpretation of the regression coefficients:</p>
<ul>
<li><p>We estimate that childeren that all lived for the same time near the smelter, their FWT decreases on average with <span class="math inline">\(4\)</span> taps in a 10 second interval for an increase of their blood lead levels with 10 microgram per 100ml. This effect is often referred to as the effect of blood lead concentrations, <em>corrected for</em> (or <em>controlling for</em>) years living near the smelter. Note that for expressing the effect of blood lead levels we do not need to specify the exact value of the <em>Totyrs</em>. The effect of blood lead levels is the same for each fixed value of <em>Totyrs</em>.</p></li>
<li><p>We estimate that children that all have the same blood lead contrentrations, their FWT increases on average with <span class="math inline">\(1.3\)</span> taps in a 10 second interval for an increase of their time living near the smelter with 1 year. This is the estimated effect of time, <em>corrected for</em> (or <em>controlling for</em>) blood lead levels. Note that for expressing the effect of <em>Totyrs</em> we do not need to specify the exact value of the <em>Ld73</em>. The effect of <em>Totyrs</em> is the same for each fixed value of <em>Ld73</em>.</p></li>
</ul>
<p>With only two regressors, the fitted regression model can be visualised in a three-dimensional graph. It is a flat plane. See Figure <a href="#fig:LeadFit3D">3.1</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">range</span>(lead<span class="op">$</span>Ld73)[<span class="dv">1</span>],<span class="kw">range</span>(lead<span class="op">$</span>Ld73)[<span class="dv">2</span>], <span class="dt">length.out =</span> <span class="dv">10</span>)
y &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">range</span>(lead<span class="op">$</span>Totyrs)[<span class="dv">1</span>],<span class="kw">range</span>(lead<span class="op">$</span>Totyrs)[<span class="dv">2</span>], <span class="dt">length.out =</span> <span class="dv">10</span>)
MAXFWT &lt;-<span class="st"> </span><span class="kw">outer</span>(x, y, <span class="cf">function</span>(a, b){<span class="kw">predict</span>(m.LdTot,<span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">Ld73=</span>a,<span class="dt">Totyrs=</span>b))})

axx&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;blood lead concentration (microgram/100ml)&quot;</span>)
axy&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;Number of years living near smelter&quot;</span>)
axz&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;FWT (taps per 10 seconds)&quot;</span>)

<span class="kw">plot_ly</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">add_markers</span>(<span class="dt">x=</span>lead<span class="op">$</span>Ld73,
              <span class="dt">y=</span>lead<span class="op">$</span>Totyrs,
              <span class="dt">z=</span>lead<span class="op">$</span>MAXFWT) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_surface</span>(<span class="dt">x=</span><span class="op">~</span>x,
              <span class="dt">y=</span><span class="op">~</span>y,
              <span class="dt">z=</span><span class="op">~</span>MAXFWT,
              <span class="dt">showscale=</span><span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">layout</span>(<span class="dt">scene =</span> <span class="kw">list</span>(<span class="dt">xaxis=</span>axx,<span class="dt">yaxis=</span>axy,<span class="dt">zaxis=</span>axz))</code></pre></div>
<pre><code>## Warning: Ignoring 19 observations</code></pre>
<pre><code>## Warning: `arrange_()` is deprecated as of dplyr 0.7.0.
## Please use `arrange()` instead.
## See vignette(&#39;programming&#39;) for more help
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.</code></pre>
<div class="figure" style="text-align: center"><span id="fig:LeadFit3D"></span>
<div id="htmlwidget-94327bfd12639e906873" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-94327bfd12639e906873">{"x":{"visdat":{"dc114dcb416":["function () ","plotlyVisDat"]},"cur_data":"dc114dcb416","attrs":{"dc114dcb416":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[18,28,29,30,34,25,24,15,16,24,19,27,29,32,25,23,28,19,22,22,27,38,31,25,24,24,20,18,18,26,27,24,22,31,34,37,25,20,16,21,34,27,22,26,30,33,26,23,23,23,24,31,39,32,23,16,28,33,34,38,21,31,33,53,49,40,40,47,45,43,58,48,50,40,58,57,51,47,48,43,54,52,34,27,26,20,27,28,31,28,37,34,34,23,25,25,23,43,49,47,45,45],"y":[11,6,5,5,11,6,6,15,7,7,12,10,12,12,10,10,15,9,8,11,7,10,5,2,2,2,9,1,1,5,5,5,6,6,12,6,2,11,9,9,10,6,3,3,5,15,6,14,6,5,5,3,3,8,8,8,7,2,6,6,6,8,15,10,11,9,6,4,12,6,7,6,6,5,6,7,10,12,8,8,6,8,3,4,3,4,4,4,4,4,4,3,4,4,5,4,6,4,4,4,3,3],"z":[72,61,49,48,51,49,50,58,50,51,59,65,57,53,74,50,84,46,52,64,59,55,null,46,52,63,52,42,57,23,65,38,59,26,53,50,56,49,76,68,60,46,57,45,46,64,40,62,13,79,61,46,50,48,65,62,56,54,72,57,50,65,56,54,57,48,41,34,54,38,49,58,14,40,13,51,44,52,42,55,44,48,null,null,null,null,null,null,null,null,null,48,null,null,null,null,50,null,null,null,null,null],"type":"scatter3d","mode":"markers","inherit":true},"dc114dcb416.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"showscale":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"blood lead concentration (microgram/100ml)"},"yaxis":{"title":"Number of years living near smelter"},"zaxis":{"title":"FWT (taps per 10 seconds)"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[18,28,29,30,34,25,24,15,16,24,19,27,29,32,25,23,28,19,22,22,27,38,25,24,24,20,18,18,26,27,24,22,31,34,37,25,20,16,21,34,27,22,26,30,33,26,23,23,23,24,31,39,32,23,16,28,33,34,38,21,31,33,53,49,40,40,47,45,43,58,48,50,40,58,57,51,47,48,43,54,52,34,23],"y":[11,6,5,5,11,6,6,15,7,7,12,10,12,12,10,10,15,9,8,11,7,10,2,2,2,9,1,1,5,5,5,6,6,12,6,2,11,9,9,10,6,3,3,5,15,6,14,6,5,5,3,3,8,8,8,7,2,6,6,6,8,15,10,11,9,6,4,12,6,7,6,6,5,6,7,10,12,8,8,6,8,3,6],"z":[72,61,49,48,51,49,50,58,50,51,59,65,57,53,74,50,84,46,52,64,59,55,46,52,63,52,42,57,23,65,38,59,26,53,50,56,49,76,68,60,46,57,45,46,64,40,62,13,79,61,46,50,48,65,62,56,54,72,57,50,65,56,54,57,48,41,34,54,38,49,58,14,40,13,51,44,52,42,55,44,48,48,50],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"MAXFWT","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333332","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[50.4431143620533,52.4699548026394,54.4967952432254,56.5236356838115,58.5504761243976,60.5773165649837,62.6041570055698,64.6309974461559,66.6578378867419,68.684678327328],[48.4766608958821,50.5035013364682,52.5303417770542,54.5571822176403,56.5840226582264,58.6108630988125,60.6377035393986,62.6645439799847,64.6913844205708,66.7182248611568],[46.5102074297109,48.537047870297,50.563888310883,52.5907287514691,54.6175691920552,56.6444096326413,58.6712500732274,60.6980905138135,62.7249309543996,64.7517713949856],[44.5437539635397,46.5705944041258,48.5974348447118,50.6242752852979,52.651115725884,54.6779561664701,56.7047966070562,58.7316370476423,60.7584774882284,62.7853179288144],[42.5773004973685,44.6041409379546,46.6309813785406,48.6578218191267,50.6846622597128,52.7115027002989,54.738343140885,56.7651835814711,58.7920240220572,60.8188644626432],[40.6108470311973,42.6376874717834,44.6645279123695,46.6913683529555,48.7182087935416,50.7450492341277,52.7718896747138,54.7987301152999,56.825570555886,58.852410996472],[38.6443935650261,40.6712340056122,42.6980744461983,44.7249148867843,46.7517553273704,48.7785957679565,50.8054362085426,52.8322766491287,54.8591170897148,56.8859575303009],[36.6779400988549,38.704780539441,40.7316209800271,42.7584614206131,44.7853018611992,46.8121423017853,48.8389827423714,50.8658231829575,52.8926636235436,54.9195040641297],[34.7114866326837,36.7383270732698,38.7651675138559,40.7920079544419,42.818848395028,44.8456888356141,46.8725292762002,48.8993697167863,50.9262101573724,52.9530505979585],[32.7450331665125,34.7718736070986,36.7987140476847,38.8255544882708,40.8523949288568,42.8792353694429,44.906075810029,46.9329162506151,48.9597566912012,50.9865971317873]],"type":"surface","x":[15,19.7777777777778,24.5555555555556,29.3333333333333,34.1111111111111,38.8888888888889,43.6666666666667,48.4444444444444,53.2222222222222,58],"y":[1,2.55555555555556,4.11111111111111,5.66666666666667,7.22222222222222,8.77777777777778,10.3333333333333,11.8888888888889,13.4444444444444,15],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 3.1: Three-dimensional scatter plot and fitted regression model for the lead concentration example.
</p>
</div>
<p>The standard error of the parameter estimates are also part of the output. The 95% confidence intervals of the parameter esimates can be obtained as before, as illustrated in the next R code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m.LdTot)</code></pre></div>
<pre><code>##                  2.5 %    97.5 %
## (Intercept) 46.2734484 64.354341
## Ld73        -0.6364626 -0.186704
## Totyrs       0.5670955  2.038842</code></pre>
<p>Thus, for children with the same blood lead levels, we expect with a probability of 95% that their FWT increases on average with <span class="math inline">\(0.6\)</span> up to <span class="math inline">\(2\)</span> taps per 10 seconds for each extra year of living in the neighborhood of the smelter. This is of course an unexpected and weird conclusion, because one would expect that the accumulated exposure to the lead-poluled environment would result in a negative effect on the neuromuscular system, rather than the positive effect that we observe here. Since the data come from an observational study, we have here only established associations and no causations.</p>
<p>Many research questions can be translated in hypotheses such as (<span class="math inline">\(j=1,\ldots, p-1\)</span>) <span class="math display">\[
  H_0: \beta_j = 0 \text{ versus }
  H_1: \beta_j \neq 0
\]</span> (or one-sided alternatives).</p>
<p>The null hypothesis expresses that regressor <span class="math inline">\(j\)</span> has no linear effect on the mean outcome, given that all other regressors in the model remain constant, i.e. <span class="math display">\[
 \E{Y \mid x_1, \ldots, x_j+1,\ldots, x_{p-1}} = \E{Y \mid x_1, \ldots, x_j,\ldots, x_{p-1}}
\]</span> for all <span class="math inline">\(x_1, x_2, \ldots, x_{p-1}\)</span>.</p>
<p>If <span class="math inline">\(H_0:\beta_j=0\)</span> is true, and assuming that model <a href="#eq:Mod6">(3.2)</a> holds, we find <span class="math display">\[
  T= \frac{\hat\beta_j}{\hat\sigma_{\beta_j}} \HSim t_{n-p}.
\]</span> This result is sufficient for finding <span class="math inline">\(p\)</span>-values and critical values for the test, just like in the previous chapter.</p>
<p>First we test the hypotheses <span class="math display">\[
   H_0: \beta_1=0 \text{ versus } H_1:\beta_1\neq 0 .
 \]</span> This null hypothesis implies that for children living for the same time near the smelter, their blood lead levels have no linear effect on the average FWT.</p>
<p>Next we will test <span class="math display">\[
   H_0: \beta_2=0 \text{ versus } H_1:\beta_2\neq 0 .
 \]</span> Here the null hypothesis implies that for children with the same blood lead levels, the time living near the smelter has no linear effect on the average FWT. The <span class="math inline">\(p\)</span>-values can be read from the <em>summary</em> output.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m.LdTot)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73 + Totyrs, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.665  -4.731  -0.464   7.645  26.638 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  55.3139     4.5428  12.176  &lt; 2e-16 ***
## Ld73         -0.4116     0.1130  -3.642 0.000478 ***
## Totyrs        1.3030     0.3698   3.524 0.000707 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 11.42 on 80 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.2351, Adjusted R-squared:  0.216 
## F-statistic: 12.29 on 2 and 80 DF,  p-value: 2.21e-05</code></pre>
<p>From the R output we read that the <span class="math inline">\(p\)</span>-values for both hypothesis tests are smaller than the nominal significance level of <span class="math inline">\(\alpha=0.05\)</span>. Thus, at the 5% level of significance, we conclude that the blood level concentration has a negative effect on the mean FWT, while controlling for the number of years living near the lead smelter (<span class="math inline">\(p=0.0005\)</span>). We also conclude that the time living near the smelter has a significant positive effect on the mean FWT while controlling for blood lead levels (<span class="math inline">\(p=0.0007\)</span>).</p>
<p>We now illustrate how to make predictions based on the fitted multiple linear regression model. Suppose that we want to predict the FWT score of a child with a blood lead concentration of 35 microgram / 100ml and who lived 4 years near the smelter.</p>
<p>The numerical value of this prediction is calculated as <span class="math display">\[
   \hat{y} = \hat\beta_0 +\hat\beta_1 \text{Ld73} + \hat\beta_2\text{Totyrs} = 55.31-0.41\times 35 + 1.30\times 4 = 46.12.
 \]</span> The 95% PI is computed with the following R code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(m.LdTot,<span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">Ld73=</span><span class="dv">35</span>,<span class="dt">Totyrs=</span><span class="dv">4</span>), <span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>)</code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 46.12036 23.10707 69.13364</code></pre>
<p>So we conclude that with a chance of 95% a child with a blood lead level of 35 microgram / 100ml and who lived for 4 years near the smelter will have a FWT of somewhere between 23 and 69 taps per 10 seconds. Note that this is a rather wide interval, knowing that the range of FWT values in the dataset goes from 13 to 84 taps per 10 seconds.</p>
</div>
<div id="exercise-lead-concentration" class="section level2 unnumbered">
<h2>Exercise: Lead concentration</h2>
<p>Consider again the lead concentration dataset, but now we want to assess the effect of exposure on the mean FWT. Exposure is a binary variable (<em>Exposed</em>) coded as 1 when the subject was exposed, and 0 when not. Since we expect that also age has an effect on the FWT (these are children and with age their neuromuscular system still becomes better), we will correct the effect of exposure for age.</p>
<p>Fit a regression model with <em>Age</em> and <em>Exposed</em> in the model. Interpret the model parameter estimates. Assess the effect of exposure, corrected for age, and also assess the effect of age, corrected for exposure. Make an informative graph.</p>
<p><details> <summary markdown="span">Try to first make the exercise yourself. You can expand this page to see a solution. </summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lead<span class="op">$</span>Gender&lt;-<span class="kw">ifelse</span>(lead<span class="op">$</span>Sex<span class="op">==</span><span class="dv">1</span>,<span class="st">&quot;Boy&quot;</span>,<span class="st">&quot;Girl&quot;</span>)
lead<span class="op">$</span>Exposure&lt;-<span class="kw">ifelse</span>(lead<span class="op">$</span>Exposed<span class="op">==</span><span class="dv">1</span>,<span class="st">&quot;Exposed&quot;</span>,<span class="st">&quot;Not Exposed&quot;</span>)
m&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Age<span class="op">+</span>Exposure,<span class="dt">data=</span>lead)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Age + Exposure, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -33.505  -4.250   2.049   6.098  16.773 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          19.7742     3.8571   5.127 2.01e-06 ***
## Age                   2.6202     0.3464   7.564 5.76e-11 ***
## ExposureNot Exposed   7.5158     2.4776   3.033  0.00326 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.367 on 80 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.4852, Adjusted R-squared:  0.4723 
## F-statistic:  37.7 on 2 and 80 DF,  p-value: 2.923e-12</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m)</code></pre></div>
<pre><code>##                         2.5 %    97.5 %
## (Intercept)         12.098393 27.450083
## Age                  1.930831  3.309656
## ExposureNot Exposed  2.585143 12.446383</code></pre>
<p>Based on the this output we come to the following conclusions:</p>
<ul>
<li><p>It is estimated that for children of the same age, on average the FWT is <span class="math inline">\(7.5\)</span> taps per 10 seconds lower in the exposed group as compared to the non-exposed group. With a probability of 95%, this effect ranges between <span class="math inline">\(2.6\)</span> and <span class="math inline">\(12.4\)</span> taps per 10 seconds. This is a significant result at the 5% level of significance (two-sided p-value = <span class="math inline">\(0.0033\)</span>).</p></li>
<li><p>For children within the same exposure group, we estimate that the mean FWT score increases with <span class="math inline">\(2.6\)</span> taps per 10 seconds for each extra year of each. This comes with a 95% confidence interval ranging from <span class="math inline">\(1.9\)</span> to <span class="math inline">\(3.1\)</span> taps per 10 seconds. This is a significant result at the 5% level of significance (two-sided p-value <span class="math inline">\(&lt;0.001\)</span>).</p></li>
</ul>
<p>The next graph gives a graphical appreciation of the results. The graph illustrates the additivity of the model fit:</p>
<ul>
<li><p>within an exposure group, the effect of age on the mean FWT is the same</p></li>
<li><p>for any given age, the effect of exposure on the mean FWT is the same</p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lead<span class="op">$</span>fitted[indNA]&lt;-<span class="kw">predict</span>(m, <span class="dt">newdata =</span> lead[indNA,])

<span class="kw">ggplot</span>(lead[indNA,], <span class="kw">aes</span>(<span class="dt">x=</span>Age, <span class="dt">y=</span>MAXFWT, <span class="dt">color=</span>Exposure)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y=</span>fitted), <span class="dt">size=</span><span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>), <span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Age (years)&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;FWT (taps per 10 seconds)&quot;</span>) </code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-76-1.png" width="672" /></p>
<p></details></p>
</div>
<div id="the-non-additive-multiple-linear-regression-model" class="section level2">
<h2><span class="header-section-number">3.2</span> The Non-Additive Multiple Linear Regression Model</h2>
<div id="interaction" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Interaction</h3>
<p>We extend model <a href="#eq:Mod5">(3.1)</a> by added an <strong>interaction term</strong>. To introduce this new concept, we start with a model with only two regressors. Consider the model</p>
<span class="math display" id="eq:tmp28797869287">\[\begin{equation}
\tag{3.3}
 Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i1}x_{i2} + \eps_ i
\end{equation}\]</span>
<p>with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>. The only difference with model <a href="#eq:Mod5">(3.1)</a> for <span class="math inline">\(p-1=2\)</span> regressors is that the term <span class="math inline">\(\beta_3x_{i1}x_{i2}\)</span> is added. This extra term is the <strong>interaction term</strong>; it quantifies the <strong>interaction effect</strong> of the regressors <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> on the mean outcome (note: it is <em>not</em> the interaction between two regressors). In model <a href="#eq:tmp28797869287">(3.3)</a> we call the terms <span class="math inline">\(\beta_1x_{i1}\)</span> and <span class="math inline">\(\beta_2x_{i2}\)</span> the <strong>main effects</strong> of the regressors <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, respectively.</p>
<p>To understand the implications of the addition of the interaction term, we proceed as we studied the effect of <span class="math inline">\(x_1\)</span> in Section <a href="#S:AddMeervoudigModel">3.1</a>: we calculate the difference in expected outcome when <span class="math inline">\(x_1\)</span> increases with one unit when the regressor <span class="math inline">\(x_2\)</span> is kept constant:</p>
<span class="math display">\[\begin{eqnarray*}
  &amp; &amp; \E{Y \mid x_1+1,x_2} - \E{Y\mid x_1,x_2} \\
   &amp;=&amp; [\beta_0 + \beta_1 (x_{1}+1) + \beta_2 x_{2} + \beta_3 (x_{1}+1)x_{2}]- [\beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \beta_3 x_{1}x_{2}] \\
   &amp;=&amp; \beta_1 + \beta_3 x_{2}.
\end{eqnarray*}\]</span>
<p>This expression shows that the effect of <span class="math inline">\(x_1\)</span> depends on the value of <span class="math inline">\(x_2\)</span>! In the additive model, <span class="math inline">\(\beta_3=0\)</span> and the effect of <span class="math inline">\(x_1\)</span> is quantified by <span class="math inline">\(\beta_1\)</span> alone, and this effect is the same for all values of <span class="math inline">\(x_2\)</span>.</p>
<p>Similar, for the effect of <span class="math inline">\(x_2\)</span>, at a constant value of <span class="math inline">\(x_1\)</span>:</p>
<span class="math display">\[\begin{eqnarray*}
  &amp; &amp; \E{Y \mid x_1,x_2+1} - \E{Y\mid x_1,x_2} \\
   &amp;=&amp; [\beta_0 + \beta_1 x_{1} + \beta_2 (x_{2}+1) + \beta_3 x_{1}(x_{2}+1)]- [\beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \beta_3 x_{1}x_{2}] \\
   &amp;=&amp; \beta_2 + \beta_3 x_{1}.
\end{eqnarray*}\]</span>
<p>The presence of an interaction effect has a very important implication: the main effects, as quantified by <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>, have no longer a meaningful interpration. For example. the parameter <span class="math inline">\(\beta_1\)</span> only has an interpretation of the effect of <span class="math inline">\(x_1\)</span> if <span class="math inline">\(x_2=0\)</span>. As a consequence, testing null hypotheses <span class="math inline">\(H_0:\beta_1=0\)</span> and <span class="math inline">\(H_0:\beta_2=0\)</span> in model <a href="#eq:tmp28797869287">(3.3)</a> is meaningless and should not be done, unless there is no interaction. These arguments result in an example of <strong>hierarchical model building</strong>: in model <a href="#eq:tmp28797869287">(3.3)</a> we test the hypotheses in the following order:</p>
<ol style="list-style-type: decimal">
<li><p>test <span class="math inline">\(H_0:\beta_3=0\)</span> (test for absence of interaction effect)</p></li>
<li><p>test <span class="math inline">\(H_0:\beta_1=0\)</span> and/or <span class="math inline">\(H_0:\beta_2=0\)</span> (test of absence of main effects) only if there is no evidence or indication for the presence of an interaction effect.</p></li>
</ol>
</div>
<div id="parameter-estimators" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Parameter Estimators</h3>
<p>We can be very brief about estimating the parameters of model <a href="#eq:tmp28797869287">(3.3)</a>. Note that we can define a third regressor as <span class="math inline">\(x_{i3}=x_{i1}x_{i2}\)</span> (<span class="math inline">\(i=1,\ldots, n\)</span>), with which we can rewrite the model as</p>
<span class="math display">\[\begin{equation*}
 Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \eps_ i
\end{equation*}\]</span>
<p>with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>. This model is of the form of model <a href="#eq:Mod5">(3.1)</a> for <span class="math inline">\(p-1=3\)</span> regressors. Hence, all theory for the (additive) regression model still applies. It is only the interpration of the parameters that changes.</p>
</div>
<div id="example-lead-concentration-2" class="section level3 unnumbered">
<h3>Example (Lead concentration)</h3>
<p>Earlier we have analysed the Lead Concentration data with an additive model. We now know that this analysis only makes sense in the absence of an interacion effect. We therefore now analyse the data with the model <span class="math display">\[
   Y_i = \beta_0 + \beta_1 \text{Ld73}_i + \beta_2 \text{Totyrs}_i + \beta_3 \text{Ld73}_i\times \text{Totyrs}_i +\eps_i
 \]</span> with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>. We will test the hypotheses <span class="math display">\[
   H_0:\beta_3=0 \text{ versus } H_1:\beta_3\neq 0 
 \]</span> at the <span class="math inline">\(5\%\)</span> level of significance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.LdTotInt&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Ld73<span class="op">+</span>Totyrs<span class="op">+</span>Ld73<span class="op">:</span>Totyrs,<span class="dt">data=</span>lead)
<span class="kw">summary</span>(m.LdTotInt)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73 + Totyrs + Ld73:Totyrs, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -41.050  -5.041  -0.778   7.145  26.003 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 65.44259   10.11101   6.472 7.48e-09 ***
## Ld73        -0.76982    0.33894  -2.271   0.0259 *  
## Totyrs      -0.02079    1.23740  -0.017   0.9866    
## Ld73:Totyrs  0.04665    0.04162   1.121   0.2658    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 11.4 on 79 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.2471, Adjusted R-squared:  0.2185 
## F-statistic: 8.641 on 3 and 79 DF,  p-value: 4.998e-05</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m.LdTotInt)</code></pre></div>
<pre><code>##                   2.5 %      97.5 %
## (Intercept) 45.31712607 85.56806075
## Ld73        -1.44446176 -0.09517508
## Totyrs      -2.48376371  2.44218377
## Ld73:Totyrs -0.03619254  0.12949028</code></pre>
<p>From the R output we read on the line of <em>Ld73:Totyrs</em> the parameter estimate of the interaction effect: <span class="math inline">\(\hat\beta_3=0.047\)</span>. This tells us that we estimate that the effect of the time living near the smelter (<span class="math inline">\(\beta_2\)</span>), increases with an additional <span class="math inline">\(10\times 0.047\approx 0.5\)</span> taps per 10 seconds for each increase in blood lead concentration of 10 micrograms / 100ml (this sentence refers to the effect of <em>Totyrs</em> being <span class="math inline">\(\beta_2 + \beta_3\)</span><em>Ld73</em>). Thus, the larger the lead concentration in the blood, the stronger the positive effect of <em>Totyrs</em>! Also the 95% confidence interval of <span class="math inline">\(\beta_3\)</span> suggests only small interaction effects, mostly positive, but potentially also negative effects are plausible. The <span class="math inline">\(p\)</span>-value for testing <span class="math inline">\(H_0:\beta_3=0\)</span> versus <span class="math inline">\(H_1: \beta_3\neq 0\)</span> equals <span class="math inline">\(p=0.2658\)</span>. Thus, at the <span class="math inline">\(5\%\)</span> level of significance there is hardly any evidence of an interaction effect, and if it were present the estimate and its confidence interval indicate only a very small effect. Hence, it seems OK to remove the interaction term from the model, and continue with the analysis based on the additive model, as we have done before.</p>
<p>Some notes:</p>
<ul>
<li><p>when the interaction effect is not significant, we adapt the convention to first remove the interaction term from the model and refitting the model before looking at the main effects.</p></li>
<li><p>when looking at the estimates of the main effects in the model with interaction, we see e.g. the estimate <span class="math inline">\(\hat\beta_2\)</span> is now negative. It was positve in the additive model. However, this parameter is hard to interpret. Moreover, although the estimate is negative, the effect of <em>Totyrs</em> is still positive! This can be seen as follows. With the interaction term, the effect of <em>Totyrs</em> is given by <span class="math inline">\(\hat\beta_2 + \hat\beta_3\)</span><em>Ld73</em>, and the range of blood lead concentrations is 15 to 58 microgram / 100ml. The estimated effect of <em>Totyrs</em> thus ranges from <span class="math inline">\(0.7\)</span> to <span class="math inline">\(2.7\)</span> taps per 10 seconds per increase of <em>Totyrs</em> with one year.</p></li>
</ul>
<p>Figure <a href="#fig:Interactie">3.2</a> gives a three-dimensional illustration of model <a href="#eq:tmp28797869287">(3.3)</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">range</span>(lead<span class="op">$</span>Ld73)[<span class="dv">1</span>],<span class="kw">range</span>(lead<span class="op">$</span>Ld73)[<span class="dv">2</span>], <span class="dt">length.out =</span> <span class="dv">10</span>)
y &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">range</span>(lead<span class="op">$</span>Totyrs)[<span class="dv">1</span>],<span class="kw">range</span>(lead<span class="op">$</span>Totyrs)[<span class="dv">2</span>], <span class="dt">length.out =</span> <span class="dv">10</span>)
MAXFWT &lt;-<span class="st"> </span><span class="kw">outer</span>(x, y, <span class="cf">function</span>(a, b){<span class="kw">predict</span>(m.LdTotInt,<span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">Ld73=</span>a,<span class="dt">Totyrs=</span>b))})

axx&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;blood lead concentration (microgram/100ml)&quot;</span>)
axy&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;Number of years living near smelter&quot;</span>)
axz&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;FWT (taps per 10 seconds)&quot;</span>)

<span class="kw">plot_ly</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">add_markers</span>(<span class="dt">x=</span>lead<span class="op">$</span>Ld73,
              <span class="dt">y=</span>lead<span class="op">$</span>Totyrs,
              <span class="dt">z=</span>lead<span class="op">$</span>MAXFWT) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_surface</span>(<span class="dt">x=</span><span class="op">~</span>x,
              <span class="dt">y=</span><span class="op">~</span>y,
              <span class="dt">z=</span><span class="op">~</span>MAXFWT,
              <span class="dt">showscale=</span><span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">layout</span>(<span class="dt">scene =</span> <span class="kw">list</span>(<span class="dt">xaxis=</span>axx,<span class="dt">yaxis=</span>axy,<span class="dt">zaxis=</span>axz))</code></pre></div>
<pre><code>## Warning: Ignoring 19 observations</code></pre>
<div class="figure" style="text-align: center"><span id="fig:Interactie"></span>
<div id="htmlwidget-51f26eebcbbe032b96f4" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-51f26eebcbbe032b96f4">{"x":{"visdat":{"dc17f27114d":["function () ","plotlyVisDat"]},"cur_data":"dc17f27114d","attrs":{"dc17f27114d":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[18,28,29,30,34,25,24,15,16,24,19,27,29,32,25,23,28,19,22,22,27,38,31,25,24,24,20,18,18,26,27,24,22,31,34,37,25,20,16,21,34,27,22,26,30,33,26,23,23,23,24,31,39,32,23,16,28,33,34,38,21,31,33,53,49,40,40,47,45,43,58,48,50,40,58,57,51,47,48,43,54,52,34,27,26,20,27,28,31,28,37,34,34,23,25,25,23,43,49,47,45,45],"y":[11,6,5,5,11,6,6,15,7,7,12,10,12,12,10,10,15,9,8,11,7,10,5,2,2,2,9,1,1,5,5,5,6,6,12,6,2,11,9,9,10,6,3,3,5,15,6,14,6,5,5,3,3,8,8,8,7,2,6,6,6,8,15,10,11,9,6,4,12,6,7,6,6,5,6,7,10,12,8,8,6,8,3,4,3,4,4,4,4,4,4,3,4,4,5,4,6,4,4,4,3,3],"z":[72,61,49,48,51,49,50,58,50,51,59,65,57,53,74,50,84,46,52,64,59,55,null,46,52,63,52,42,57,23,65,38,59,26,53,50,56,49,76,68,60,46,57,45,46,64,40,62,13,79,61,46,50,48,65,62,56,54,72,57,50,65,56,54,57,48,41,34,54,38,49,58,14,40,13,51,44,52,42,55,44,48,null,null,null,null,null,null,null,null,null,48,null,null,null,null,50,null,null,null,null,null],"type":"scatter3d","mode":"markers","inherit":true},"dc17f27114d.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"showscale":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"blood lead concentration (microgram/100ml)"},"yaxis":{"title":"Number of years living near smelter"},"zaxis":{"title":"FWT (taps per 10 seconds)"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[18,28,29,30,34,25,24,15,16,24,19,27,29,32,25,23,28,19,22,22,27,38,25,24,24,20,18,18,26,27,24,22,31,34,37,25,20,16,21,34,27,22,26,30,33,26,23,23,23,24,31,39,32,23,16,28,33,34,38,21,31,33,53,49,40,40,47,45,43,58,48,50,40,58,57,51,47,48,43,54,52,34,23],"y":[11,6,5,5,11,6,6,15,7,7,12,10,12,12,10,10,15,9,8,11,7,10,2,2,2,9,1,1,5,5,5,6,6,12,6,2,11,9,9,10,6,3,3,5,15,6,14,6,5,5,3,3,8,8,8,7,2,6,6,6,8,15,10,11,9,6,4,12,6,7,6,6,5,6,7,10,12,8,8,6,8,3,6],"z":[72,61,49,48,51,49,50,58,50,51,59,65,57,53,74,50,84,46,52,64,59,55,46,52,63,52,42,57,23,65,38,59,26,53,50,56,49,76,68,60,46,57,45,46,64,40,62,13,79,61,46,50,48,65,62,56,54,72,57,50,65,56,54,57,48,41,34,54,38,49,58,14,40,13,51,44,52,42,55,44,48,48,50],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"MAXFWT","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666666","rgba(70,19,97,1)"],["0.0833333333333334","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[54.5742601434174,55.6303937856728,56.6865274279282,57.7426610701836,58.7987947124389,59.8549283546943,60.9110619969497,61.9671956392051,63.0233292814605,64.0794629237159],[51.1191167222872,52.5219493614186,53.92478200055,55.3276146396814,56.7304472788128,58.1332799179442,59.5361125570755,60.9389451962069,62.3417778353383,63.7446104744697],[47.663973301157,49.4135049371644,51.1630365731718,52.9125682091792,54.6620998451866,56.411631481194,58.1611631172013,59.9106947532087,61.6602263892161,63.4097580252235],[44.2088298800268,46.3050605129102,48.4012911457936,50.497521778677,52.5937524115604,54.6899830444437,56.7862136773271,58.8824443102105,60.9786749430939,63.0749055759773],[40.7536864588966,43.196616088656,45.6395457184154,48.0824753481748,50.5254049779342,52.9683346076936,55.411264237453,57.8541938672123,60.2971234969717,62.7400531267311],[37.2985430377664,40.0881716644018,42.8778002910372,45.6674289176726,48.457057544308,51.2466861709434,54.0363147975788,56.8259434242141,59.6155720508495,62.4052006774849],[33.8433996166362,36.9797272401476,40.116054863659,43.2523824871704,46.3887101106818,49.5250377341932,52.6613653577046,55.7976929812159,58.9340206047273,62.0703482282387],[30.388256195506,33.8712828158934,37.3543094362808,40.8373360566682,44.3203626770556,47.803389297443,51.2864159178304,54.7694425382178,58.2524691586052,61.7354957789925],[26.9331127743758,30.7628383916392,34.5925640089026,38.422289626166,42.2520152434294,46.0817408606928,49.9114664779562,53.7411920952196,57.5709177124829,61.4006433297463],[23.4779693532456,27.654393967385,31.8308185815244,36.0072431956638,40.1836678098032,44.3600924239426,48.536517038082,52.7129416522214,56.8893662663608,61.0657908805001]],"type":"surface","x":[15,19.7777777777778,24.5555555555556,29.3333333333333,34.1111111111111,38.8888888888889,43.6666666666667,48.4444444444444,53.2222222222222,58],"y":[1,2.55555555555556,4.11111111111111,5.66666666666667,7.22222222222222,8.77777777777778,10.3333333333333,11.8888888888889,13.4444444444444,15],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 3.2: Three-dimensional scatter plot and fitted regression model for the lead concentration example
</p>
</div>
</div>
</div>
<div id="example-blood-pressure" class="section level2 unnumbered">
<h2>Example (Blood Pressure)</h2>
<p>We consider again the Blood Pressure example and build a model with both dose and gender as regressors. We start with the model that also contains the interaction effect. The <em>gender</em> variable in <em>BloodPressure</em> dataset is coded as follows: 0=man, 1=women.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(bp.reduction<span class="op">~</span>dose<span class="op">*</span>gender,<span class="dt">data=</span>BloodPressure)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = bp.reduction ~ dose * gender, data = BloodPressure)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.3817 -2.3941  0.3182  2.2763  7.1774 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   2.2618     1.2552   1.802   0.0799 .  
## dose          1.4561     0.2151   6.769 6.62e-08 ***
## gender       -4.8307     1.8440  -2.620   0.0128 *  
## dose:gender   0.7390     0.3271   2.259   0.0300 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.856 on 36 degrees of freedom
## Multiple R-squared:  0.7816, Adjusted R-squared:  0.7634 
## F-statistic: 42.95 on 3 and 36 DF,  p-value: 5.55e-12</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m)</code></pre></div>
<pre><code>##                   2.5 %    97.5 %
## (Intercept) -0.28384132  4.807536
## dose         1.01978533  1.892366
## gender      -8.57039187 -1.090976
## dose:gender  0.07561727  1.402342</code></pre>
<p>This analysis shows that at the 5% level of significance we reject the null hypothesis of no-interaction (<span class="math inline">\(p=0.03\)</span>). The interaction effect size is estimated as <span class="math inline">\(0.74\)</span>. This means that the effect of dose is estimated to be <span class="math inline">\(0.74\)</span> mmHg / mg/day larger among women as compared to men. In combination with the estimated main effects, this means that we estimate the effect of the daily dose for woman to be <span class="math inline">\(1.46+0.74=2.2\)</span> mmHg / mg, whereas it is only <span class="math inline">\(1.46\)</span> mmHg / mg for men.</p>
<p>This is visualised in the following graph.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">BloodPressure<span class="op">$</span>fitted&lt;-<span class="kw">predict</span>(m)

<span class="kw">ggplot</span>(BloodPressure, <span class="kw">aes</span>(<span class="dt">x=</span>dose, <span class="dt">y=</span>bp.reduction, <span class="dt">color=</span><span class="kw">as.factor</span>(gender))) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y=</span>fitted), <span class="dt">size=</span><span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>), <span class="dt">axis.text =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Dose (mg / day)&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Diastolic blood pressure reduction (mmHg)&quot;</span>) </code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-79-1.png" width="672" /></p>
<p>Interaction is sometimes also referred to as <strong>effect modification</strong>. In this example, we could say that gender is an effect modifyer. It modifies the effect of the dose of the treatment.</p>
</div>
<div id="exercise-blood-pressure" class="section level2 unnumbered">
<h2>Exercise: Blood Pressure</h2>
<p>In the previous example, we have given dose effect estimates for men and women separately. What are the standard errors and 95% confidence intervals for these two effects?</p>
<p><details> <summary markdown="span">Try to first make the exercise yourself. You can expand this page to see a solution. </summary></p>
<p>For men, the answer is easy, because the effect for men is directly estimated by a single parameter. So the estimated effect is <span class="math inline">\(1.46\)</span> mmHg / mg/day, with SE<span class="math inline">\(=0.22\)</span> mmHg / mg/day. The 95% confidence interval can also be read from the output, it is <span class="math inline">\(1.2\)</span> to <span class="math inline">\(1.9\)</span> mmHg / mg/day.</p>
<p>For women the effect of dose is given by the sum of two parameter estimates. If we say that <span class="math inline">\(\beta_1\)</span> is the mean effect of dose and <span class="math inline">\(\beta_3\)</span> is the interaction effect, then the estimated dose effect for women is given by <span class="math inline">\(\hat\beta_1+\hat\beta_3\)</span>. This can be written as <span class="math display">\[
  \mb{c}^t \hat{\mb\beta}
\]</span> with <span class="math inline">\(\hat{\mb\beta}^t=(\hat\beta_0, \hat\beta_1, \hat\beta_2, \hat\beta_3)\)</span> and with <span class="math inline">\(\mb{c}^t=(0,1,0,1)\)</span>.</p>
<p>As part of Theorem <a href="#thm:BLUE">2.3</a> we have seen that <span class="math display">\[
  \var{\mb{c}^t\hat{\mb\beta}} = \mb{c}^t \mb\Sigma_\beta \mb{c}
\]</span> with <span class="math display">\[
  \mb\Sigma_\beta= \var{\hat{\mb\beta}} = (\mb{X}^t\mb{X})^{-1}\sigma^2.
\]</span> This covariance matrix can be estimated by replacing <span class="math inline">\(\sigma^2\)</span> with MSE.</p>
<p>In R, there we find the estimated covariance matrix as follows. I also show the standard errors of the parameter estimates so that you can verify that these indeed agree with the <em>summary</em> output.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m)<span class="op">$</span>cov<span class="op">*</span><span class="kw">summary</span>(m)<span class="op">$</span>sigma<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>##             (Intercept)        dose     gender dose:gender
## (Intercept)   1.5755583 -0.20404426 -1.5755583  0.20404426
## dose         -0.2040443  0.04627808  0.2040443 -0.04627808
## gender       -1.5755583  0.20404426  3.4001588 -0.45024729
## dose:gender   0.2040443 -0.04627808 -0.4502473  0.10698568</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">summary</span>(m)<span class="op">$</span>cov<span class="op">*</span><span class="kw">summary</span>(m)<span class="op">$</span>sigma<span class="op">^</span><span class="dv">2</span>)) <span class="co"># standard errors</span></code></pre></div>
<pre><code>## (Intercept)        dose      gender dose:gender 
##   1.2552124   0.2151234   1.8439520   0.3270867</code></pre>
<p>The estimated variance of <span class="math inline">\(\hat\beta_1+\hat\beta_3\)</span> can thus be computed in R as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Sigma.hat&lt;-<span class="kw">summary</span>(m)<span class="op">$</span>cov<span class="op">*</span><span class="kw">summary</span>(m)<span class="op">$</span>sigma<span class="op">^</span><span class="dv">2</span>
cvector&lt;-<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>),<span class="dt">ncol=</span><span class="dv">1</span>)
var.effect&lt;-<span class="kw">t</span>(cvector)<span class="op">%*%</span>Sigma.hat<span class="op">%*%</span>cvector
var.effect</code></pre></div>
<pre><code>##           [,1]
## [1,] 0.0607076</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(var.effect)</code></pre></div>
<pre><code>##           [,1]
## [1,] 0.2463891</code></pre>
<p>The standard error is thus given by SE = 0.25. With this SE we calculate the 95% confidence interval for the effect of dose for women as <span class="math display">\[
  2.2 \pm SE \times t_{36,0.975}.
\]</span> Thus, with a probability of 95% we expect the dose effect for woman to be in the interval from 1.7 to 2.7 mmHG / mg/day.</p>
<p></details></p>
</div>
<div id="example-lead-concentration-3" class="section level2 unnumbered">
<h2>Example (Lead concentration)</h2>
<p>We consider again the Lead Concentration example. This time we fit a model that also includes the age of the children. For simplicity we will assume that there are no interaction effects (see the next exercise in which you are asked to check this assumption).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.LdTotAge&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Ld73<span class="op">+</span>Totyrs<span class="op">+</span>Age, <span class="dt">data=</span>lead)
<span class="kw">summary</span>(m.LdTotAge)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73 + Totyrs + Age, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -33.956  -4.460   1.028   5.633  16.187 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 33.91688    5.18388   6.543 5.52e-09 ***
## Ld73        -0.26067    0.09734  -2.678  0.00901 ** 
## Totyrs      -0.06031    0.38183  -0.158  0.87489    
## Age          2.64503    0.43826   6.035 4.86e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.506 on 79 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.4765, Adjusted R-squared:  0.4566 
## F-statistic: 23.97 on 3 and 79 DF,  p-value: 3.961e-11</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m.LdTotAge)</code></pre></div>
<pre><code>##                  2.5 %      97.5 %
## (Intercept) 23.5986229 44.23512971
## Ld73        -0.4544291 -0.06691981
## Totyrs      -0.8203220  0.69969630
## Age          1.7726961  3.51737026</code></pre>
<p>Some conclusions:</p>
<ul>
<li><p>This output shows a large effect size of age: on average <span class="math inline">\(2.65\)</span> (SE <span class="math inline">\(0.44\)</span>) taps per 10 seconds per extra year of age, controlling for blood lead levels and for time living near the smelter. This effect is significant at the 5% level of significance (<span class="math inline">\(p&lt;0.001\)</span>).</p></li>
<li><p>The effect size of years living near the smelter is very small: an average reduction of <span class="math inline">\(0.8\)</span> (SE <span class="math inline">\(0.7\)</span>) taps per 10 seconds per extra year living near the smelter, controlling for age and blood lead levels. This effect is not significant (<span class="math inline">\(p=0.87\)</span>) at all; there seems to be no additional effect of this regressor when controlling for age and blood lead levels.</p></li>
<li><p>The effect of blood lead levels is negative and significant (<span class="math inline">\(p=0.009\)</span>), with an estimated effect size of <span class="math inline">\(-0.26\)</span>, i.e. with each increase of 10 microgram/100ml we exect the mean FWT to decrease with <span class="math inline">\(2.6\)</span> taps per 10 seconds, while controlling for age and years living in the neighborhood of the smelter.</p></li>
</ul>
<p>For convenience, I also show the results of our previous analysis with only <em>Ld73</em> and <em>Totyrs</em> as regressors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m.LdTot)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73 + Totyrs, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.665  -4.731  -0.464   7.645  26.638 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  55.3139     4.5428  12.176  &lt; 2e-16 ***
## Ld73         -0.4116     0.1130  -3.642 0.000478 ***
## Totyrs        1.3030     0.3698   3.524 0.000707 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 11.42 on 80 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.2351, Adjusted R-squared:  0.216 
## F-statistic: 12.29 on 2 and 80 DF,  p-value: 2.21e-05</code></pre>
<p>Comparing the two model fits, we see that the years living near the smelter was highly significant and with a large effect size when age was not in the model. After adding age to the model, the importance of <em>Totyrs</em> diminished. This may be explained by the positive correlation betwen <em>Totyrs</em> and <em>age</em>, which is equal to 0.66. This positive correlation makes sense: the older the child, the more chance there is that he/she lived for a longer time near the smelter. Thus age takes away some of the effect of <em>Totyrs</em>.</p>
</div>
<div id="exercise-lead-concentration-1" class="section level2 unnumbered">
<h2>Exercise: Lead concentration</h2>
<p>Referring to the previous example, you are asked to assess whether there are interaction effects present. Note that with three regressors in the model, you should consider the following interacation effects:</p>
<ul>
<li><p>of two regressors (there are three such interaction effects). These are referred to as <strong>two-way interactions</strong></p></li>
<li><p>of three regressors (there is only a single three-way interaction). This is known as a <strong>three-way interaction</strong></p></li>
</ul>
<p><details> <summary markdown="span">Try to first make the exercise yourself. You can expand this page to see a solution. </summary></p>
<p>Adhering to the <strong>hierarchical modelling approach</strong>, we should start with the model with all interaction terms. This is the following model (illustrating an alternative indexing system for the parameters): <span class="math display">\[
  Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_{1:2} x_{i1} x_{i2} + \beta_{1:3} x_{i1} x_{i3} +\beta_{2:3} x_{i2} x_{i3} + \beta_{1:2:3} x_{i1} x_{i2} x_{i3} + \eps_i
\]</span> with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Ld73<span class="op">*</span>Totyrs<span class="op">*</span>Age, <span class="dt">data=</span>lead)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73 * Totyrs * Age, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -34.042  -4.525   1.101   5.915  18.320 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)     28.808156  31.401213   0.917    0.362
## Ld73            -0.280911   1.038849  -0.270    0.788
## Totyrs           0.288832   4.632556   0.062    0.950
## Age              3.427570   2.899310   1.182    0.241
## Ld73:Totyrs      0.020753   0.153366   0.135    0.893
## Ld73:Age        -0.010893   0.099059  -0.110    0.913
## Totyrs:Age      -0.060295   0.380883  -0.158    0.875
## Ld73:Totyrs:Age -0.000541   0.012916  -0.042    0.967
## 
## Residual standard error: 9.715 on 75 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.4809, Adjusted R-squared:  0.4324 
## F-statistic: 9.924 on 7 and 75 DF,  p-value: 1.051e-08</code></pre>
<p>From this output we read a very small estimate for the three-way interaction: <span class="math inline">\(\hat\beta_{1:2:3}=-0.0005\)</span>. This comes with <span class="math inline">\(p=0.967\)</span>, clearly indicating a non-significant effect at the 5% level of significance. There seems to be no evidence at all for a three-way interaction effect.</p>
<p>Before we can look at the two-way interactions, we have to remove the three-way interaction from the model and refit the new model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>(Ld73<span class="op">+</span>Totyrs<span class="op">+</span>Age)<span class="op">^</span><span class="dv">2</span>, <span class="dt">data=</span>lead)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ (Ld73 + Totyrs + Age)^2, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -34.040  -4.544   1.112   5.952  18.304 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) 27.64602   14.60797   1.893   0.0622 .
## Ld73        -0.23998    0.35017  -0.685   0.4952  
## Totyrs       0.46335    2.01155   0.230   0.8184  
## Age          3.53353    1.40709   2.511   0.0142 *
## Ld73:Totyrs  0.01462    0.04553   0.321   0.7490  
## Ld73:Age    -0.01467    0.04068  -0.361   0.7193  
## Totyrs:Age  -0.07550    0.11437  -0.660   0.5111  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.651 on 76 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.4809, Adjusted R-squared:  0.4399 
## F-statistic: 11.73 on 6 and 76 DF,  p-value: 2.892e-09</code></pre>
<p>None of the two-way interaction effects appear to be important or significant at the 5% level; all <span class="math inline">\(p\)</span>-values are larger than <span class="math inline">\(0.5\)</span>. So the additive model we interpreted in the previous example was a correct model.</p>
<p></details></p>
</div>
<div id="the-anova-table" class="section level2">
<h2><span class="header-section-number">3.3</span> The ANOVA Table</h2>
<div id="sstot-ssr-and-sse" class="section level3">
<h3><span class="header-section-number">3.3.1</span> SSTot, SSR and SSE</h3>
<p>For the simple linear regression model we have discussed the decompositon of SStot into SSR and SSE in some detail. Here we extend this result to the multiple linear regression model.</p>
<p>The total sum of squares is defined as before (note that its defintion does not depend on a model), <span class="math display">\[
  \SSTot = \sum_{i=1}^n (Y_i - \bar{Y})^2.
\]</span> It is still a measure for the total variability in the observed sample outcomes. Also the residual sum of squares is defined as before: <span class="math display">\[
  \SSE = \sum_{i=1}^n (Y_i-\hat{Y}_i)^2.
\]</span></p>
<p>Consider now a multiple linear regression model with <span class="math inline">\(p-1\)</span> regressors. Then the following decompositon still holds <span class="math display">\[
  \SSTot = \SSR + \SSE ,
\]</span> with <span class="math display">\[
  \SSR = \sum_{i=1}^n (\hat{Y}_i-\bar{Y})^2.
\]</span> The sum of squares of the regression can still be interpreted as the variability in the outcomes that can be explained by the regression model.</p>
<p>For the degrees of freedom (df) and the mean sum of squares it holds that:</p>
<ul>
<li><p>SSTot has <span class="math inline">\(n-1\)</span> df and <span class="math inline">\(\SSTot/(n-1)\)</span> is an estimator of the marginal variance of <span class="math inline">\(Y\)</span> (i.e. variance of the marginal outcome distribution).</p></li>
<li><p>SSE has <span class="math inline">\(n-p\)</span> df and <span class="math inline">\(\MSE=\SSE/(n-p)\)</span> is an estimator of the residual variance <span class="math inline">\(\sigma^2\)</span>, i.e. the variance of <span class="math inline">\(Y\)</span> given the regressors.</p></li>
<li><p>SSR has <span class="math inline">\(p-1\)</span> df and <span class="math inline">\(\MSR=\SSR/(p-1)\)</span> is the mean sum of squares of the regression.</p></li>
</ul>
<p>A consequence of the decomposition of SSTot is that the *coefficient of determination** remains defined as before, i.e.</p>
<p><span class="math display">\[
  R^2 = 1-\frac{\SSE}{\SSTot} = \frac{\SSR}{\SSTot}
\]</span> is the proportion of the total outcome variability that can be explained by the regression model.</p>
</div>
</div>
<div id="multicollinearity" class="section level2">
<h2><span class="header-section-number">3.4</span> Multicollinearity</h2>
</div>
<div id="example-lead-concentration-4" class="section level2 unnumbered">
<h2>Example (Lead concentration)</h2>
<p>We consider again the lead concentration example, but suppose that we are this time only interested in assessing the effect of the blood lead concentrations on the mean FWT. We have blood lead levels measured in 1972 and in 1973. It could make sense to include both in the model. We will do so, but first we will look at simple linear regression models with only <em>Ld72</em> and only <em>Ld73</em> in the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Ld72&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Ld72,<span class="dt">data=</span>lead)
<span class="kw">summary</span>(m.Ld72)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld72, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -39.821  -5.500   0.160   7.868  30.802 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  58.4812     2.9357  19.921   &lt;2e-16 ***
## Ld72         -0.1887     0.0761  -2.479   0.0152 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.51 on 81 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.07053,    Adjusted R-squared:  0.05905 
## F-statistic: 6.146 on 1 and 81 DF,  p-value: 0.01524</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Ld73&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Ld73,<span class="dt">data=</span>lead)
<span class="kw">summary</span>(m.Ld73)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -42.308  -5.808   0.993   7.584  30.661 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  64.3660     4.0020  16.083   &lt;2e-16 ***
## Ld73         -0.3938     0.1206  -3.266   0.0016 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.2 on 81 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.1164, Adjusted R-squared:  0.1055 
## F-statistic: 10.67 on 1 and 81 DF,  p-value: 0.0016</code></pre>
<p>And now the model with both blood lead levels.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Ld7273&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Ld73<span class="op">+</span>Ld72,<span class="dt">data=</span>lead)
<span class="kw">summary</span>(m.Ld7273)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73 + Ld72, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -42.204  -5.713   1.214   7.558  30.624 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 64.29882    4.04956  15.878   &lt;2e-16 ***
## Ld73        -0.37270    0.18240  -2.043   0.0443 *  
## Ld72        -0.01741    0.11224  -0.155   0.8771    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.27 on 80 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.1166, Adjusted R-squared:  0.09455 
## F-statistic: 5.281 on 2 and 80 DF,  p-value: 0.007009</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#m&lt;-lm(MAXFWT~Ld73*Ld72,data=lead)</span>
<span class="co">#summary(m)</span></code></pre></div>
<p>These analyses show us (at the 5% level of significance):</p>
<ul>
<li><p>a significant negative effect of <em>Ld72</em> (<span class="math inline">\(p=0.0152\)</span>)</p></li>
<li><p>a significant negative effect of <em>Ld73</em> (<span class="math inline">\(p=0.0016\)</span>)</p></li>
<li><p>a significant negative effect of <em>Ld73</em> when controlling for <em>Ld72</em> (<span class="math inline">\(p=0.0443\)</span>). Note that this <span class="math inline">\(p\)</span>-value is larger than in the model with only <em>Ld73</em>.</p></li>
<li><p>a non-significant negative effect of <em>Ld72</em> when controlling for <em>Ld73</em> (<span class="math inline">\(p=0.8771\)</span>).</p></li>
</ul>
<p>The increase in <span class="math inline">\(p\)</span>-values in the model with both <em>Ld72</em> and <em>Ld73</em> can at least be partly attributed to a decrease of the estimated effect sizes as compared to their effect size estimated in the simple linear regression models. Intuitively, this can be understood as follows: we may expect that a child's blood lead level does not change very much from year to year. Thus, when we start with a model that includes <em>Ld72</em> as a regressor, and we add <em>Ld73</em> to it, then there is not much information in <em>Ld73</em> added to the regression model. In other words, when <em>Ld72</em> and <em>Ld73</em> share a lot of information about the mean FWD, then their total effect will be split into two smaller effect sizes as compared to their effects in their simple linear regression models. I used the term <em>sharing information</em>. This can, at least to some extent, be translated in correlation between the two regressors.</p>
<p>Another observation from the outputs of the three regression model fits, is that the standard errors of the effects are larger in the multiple regression model as compared to the corresponding standard errors in the simple regression models. Increased standard errors may also (partly) explain the increase in <span class="math inline">\(p\)</span>-values.</p>
<p>Let us look at the correlation between <em>Ld72</em> and <em>Ld73</em>. To make a fair assessment, we will limit the computation to those cases for which the outcome <em>MAXFWT</em> was observed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(lead[indNA,<span class="kw">c</span>(<span class="st">&quot;Ld72&quot;</span>,<span class="st">&quot;Ld73&quot;</span>)])</code></pre></div>
<pre><code>##           Ld72      Ld73
## Ld72 1.0000000 0.7467369
## Ld73 0.7467369 1.0000000</code></pre>
<p>The Pearson correlation is thus estimated as <span class="math inline">\(0.747\)</span>, which is clearly indicating a moderately strong positive correlation between the blood lead concentrations measured in 1972 and 1973.</p>
<p>Next we present a visual inspection.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggpairs</span>(lead[indNA,<span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">7</span>,<span class="dv">8</span>)],
        <span class="dt">progress=</span><span class="ot">FALSE</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-89-1.png" width="672" /></p>
<p>The problem that we just described is the problem of <strong>multicollinearity</strong>: increased standard errors of estimates of regression coefficients due to strong correlation between regressors in the sample data. This increase in standard error (increased imprecision of parameter estimates) causes the power of statistical tests to decrease (larger <span class="math inline">\(p\)</span>-values) and the confidence intervals to become wider.</p>
To better understand the issue, we look at the regression model with centered regressors (we have already demonstrated that centering has no effect on the parameter estimates, except for the intercept). Consider the model
<span class="math display">\[\begin{equation*}
 Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \eps_ i
\end{equation*}\]</span>
<p>with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>, in which the sample means of <span class="math inline">\(x_{i1}\)</span> and <span class="math inline">\(x_{i2}\)</span> are equal to zero.</p>
<p>The parameter estimates are given by <span class="math display">\[
  \hat{\mb\beta}=(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}
\]</span> with the <span class="math inline">\(n\times 3\)</span> design matrix <span class="math display">\[
 \mb{X}=\begin{pmatrix}
   1 &amp; x_{11} &amp; x_{12}  \\
   1 &amp; x_{21} &amp; x_{22}  \\
   \vdots &amp; \vdots &amp; \vdots \\
   1 &amp; x_{n1} &amp; x_{n2} \end{pmatrix}.
 \]</span> The matrix <span class="math inline">\(\mb{X}^t\mb{X}\)</span> is thus proportional to the correlation matrix of the columns of <span class="math inline">\(\mb{X}\)</span>. For example, the element on position <span class="math inline">\((2,3)\)</span> is <span class="math display">\[
   \sum_{i=1}^n x_{i1}x_{i2} = \sum_{i=1}^n (x_{i1}-\bar{x}_1)(x_{i2}-\bar{x}_2)  .
 \]</span></p>
<p>The variance of <span class="math inline">\(\hat{\mb\beta}\)</span> is given by <span class="math display">\[
   \var{\hat{\mb{\beta}}}=(\mb{X}^t\mb{X})^{-1}\sigma^2.
 \]</span> The matrix <span class="math inline">\(\mb{X}^t\mb{X}\)</span> thus plays an important role.</p>
<div id="illustrations-via-simulations" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Illustrations via Simulations</h3>
<p>The next chunck of R code illustrates via a simulation study the behaviour of estimators of the <span class="math inline">\(\beta\)</span>-parameters for a (random) design with <span class="math inline">\(n=50\)</span> observations with uncorrelated regressors (<span class="math inline">\(p-1=2\)</span>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">87561</span>)
N&lt;-N10000 
n&lt;-<span class="dv">50</span> <span class="co"># sample size</span>
beta0&lt;-<span class="dv">50</span>
beta1&lt;-<span class="dv">5</span>
beta2&lt;-<span class="dv">8</span>
sigma&lt;-<span class="dv">5</span> <span class="co"># residual standard deviation</span>
rho&lt;-<span class="dv">0</span> <span class="co"># correlation between the regressors</span>
var.beta&lt;-<span class="kw">matrix</span>(<span class="dv">0</span>,<span class="dt">nrow=</span><span class="dv">3</span>,<span class="dt">ncol=</span><span class="dv">3</span>)
beta.hat&lt;-<span class="kw">matrix</span>(<span class="dt">nrow=</span>N,<span class="dt">ncol=</span><span class="dv">3</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    x&lt;-<span class="dv">2</span><span class="op">*</span><span class="kw">mvrnorm</span>(<span class="dt">n=</span>n, <span class="dt">mu=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">Sigma=</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,rho,rho,<span class="dv">1</span>), <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span>T))
    x1&lt;-x[,<span class="dv">1</span>]
    x2&lt;-x[,<span class="dv">2</span>]
    y&lt;-beta0<span class="op">+</span>beta1<span class="op">*</span>x1<span class="op">+</span>beta2<span class="op">*</span>x2<span class="op">+</span><span class="kw">rnorm</span>(n,<span class="dt">sd=</span>sigma) 
    m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x1<span class="op">+</span>x2,<span class="dt">x=</span>T) 
    X&lt;-m<span class="op">$</span>x
    var.beta&lt;-var.beta<span class="op">+</span><span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X)<span class="op">*</span>sigma<span class="op">^</span><span class="dv">2</span>  
    beta.hat[i,]&lt;-<span class="kw">coef</span>(m)
}

<span class="kw">colMeans</span>(beta.hat) <span class="co"># approximation of expectation of par. estimators</span></code></pre></div>
<pre><code>## [1] 49.99716  5.00103  8.00034</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(var.beta<span class="op">/</span>N,<span class="dv">4</span>) <span class="co"># approximation of expection of the estimated covariance matrix of beta.hat</span></code></pre></div>
<pre><code>##             (Intercept)      x1      x2
## (Intercept)      0.5220  0.0010 -0.0004
## x1               0.0010  0.1364 -0.0002
## x2              -0.0004 -0.0002  0.1359</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(beta.hat) <span class="co"># approximation of covariance matrix of beta.hat</span></code></pre></div>
<pre><code>##              [,1]         [,2]         [,3]
## [1,]  0.511119882  0.004228924 -0.003136406
## [2,]  0.004228924  0.135666885 -0.001201206
## [3,] -0.003136406 -0.001201206  0.137048412</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(beta.hat) <span class="co"># correlation matrix of beta.hat</span></code></pre></div>
<pre><code>##             [,1]         [,2]         [,3]
## [1,]  1.00000000  0.016059485 -0.011850425
## [2,]  0.01605948  1.000000000 -0.008809345
## [3,] -0.01185043 -0.008809345  1.000000000</code></pre>
<p>This simulation study shows that</p>
<ul>
<li><p><span class="math inline">\(\var{\hat\beta_1}\approx\)</span> 0.1357</p></li>
<li><p><span class="math inline">\(\var{\hat\beta_2}\approx\)</span> 0.137</p></li>
<li><p><span class="math inline">\(\cor{\hat\beta_1,\hat\beta_2} \approx 0\)</span></p></li>
</ul>
<p>We repeat the simulation study a few times, with as only difference that now the correlation (<span class="math inline">\(\rho\)</span>) between the two regressors is set to <span class="math inline">\(\rho=0.5\)</span>, <span class="math inline">\(\rho=0.9\)</span> and <span class="math inline">\(\rho=0.99\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">87561</span>)
N&lt;-N10000 
n&lt;-<span class="dv">50</span> 
beta0&lt;-<span class="dv">50</span>
beta1&lt;-<span class="dv">5</span>
beta2&lt;-<span class="dv">8</span>
sigma&lt;-<span class="dv">5</span> <span class="co"># residuel standard deviation</span>
rho&lt;-<span class="fl">0.5</span>
var.beta&lt;-<span class="kw">matrix</span>(<span class="dv">0</span>,<span class="dt">nrow=</span><span class="dv">3</span>,<span class="dt">ncol=</span><span class="dv">3</span>)
beta.hat&lt;-<span class="kw">matrix</span>(<span class="dt">nrow=</span>N,<span class="dt">ncol=</span><span class="dv">3</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    x&lt;-<span class="dv">2</span><span class="op">*</span><span class="kw">mvrnorm</span>(<span class="dt">n=</span>n, <span class="dt">mu=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">Sigma=</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,rho,rho,<span class="dv">1</span>), <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span>T))
    x1&lt;-x[,<span class="dv">1</span>]
    x2&lt;-x[,<span class="dv">2</span>]
    y&lt;-beta0<span class="op">+</span>beta1<span class="op">*</span>x1<span class="op">+</span>beta2<span class="op">*</span>x2<span class="op">+</span><span class="kw">rnorm</span>(n,<span class="dt">sd=</span>sigma) 
    m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x1<span class="op">+</span>x2,<span class="dt">x=</span>T) 
    X&lt;-m<span class="op">$</span>x
    var.beta&lt;-var.beta<span class="op">+</span><span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X)<span class="op">*</span>sigma<span class="op">^</span><span class="dv">2</span>  
    beta.hat[i,]&lt;-<span class="kw">coef</span>(m)
}
<span class="kw">colMeans</span>(beta.hat) <span class="co"># approximation of expectation of par. estimators</span></code></pre></div>
<pre><code>## [1] 49.997155  5.001227  7.999166</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(var.beta<span class="op">/</span>N,<span class="dv">4</span>) <span class="co"># approximation of expection of the estimated covariance matrix of beta.hat</span></code></pre></div>
<pre><code>##             (Intercept)      x1      x2
## (Intercept)      0.5220  0.0007 -0.0012
## x1               0.0007  0.1815 -0.0911
## x2              -0.0012 -0.0911  0.1819</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(beta.hat) <span class="co"># approximation of covariance matrix of beta.hat</span></code></pre></div>
<pre><code>##              [,1]         [,2]         [,3]
## [1,]  0.511119882  0.002418119 -0.006039729
## [2,]  0.002418119  0.179962655 -0.089984081
## [3,] -0.006039729 -0.089984081  0.182736722</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(beta.hat) <span class="co"># correlation matrix of beta.hat</span></code></pre></div>
<pre><code>##              [,1]         [,2]        [,3]
## [1,]  1.000000000  0.007973062 -0.01976256
## [2,]  0.007973062  1.000000000 -0.49620550
## [3,] -0.019762559 -0.496205501  1.00000000</code></pre>
<p>For <span class="math inline">\(\rho=0.5\)</span> we find</p>
<ul>
<li><p><span class="math inline">\(\var{\hat\beta_1}\approx\)</span> 0.18</p></li>
<li><p><span class="math inline">\(\var{\hat\beta_2}\approx\)</span> 0.1827</p></li>
<li><p><span class="math inline">\(\cor{\hat\beta_1,\hat\beta_2} \approx\)</span> -0.496</p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">87561</span>)
N&lt;-N10000 
n&lt;-<span class="dv">50</span> 
beta0&lt;-<span class="dv">50</span>
beta1&lt;-<span class="dv">5</span>
beta2&lt;-<span class="dv">8</span>
sigma&lt;-<span class="dv">5</span> <span class="co"># residuel standard deviation</span>
rho&lt;-<span class="fl">0.9</span>
var.beta&lt;-<span class="kw">matrix</span>(<span class="dv">0</span>,<span class="dt">nrow=</span><span class="dv">3</span>,<span class="dt">ncol=</span><span class="dv">3</span>)
beta.hat&lt;-<span class="kw">matrix</span>(<span class="dt">nrow=</span>N,<span class="dt">ncol=</span><span class="dv">3</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    x&lt;-<span class="dv">2</span><span class="op">*</span><span class="kw">mvrnorm</span>(<span class="dt">n=</span>n, <span class="dt">mu=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">Sigma=</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,rho,rho,<span class="dv">1</span>), <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span>T))
    x1&lt;-x[,<span class="dv">1</span>]
    x2&lt;-x[,<span class="dv">2</span>]
    y&lt;-beta0<span class="op">+</span>beta1<span class="op">*</span>x1<span class="op">+</span>beta2<span class="op">*</span>x2<span class="op">+</span><span class="kw">rnorm</span>(n,<span class="dt">sd=</span>sigma) 
    m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x1<span class="op">+</span>x2,<span class="dt">x=</span>T) 
    X&lt;-m<span class="op">$</span>x
    var.beta&lt;-var.beta<span class="op">+</span><span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X)<span class="op">*</span>sigma<span class="op">^</span><span class="dv">2</span>  
    beta.hat[i,]&lt;-<span class="kw">coef</span>(m)
}
<span class="kw">colMeans</span>(beta.hat) <span class="co"># approximation of expectation of par. estimators</span></code></pre></div>
<pre><code>## [1] 49.997155  5.002479  7.997870</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(var.beta<span class="op">/</span>N,<span class="dv">4</span>) <span class="co"># approximation of expection of the estimated covariance matrix of beta.hat</span></code></pre></div>
<pre><code>##             (Intercept)      x1      x2
## (Intercept)      0.5220  0.0019 -0.0024
## x1               0.0019  0.7173 -0.6462
## x2              -0.0024 -0.6462  0.7181</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(beta.hat) <span class="co"># approximation of covariance matrix of beta.hat</span></code></pre></div>
<pre><code>##             [,1]        [,2]        [,3]
## [1,]  0.51111988  0.00784722 -0.01106511
## [2,]  0.00784722  0.71164404 -0.64226905
## [3,] -0.01106511 -0.64226905  0.71715555</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(beta.hat) <span class="co"># correlation matrix of beta.hat</span></code></pre></div>
<pre><code>##             [,1]        [,2]        [,3]
## [1,]  1.00000000  0.01301137 -0.01827627
## [2,]  0.01301137  1.00000000 -0.89903977
## [3,] -0.01827627 -0.89903977  1.00000000</code></pre>
<p>For <span class="math inline">\(\rho=0.9\)</span> we find</p>
<ul>
<li><p><span class="math inline">\(\var{\hat\beta_1}\approx\)</span> 0.7116</p></li>
<li><p><span class="math inline">\(\var{\hat\beta_2}\approx\)</span> 0.7172</p></li>
<li><p><span class="math inline">\(\cor{\hat\beta_1,\hat\beta_2} \approx\)</span> -0.899</p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">87561</span>)
N&lt;-N10000 
n&lt;-<span class="dv">50</span> 
beta0&lt;-<span class="dv">50</span>
beta1&lt;-<span class="dv">5</span>
beta2&lt;-<span class="dv">8</span>
sigma&lt;-<span class="dv">5</span> <span class="co"># residuel standard deviation</span>
rho&lt;-<span class="fl">0.99</span>
var.beta&lt;-<span class="kw">matrix</span>(<span class="dv">0</span>,<span class="dt">nrow=</span><span class="dv">3</span>,<span class="dt">ncol=</span><span class="dv">3</span>)
beta.hat&lt;-<span class="kw">matrix</span>(<span class="dt">nrow=</span>N,<span class="dt">ncol=</span><span class="dv">3</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {
    x&lt;-<span class="dv">2</span><span class="op">*</span><span class="kw">mvrnorm</span>(<span class="dt">n=</span>n, <span class="dt">mu=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">Sigma=</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,rho,rho,<span class="dv">1</span>), <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span>T))
    x1&lt;-x[,<span class="dv">1</span>]
    x2&lt;-x[,<span class="dv">2</span>]
    y&lt;-beta0<span class="op">+</span>beta1<span class="op">*</span>x1<span class="op">+</span>beta2<span class="op">*</span>x2<span class="op">+</span><span class="kw">rnorm</span>(n,<span class="dt">sd=</span>sigma) 
    m&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x1<span class="op">+</span>x2,<span class="dt">x=</span>T) 
    X&lt;-m<span class="op">$</span>x
    var.beta&lt;-var.beta<span class="op">+</span><span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X)<span class="op">*</span>sigma<span class="op">^</span><span class="dv">2</span>  
    beta.hat[i,]&lt;-<span class="kw">coef</span>(m)
}
<span class="kw">colMeans</span>(beta.hat) <span class="co"># approximation of expectation of par. estimators</span></code></pre></div>
<pre><code>## [1] 49.997155  5.007457  7.992884</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(var.beta<span class="op">/</span>N,<span class="dv">4</span>) <span class="co"># approximation of expection of the estimated covariance matrix of beta.hat</span></code></pre></div>
<pre><code>##             (Intercept)      x1      x2
## (Intercept)      0.5220  0.0066 -0.0071
## x1               0.0066  6.8522 -6.7852
## x2              -0.0071 -6.7852  6.8548</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(beta.hat) <span class="co"># approximation of covariance matrix of beta.hat</span></code></pre></div>
<pre><code>##             [,1]        [,2]        [,3]
## [1,]  0.51111988  0.02833087 -0.03147515
## [2,]  0.02833087  6.80926340 -6.74890998
## [3,] -0.03147515 -6.74890998  6.82629365</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(beta.hat) <span class="co"># correlation matrix of beta.hat</span></code></pre></div>
<pre><code>##             [,1]        [,2]        [,3]
## [1,]  1.00000000  0.01518618 -0.01685054
## [2,]  0.01518618  1.00000000 -0.98989945
## [3,] -0.01685054 -0.98989945  1.00000000</code></pre>
<p>For <span class="math inline">\(\rho=0.99\)</span> we find</p>
<ul>
<li><p><span class="math inline">\(\var{\hat\beta_1}\approx\)</span> 6.8093</p></li>
<li><p><span class="math inline">\(\var{\hat\beta_2}\approx\)</span> 6.8263</p></li>
<li><p><span class="math inline">\(\cor{\hat\beta_1,\hat\beta_2} \approx\)</span> -0.99</p></li>
</ul>
<p>Based on the simulation studies we conclude (for a fixed sample size):</p>
<ul>
<li><p>The smallest variance on parameter estimates is obtained for uncorrelated regressors</p></li>
<li><p>As the correlation between regressors increases, the variances of the parameter estimators increase</p></li>
<li><p>As the correlation between regressors increases, the correlation between the parameter estimators increases</p></li>
<li><p>For extremely large correlations between regressors (e.g. <span class="math inline">\(\rho=0.99\)</span>), we get extremely large variances of the parameter estimators.</p></li>
</ul>
<p>Figures <a href="#fig:MultCol1">3.3</a> and <a href="#fig:MultCol2">3.4</a> show three-dimensional scatter plots of simulated data, once with <span class="math inline">\(\rho=0\)</span> and once with <span class="math inline">\(\rho=0.99\)</span>. For the former, you can easily imagine a flat plane fitting through the cloud of points (fitted additive model), whereas for the latter it is not obvious where to imagine the plane. There appear to by many possible possitions of a plane going through the cloud of points, and it is not clear which plane would fit best. This <em>uncertainty</em> is an expression of the <em>imprecision</em> of the parameter estimates!</p>
<div class="figure"><span id="fig:MultCol1"></span>
<div id="htmlwidget-3db4e92c61b97d81d8ad" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-3db4e92c61b97d81d8ad">{"x":{"visdat":{"dc165c348a9":["function () ","plotlyVisDat"]},"cur_data":"dc165c348a9","attrs":{"dc165c348a9":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[2.3784652744587,0.975306324484654,-1.75932887084459,0.56120795081297,-1.06641441784728,0.515762365043503,3.19497495747883,-0.0878108473371359,4.04642500954806,-1.85011961378265,0.599690530596075,0.894149099029415,2.79238880356175,3.45837574589989,0.261066881570417,2.17179585624807,0.603943696726338,0.238607297490702,-0.171132534114863,-0.215393214593134,2.25487595274371,3.84778571350042,-2.30858746438483,-3.8535832999874,-1.72165658839257,-1.33160899041643,0.280877105734206,0.480295850948022,-1.91150964475358,0.560013947676214,-0.526408699498642,-0.670832805407232,-2.55174043715131,1.02821739535656,2.02373735638644,-0.535123158644984,-1.12619381381626,-2.41102354949056,-0.435917654330441,1.36319456059915,1.05597031690358,-2.3084051170794,-1.6348764839262,1.38590843525492,-1.28551384203373,2.41196471174138,0.176488181312493,-0.669540511046102,-2.48723031007246,1.24369444780231],"y":[1.00125550602661,-0.211805916360858,2.58441855506921,1.55965821947807,2.06433400087722,2.12646237246761,0.0956248386996567,-1.53709013097185,-3.513577146329,2.64095637157618,-2.51077636343789,-1.33714688323111,-0.855988778030817,0.96924687445716,-1.27479351227566,-0.89830420208184,0.400991269512732,1.34683882098077,1.0049387600799,-1.07773519836634,1.22382192327357,-1.46564793821972,2.92240529155838,0.835711709966758,0.407240483890394,2.88490637906331,-1.43466602440795,0.0894101480203718,-2.54977837337156,0.961856831424946,5.65686497040948,1.62624325727816,-0.178464212393518,3.30742666885604,-1.64627568395074,1.35911377230422,-3.3138758284828,-1.38060824124647,-0.759670294333982,0.614691460500778,1.89835632422177,-0.961910332544514,1.6756699006572,-0.72790626707619,1.19391889361652,1.57159288321511,-0.4074751561433,1.13733937868962,0.45941043536032,-2.27549494595085],"z":[70.2116805840652,64.3698789442331,64.2448370069937,66.3907691914361,64.0013297792731,68.4094607059508,64.3355860944441,29.6129487212036,50.1593392518925,66.0518921304644,32.6476453113268,39.5944560247736,55.5027225050266,79.4537885615799,37.6782299735153,49.326333884117,54.1455706250611,56.3748596085635,58.5385522211463,33.9802167447203,69.7250141967217,53.6944099528347,70.7519939508014,33.5364879454405,40.4517541262869,62.2775312778761,42.7651083511825,44.6638314930381,25.068414788667,62.6473335512159,86.032722400514,50.5736756524326,36.673249115175,83.4921666376461,40.8898997083404,53.4586301669799,23.3981103965939,29.1602256098246,37.3074114437587,52.9325988805186,72.9435922593982,29.6897407289283,58.9699990586283,51.0777085172045,50.8156838978685,76.7760231762726,40.1263188993674,56.008971331872,41.6645650162946,34.706709331544],"type":"scatter3d","mode":"markers","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"x1"},"yaxis":{"title":"x2"},"zaxis":{"title":"y"}},"hovermode":"closest","showlegend":false},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[2.3784652744587,0.975306324484654,-1.75932887084459,0.56120795081297,-1.06641441784728,0.515762365043503,3.19497495747883,-0.0878108473371359,4.04642500954806,-1.85011961378265,0.599690530596075,0.894149099029415,2.79238880356175,3.45837574589989,0.261066881570417,2.17179585624807,0.603943696726338,0.238607297490702,-0.171132534114863,-0.215393214593134,2.25487595274371,3.84778571350042,-2.30858746438483,-3.8535832999874,-1.72165658839257,-1.33160899041643,0.280877105734206,0.480295850948022,-1.91150964475358,0.560013947676214,-0.526408699498642,-0.670832805407232,-2.55174043715131,1.02821739535656,2.02373735638644,-0.535123158644984,-1.12619381381626,-2.41102354949056,-0.435917654330441,1.36319456059915,1.05597031690358,-2.3084051170794,-1.6348764839262,1.38590843525492,-1.28551384203373,2.41196471174138,0.176488181312493,-0.669540511046102,-2.48723031007246,1.24369444780231],"y":[1.00125550602661,-0.211805916360858,2.58441855506921,1.55965821947807,2.06433400087722,2.12646237246761,0.0956248386996567,-1.53709013097185,-3.513577146329,2.64095637157618,-2.51077636343789,-1.33714688323111,-0.855988778030817,0.96924687445716,-1.27479351227566,-0.89830420208184,0.400991269512732,1.34683882098077,1.0049387600799,-1.07773519836634,1.22382192327357,-1.46564793821972,2.92240529155838,0.835711709966758,0.407240483890394,2.88490637906331,-1.43466602440795,0.0894101480203718,-2.54977837337156,0.961856831424946,5.65686497040948,1.62624325727816,-0.178464212393518,3.30742666885604,-1.64627568395074,1.35911377230422,-3.3138758284828,-1.38060824124647,-0.759670294333982,0.614691460500778,1.89835632422177,-0.961910332544514,1.6756699006572,-0.72790626707619,1.19391889361652,1.57159288321511,-0.4074751561433,1.13733937868962,0.45941043536032,-2.27549494595085],"z":[70.2116805840652,64.3698789442331,64.2448370069937,66.3907691914361,64.0013297792731,68.4094607059508,64.3355860944441,29.6129487212036,50.1593392518925,66.0518921304644,32.6476453113268,39.5944560247736,55.5027225050266,79.4537885615799,37.6782299735153,49.326333884117,54.1455706250611,56.3748596085635,58.5385522211463,33.9802167447203,69.7250141967217,53.6944099528347,70.7519939508014,33.5364879454405,40.4517541262869,62.2775312778761,42.7651083511825,44.6638314930381,25.068414788667,62.6473335512159,86.032722400514,50.5736756524326,36.673249115175,83.4921666376461,40.8898997083404,53.4586301669799,23.3981103965939,29.1602256098246,37.3074114437587,52.9325988805186,72.9435922593982,29.6897407289283,58.9699990586283,51.0777085172045,50.8156838978685,76.7760231762726,40.1263188993674,56.008971331872,41.6645650162946,34.706709331544],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 3.3: Three-dimensional scatter plot of simulated data with <span class="math inline">\(\rho=0\)</span> correlation between the the regressors.
</p>
</div>
<div class="figure"><span id="fig:MultCol2"></span>
<div id="htmlwidget-c1e06c2a8e34040386c5" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-c1e06c2a8e34040386c5">{"x":{"visdat":{"dc165f66b87":["function () ","plotlyVisDat"]},"cur_data":"dc165f66b87","attrs":{"dc165f66b87":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[-3.09872215610821,-1.64129656238572,-1.82545967761885,1.04013045844297,2.35589256747558,-2.39078762394887,-1.78057454556649,0.55453683191285,3.72595916393503,-1.14620834488975,-0.321805999813256,-0.0670146877780076,-1.93445661148332,-0.311494715669063,-1.20334251985868,-0.345610562150516,-1.63038085799465,0.67158578941615,-3.31040491183251,-1.20415778061489,1.33085670508721,-0.233731606450318,-0.628268881239662,-1.5821323624261,0.866833094642294,2.5796219832329,-0.660324485637313,-0.674396919654675,-1.24127037514036,-2.5115528566554,3.06500538578505,2.30769676977944,-1.33976217595996,0.199096049184456,-1.86776891847088,-1.53183576843479,-1.31894690235573,-2.26175891694285,4.23714582883521,-0.151067445546016,-0.437748742927344,-0.0212279116568852,0.363064776851925,0.861552510626639,1.17455134658848,-2.78523044648607,3.65255366551743,4.394606072703,-1.08898731313567,-0.852906102921425],"y":[-3.42263787207453,-1.3452189076172,-2.18610897421636,0.998719228499312,2.28998614960951,-2.14358567270772,-1.89812303198702,0.478724603050485,3.81529332701646,-0.657500634153499,-0.292874516027754,0.227364822853622,-1.96737241504492,-1.35135794006987,-1.34703429360294,-0.431633537026034,-1.84656082039435,0.470456927460059,-3.86543900020167,-1.28744604176395,1.40227862841431,-0.00670567117326715,-0.499454099685769,-1.84018091220123,0.417053234234442,2.10570908381666,-0.231721201764698,-1.15461660474536,-1.20022081581562,-2.46766668691505,3.05954564180638,2.33389456171531,-1.97378977870937,0.00955425415240761,-1.70211175673326,-1.56875360774967,-1.20616019716984,-1.92634837600345,4.13990651066453,0.0627625341693089,-0.413770623553748,0.205469024041063,0.560552484140795,1.21954915142408,1.20463914574852,-2.76663986809218,3.55075844081584,4.65395963919026,-1.32295660818761,-0.622345371956771],"z":[10.5067119960926,32.077626248713,23.4146623581262,66.1356938986824,86.907220662261,21.5699213440957,20.7917176302361,57.3834309739898,102.342738524578,31.3856595581194,48.7997920044648,58.2248657403736,28.2357551125701,31.6078677258177,34.7362025397372,51.8348453867013,25.7765728508824,56.7914583452152,-0.259082891828117,26.5823391725765,77.6552215409888,53.2768317590813,40.3559416492888,29.0364453042167,50.0997216159591,86.7121339239354,48.7457185154846,39.8969407787656,28.7398355455262,19.1737007313698,91.0804249724727,80.3818411480242,24.0701472031116,47.0614462189598,35.6133009817934,30.9569946476442,24.9878660664894,17.9414340685221,99.6349527179906,52.179276992885,36.6122734318752,50.287528871461,55.3718849215839,62.0846224165965,64.3736954907903,18.8296384153987,98.1352368794012,106.752324929253,35.5940912679471,43.9812424912553],"type":"scatter3d","mode":"markers","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"x1"},"yaxis":{"title":"x2"},"zaxis":{"title":"y"}},"hovermode":"closest","showlegend":false},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[-3.09872215610821,-1.64129656238572,-1.82545967761885,1.04013045844297,2.35589256747558,-2.39078762394887,-1.78057454556649,0.55453683191285,3.72595916393503,-1.14620834488975,-0.321805999813256,-0.0670146877780076,-1.93445661148332,-0.311494715669063,-1.20334251985868,-0.345610562150516,-1.63038085799465,0.67158578941615,-3.31040491183251,-1.20415778061489,1.33085670508721,-0.233731606450318,-0.628268881239662,-1.5821323624261,0.866833094642294,2.5796219832329,-0.660324485637313,-0.674396919654675,-1.24127037514036,-2.5115528566554,3.06500538578505,2.30769676977944,-1.33976217595996,0.199096049184456,-1.86776891847088,-1.53183576843479,-1.31894690235573,-2.26175891694285,4.23714582883521,-0.151067445546016,-0.437748742927344,-0.0212279116568852,0.363064776851925,0.861552510626639,1.17455134658848,-2.78523044648607,3.65255366551743,4.394606072703,-1.08898731313567,-0.852906102921425],"y":[-3.42263787207453,-1.3452189076172,-2.18610897421636,0.998719228499312,2.28998614960951,-2.14358567270772,-1.89812303198702,0.478724603050485,3.81529332701646,-0.657500634153499,-0.292874516027754,0.227364822853622,-1.96737241504492,-1.35135794006987,-1.34703429360294,-0.431633537026034,-1.84656082039435,0.470456927460059,-3.86543900020167,-1.28744604176395,1.40227862841431,-0.00670567117326715,-0.499454099685769,-1.84018091220123,0.417053234234442,2.10570908381666,-0.231721201764698,-1.15461660474536,-1.20022081581562,-2.46766668691505,3.05954564180638,2.33389456171531,-1.97378977870937,0.00955425415240761,-1.70211175673326,-1.56875360774967,-1.20616019716984,-1.92634837600345,4.13990651066453,0.0627625341693089,-0.413770623553748,0.205469024041063,0.560552484140795,1.21954915142408,1.20463914574852,-2.76663986809218,3.55075844081584,4.65395963919026,-1.32295660818761,-0.622345371956771],"z":[10.5067119960926,32.077626248713,23.4146623581262,66.1356938986824,86.907220662261,21.5699213440957,20.7917176302361,57.3834309739898,102.342738524578,31.3856595581194,48.7997920044648,58.2248657403736,28.2357551125701,31.6078677258177,34.7362025397372,51.8348453867013,25.7765728508824,56.7914583452152,-0.259082891828117,26.5823391725765,77.6552215409888,53.2768317590813,40.3559416492888,29.0364453042167,50.0997216159591,86.7121339239354,48.7457185154846,39.8969407787656,28.7398355455262,19.1737007313698,91.0804249724727,80.3818411480242,24.0701472031116,47.0614462189598,35.6133009817934,30.9569946476442,24.9878660664894,17.9414340685221,99.6349527179906,52.179276992885,36.6122734318752,50.287528871461,55.3718849215839,62.0846224165965,64.3736954907903,18.8296384153987,98.1352368794012,106.752324929253,35.5940912679471,43.9812424912553],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 3.4: Three-dimensional scatter plot of simulated data with <span class="math inline">\(\rho=0.99\)</span> correlation between the the regressors.
</p>
</div>
</div>
</div>
<div id="excercise-repeatedly-fitting-model-with-rho0.99" class="section level2 unnumbered">
<h2>Excercise: repeatedly fitting model with <span class="math inline">\(\rho=0.99\)</span></h2>
<p>Repeat the simulation with <span class="math inline">\(\rho=0.99\)</span> a few times. Each time make the three dimensional scatter plot and add the fitted additive regression model. What do you see? What do you conclude?</p>
<p><details> <summary markdown="span">Try to first make the exercise yourself. You can expand this page to see a solution. </summary></p>
<p>I will use the same R code as used for the previous graph (<span class="math inline">\(\rho=0.99\)</span>). The code is repeated a few times, each time the simulated data and the fitted regression model are shown. Also the parameter estimates are listed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">85723</span>)
rho&lt;-<span class="fl">0.99</span>
x&lt;-<span class="dv">2</span><span class="op">*</span><span class="kw">mvrnorm</span>(<span class="dt">n=</span>n,<span class="dt">mu=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>),<span class="dt">Sigma=</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,rho,rho,<span class="dv">1</span>),<span class="dt">ncol=</span><span class="dv">2</span>,<span class="dt">byrow=</span>T))
x1&lt;-x[,<span class="dv">1</span>]
x2&lt;-x[,<span class="dv">2</span>]
y&lt;-beta0<span class="op">+</span>beta1<span class="op">*</span>x1<span class="op">+</span>beta2<span class="op">*</span>x2<span class="op">+</span><span class="kw">rnorm</span>(n,<span class="dt">sd=</span>sigma) 

m1&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x1<span class="op">+</span>x2)

xx &lt;-<span class="st"> </span><span class="kw">sort</span>(x1)
yy &lt;-<span class="st"> </span><span class="kw">sort</span>(x2)
fit &lt;-<span class="st"> </span><span class="kw">outer</span>(xx, yy, <span class="cf">function</span>(a, b){<span class="kw">predict</span>(m1,<span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">x1=</span>a,<span class="dt">x2=</span>b))})

axx&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;x1&quot;</span>)
axy&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;x2&quot;</span>)
axz&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;y&quot;</span>)

<span class="kw">plot_ly</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">add_markers</span>(<span class="dt">x=</span>x1,
              <span class="dt">y=</span>x2,
              <span class="dt">z=</span>y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_surface</span>(<span class="dt">x=</span><span class="op">~</span>xx,
              <span class="dt">y=</span><span class="op">~</span>yy,
              <span class="dt">z=</span><span class="op">~</span>fit,
              <span class="dt">showscale=</span><span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">layout</span>(<span class="dt">scene =</span> <span class="kw">list</span>(<span class="dt">xaxis=</span>axx,<span class="dt">yaxis=</span>axy,<span class="dt">zaxis=</span>axz))</code></pre></div>
<div id="htmlwidget-fd687a718737e8829d7d" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-fd687a718737e8829d7d">{"x":{"visdat":{"dc1319b4477":["function () ","plotlyVisDat"]},"cur_data":"dc1319b4477","attrs":{"dc1319b4477":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[-0.715798288657035,0.0398649395013548,-1.35812256167027,-1.26714446436738,2.14146334483501,3.14026008714997,3.50801069085019,-2.65793240843838,-0.521265480318938,1.39620662171399,-2.63563151534892,-3.05780283806507,2.32900014331444,3.45875612860532,2.33525296768733,-0.320074671287452,1.82219997656081,1.6730674847792,-3.39658880982085,4.08293608997422,1.3830334723996,-2.35996957865719,0.964431636964856,0.230673175344973,2.08777851377213,-1.4234935907596,1.51056474867124,-3.68720859601768,2.44360365602448,1.01420374229176,2.46747656291495,0.697040200103037,0.900883640006907,2.02101372710735,0.390255758037399,-2.61685584917697,1.22249818487302,0.241938410104874,-0.0172474728436934,3.14395686474125,2.84088466221408,1.35237709188441,-0.3535563585683,2.91124911937587,-0.429506140129029,-0.43367109640646,2.48833595026866,-0.290378867625921,-2.8428614617323,2.6696267028272],"y":[-0.639911586089848,0.373544422086501,-1.11246369957469,-1.24748150075266,2.28962092866078,3.1889472685881,3.45192812218369,-2.39140349930287,-0.591409303005398,1.42875233442496,-2.81682774425337,-3.73786254670458,2.09776381156926,3.46541694027223,2.27859123888164,-0.188208567834439,1.59698779150616,1.39404237974094,-3.5043406284154,3.94682849009638,1.25412452725694,-2.650818016905,0.519215321254824,0.422234904721515,2.42848505019461,-1.188304715176,1.60484513149822,-3.63747809054158,2.19206373267812,0.882830410974694,2.13883466717866,0.318676754866431,0.870607571621903,1.51408138304497,0.42384622735005,-2.85350805638705,0.936380133591274,0.141632271989487,-0.13974870250099,3.18951457536047,2.91088078833771,1.23225226883807,-0.218665988841759,3.11015681562905,-0.638906706343024,-0.661759302708913,2.85034841678827,-0.226797541738164,-2.87697855225446,2.77279699686677],"z":[44.5344966558363,53.077507431928,31.6885845924147,32.1652062382715,70.8075329050786,88.7222998589672,97.2515182562081,10.0909981974655,40.1282441028323,70.8105030001821,15.0608138130081,4.0134347043684,81.8576160683861,94.3884384375418,85.6795448882354,51.4505062413846,70.4642028083864,78.4081036782455,1.98828166369427,107.143853115057,67.190818529612,11.9541691168441,61.8541675815536,50.6040105979135,80.0581897973798,34.5663932223683,74.5104452696931,-6.29267326405866,80.0200628254843,53.7517000843191,77.5169061514328,56.9793870621456,57.3459677213818,75.3179045875806,57.353845250553,7.78709475535664,65.7418138762468,49.4735128072745,54.793011836345,89.5375916109722,74.8155146601138,66.6410794664439,39.8607905394907,86.4659185593933,52.3163544957849,48.9130675010038,88.5751495169416,44.2108781546426,8.70530020599637,82.6217698173004],"type":"scatter3d","mode":"markers","inherit":true},"dc1319b4477.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"showscale":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"x1"},"yaxis":{"title":"x2"},"zaxis":{"title":"y"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[-0.715798288657035,0.0398649395013548,-1.35812256167027,-1.26714446436738,2.14146334483501,3.14026008714997,3.50801069085019,-2.65793240843838,-0.521265480318938,1.39620662171399,-2.63563151534892,-3.05780283806507,2.32900014331444,3.45875612860532,2.33525296768733,-0.320074671287452,1.82219997656081,1.6730674847792,-3.39658880982085,4.08293608997422,1.3830334723996,-2.35996957865719,0.964431636964856,0.230673175344973,2.08777851377213,-1.4234935907596,1.51056474867124,-3.68720859601768,2.44360365602448,1.01420374229176,2.46747656291495,0.697040200103037,0.900883640006907,2.02101372710735,0.390255758037399,-2.61685584917697,1.22249818487302,0.241938410104874,-0.0172474728436934,3.14395686474125,2.84088466221408,1.35237709188441,-0.3535563585683,2.91124911937587,-0.429506140129029,-0.43367109640646,2.48833595026866,-0.290378867625921,-2.8428614617323,2.6696267028272],"y":[-0.639911586089848,0.373544422086501,-1.11246369957469,-1.24748150075266,2.28962092866078,3.1889472685881,3.45192812218369,-2.39140349930287,-0.591409303005398,1.42875233442496,-2.81682774425337,-3.73786254670458,2.09776381156926,3.46541694027223,2.27859123888164,-0.188208567834439,1.59698779150616,1.39404237974094,-3.5043406284154,3.94682849009638,1.25412452725694,-2.650818016905,0.519215321254824,0.422234904721515,2.42848505019461,-1.188304715176,1.60484513149822,-3.63747809054158,2.19206373267812,0.882830410974694,2.13883466717866,0.318676754866431,0.870607571621903,1.51408138304497,0.42384622735005,-2.85350805638705,0.936380133591274,0.141632271989487,-0.13974870250099,3.18951457536047,2.91088078833771,1.23225226883807,-0.218665988841759,3.11015681562905,-0.638906706343024,-0.661759302708913,2.85034841678827,-0.226797541738164,-2.87697855225446,2.77279699686677],"z":[44.5344966558363,53.077507431928,31.6885845924147,32.1652062382715,70.8075329050786,88.7222998589672,97.2515182562081,10.0909981974655,40.1282441028323,70.8105030001821,15.0608138130081,4.0134347043684,81.8576160683861,94.3884384375418,85.6795448882354,51.4505062413846,70.4642028083864,78.4081036782455,1.98828166369427,107.143853115057,67.190818529612,11.9541691168441,61.8541675815536,50.6040105979135,80.0581897973798,34.5663932223683,74.5104452696931,-6.29267326405866,80.0200628254843,53.7517000843191,77.5169061514328,56.9793870621456,57.3459677213818,75.3179045875806,57.353845250553,7.78709475535664,65.7418138762468,49.4735128072745,54.793011836345,89.5375916109722,74.8155146601138,66.6410794664439,39.8607905394907,86.4659185593933,52.3163544957849,48.9130675010038,88.5751495169416,44.2108781546426,8.70530020599637,82.6217698173004],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"fit","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[-1.26242711541375,-1.01092436283227,-0.677362380466459,0.894427666027449,0.953230537913513,1.04512922242885,1.46104922617546,2.11098515978845,4.97696204874748,5.12522329386781,5.31523502388439,6.44442773702972,6.49916490528605,6.50168252636007,6.62068230119063,7.53417892295236,7.55455167800208,7.63085955984621,7.75227068251345,8.45724127570472,8.9008077049596,9.03827290486231,9.16026181505118,9.16429881531859,9.40323610247493,10.2836126227164,10.3142356679476,10.4483988955312,11.1896755759158,11.244474231084,11.5950237721192,11.6819859321855,11.8957689362817,12.1034822689193,12.1231680122257,13.3581241954359,13.4610229272861,13.5943827820102,13.8111683961979,13.8388021300009,14.1867116583831,15.0493492184663,15.2436461875627,15.3953037115149,15.8945689494108,16.0919701861538,16.0933915139197,16.7508412014843,16.7846360239898,17.9907622962553],[1.94965030127885,2.20115305386032,2.53471503622614,4.10650508272004,4.16530795460611,4.25720663912144,4.67312664286805,5.32306257648104,8.18903946544008,8.3373007105604,8.52731244057698,9.65650515372231,9.71124232197865,9.71375994305266,9.83275971788323,10.746256339645,10.7666290946947,10.8429369765388,10.964348099206,11.6693186923973,12.1128851216522,12.2503503215549,12.3723392317438,12.3763762320112,12.6153135191675,13.495690039409,13.5263130846402,13.6604763122238,14.4017529926084,14.4565516477766,14.8071011888118,14.8940633488781,15.1078463529743,15.3155596856119,15.3352454289183,16.5702016121285,16.6731003439787,16.8064601987028,17.0232458128905,17.0508795466935,17.3987890750757,18.2614266351589,18.4557236042553,18.6073811282075,19.1066463661034,19.3040476028464,19.3054689306123,19.9629186181769,19.9967134406823,21.2028397129479],[5.69408485341215,5.94558760599362,6.27914958835943,7.85093963485334,7.90974250673941,8.00164119125474,8.41756119500135,9.06749712861434,11.9334740175734,12.0817352626937,12.2717469927103,13.4009397058556,13.4556768741119,13.458194495186,13.5771942700165,14.4906908917783,14.511063646828,14.5873715286721,14.7087826513393,15.4137532445306,15.8573196737855,15.9947848736882,16.1167737838771,16.1208107841445,16.3597480713008,17.2401245915423,17.2707476367735,17.4049108643571,18.1461875447417,18.2009861999099,18.5515357409451,18.6384979010114,18.8522809051076,19.0599942377452,19.0796799810516,20.3146361642618,20.417534896112,20.5508947508361,20.7676803650238,20.7953140988268,21.143223627209,22.0058611872922,22.2001581563886,22.3518156803408,22.8510809182367,23.0484821549797,23.0499034827456,23.7073531703101,23.7411479928156,24.9472742650812],[8.06972606444689,8.32122881702836,8.65479079939417,10.2265808458881,10.2853837177741,10.3772824022895,10.7932024060361,11.4431383396491,14.3091152286081,14.4573764737284,14.647388203745,15.7765809168904,15.8313180851467,15.8338357062207,15.9528354810513,16.866332102813,16.8867048578627,16.9630127397068,17.0844238623741,17.7893944555654,18.2329608848202,18.3704260847229,18.4924149949118,18.4964519951792,18.7353892823356,19.6157658025771,19.6463888478083,19.7805520753918,20.5218287557765,20.5766274109446,20.9271769519799,21.0141391120461,21.2279221161423,21.43563544878,21.4553211920863,22.6902773752965,22.7931761071467,22.9265359618708,23.1433215760585,23.1709553098616,23.5188648382437,24.381502398327,24.5757993674233,24.7274568913755,25.2267221292715,25.4241233660144,25.4255446937803,26.0829943813449,26.1167892038504,27.322915476116],[10.1136558448282,10.3651585974096,10.6987205797754,12.2705106262694,12.3293134981554,12.4212121826708,12.8371321864174,13.4870681200304,16.3530450089894,16.5013062541097,16.6913179841263,17.8205106972716,17.875247865528,17.877765486602,17.9967652614325,18.9102618831943,18.930634638244,19.0069425200881,19.1283536427554,19.8333242359466,20.2768906652015,20.4143558651042,20.5363447752931,20.5403817755605,20.7793190627168,21.6596955829584,21.6903186281895,21.8244818557731,22.5657585361577,22.6205571913259,22.9711067323611,23.0580688924274,23.2718518965236,23.4795652291612,23.4992509724676,24.7342071556778,24.837105887528,24.9704657422521,25.1872513564398,25.2148850902428,25.562794618625,26.4254321787082,26.6197291478046,26.7713866717568,27.2706519096527,27.4680531463957,27.4694744741616,28.1269241617262,28.1607189842317,29.3668452564972],[10.3601366368009,10.6116393893823,10.9452013717481,12.516991418242,12.5757942901281,12.6676929746434,13.0836129783901,13.733548912003,16.5995258009621,16.7477870460824,16.937798776099,18.0669914892443,18.1217286575007,18.1242462785747,18.2432460534052,19.156742675167,19.1771154302167,19.2534233120608,19.3748344347281,20.0798050279193,20.5233714571742,20.6608366570769,20.7828255672658,20.7868625675332,21.0257998546895,21.9061763749311,21.9367994201622,22.0709626477458,22.8122393281304,22.8670379832986,23.2175875243338,23.3045496844001,23.5183326884963,23.7260460211339,23.7457317644403,24.9806879476505,25.0835866795007,25.2169465342248,25.4337321484125,25.4613658822155,25.8092754105977,26.6719129706809,26.8662099397773,27.0178674637295,27.5171327016254,27.7145339383684,27.7159552661343,28.3734049536989,28.4071997762044,29.6133260484699],[10.5676548312208,10.8191575838023,11.1527195661681,12.724509612662,12.7833124845481,12.8752111690634,13.29113117281,13.941067106423,16.807043995382,16.9553052405024,17.1453169705189,18.2745096836643,18.3292468519206,18.3317644729946,18.4507642478252,19.3642608695869,19.3846336246366,19.4609415064808,19.582352629148,20.2873232223393,20.7308896515942,20.8683548514969,20.9903437616857,20.9943807619531,21.2333180491095,22.113694569351,22.1443176145822,22.2784808421657,23.0197575225504,23.0745561777185,23.4251057187538,23.51206787882,23.7258508829163,23.9335642155539,23.9532499588602,25.1882061420705,25.2911048739206,25.4244647286447,25.6412503428324,25.6688840766355,26.0167936050176,26.8794311651009,27.0737281341973,27.2253856581495,27.7246508960454,27.9220521327883,27.9234734605542,28.5809231481188,28.6147179706243,29.8208442428899],[13.4068923070758,13.6583950596572,13.991957042023,15.5637470885169,15.622549960403,15.7144486449183,16.130368648665,16.7803045822779,19.646281471237,19.7945427163573,19.9845544463739,21.1137471595192,21.1684843277756,21.1710019488496,21.2900017236801,22.2034983454419,22.2238711004916,22.3001789823357,22.4215901050029,23.1265606981942,23.5701271274491,23.7075923273518,23.8295812375407,23.8336182378081,24.0725555249644,24.9529320452059,24.9835550904371,25.1177183180207,25.8589949984053,25.9137936535735,26.2643431946087,26.351305354675,26.5650883587712,26.7728016914088,26.7924874347152,28.0274436179254,28.1303423497756,28.2637022044997,28.4804878186874,28.5081215524904,28.8560310808726,29.7186686409558,29.9129656100522,30.0646231340044,30.5638883719003,30.7612896086433,30.7627109364092,31.4201606239738,31.4539554464793,32.6600817187448],[23.757300348335,24.0088031009165,24.3423650832823,25.9141551297762,25.9729580016623,26.0648566861776,26.4807766899242,27.1307126235372,29.9966895124962,30.1449507576166,30.3349624876331,31.4641552007785,31.5188923690348,31.5214099901088,31.6404097649394,32.5539063867011,32.5742791417508,32.650587023595,32.7719981462622,33.4769687394535,33.9205351687084,34.0580003686111,34.1799892787999,34.1840262790673,34.4229635662237,35.3033400864652,35.3339631316964,35.4681263592799,36.2094030396646,36.2642016948327,36.614751235868,36.7017133959342,36.9154964000305,37.1232097326681,37.1428954759744,38.3778516591847,38.4807503910348,38.6141102457589,38.8308958599466,38.8585295937497,39.2064391221318,40.0690766822151,40.2633736513115,40.4150311752636,40.9142964131596,41.1116976499025,41.1131189776684,41.770568665233,41.8043634877385,43.0104897600041],[24.4798141488977,24.7313169014792,25.064878883845,26.6366689303389,26.695471802225,26.7873704867403,27.2032904904869,27.8532264240999,30.719203313059,30.8674645581793,31.0574762881959,32.1866690013412,32.2414061695976,32.2439237906716,32.3629235655021,33.2764201872639,33.2967929423136,33.3731008241577,33.494511946825,34.1994825400162,34.6430489692711,34.7805141691738,34.9025030793627,34.9065400796301,35.1454773667864,36.0258538870279,36.0564769322591,36.1906401598427,36.9319168402273,36.9867154953955,37.3372650364307,37.424227196497,37.6380102005932,37.8457235332308,37.8654092765372,39.1003654597474,39.2032641915976,39.3366240463217,39.5534096605094,39.5810433943124,39.9289529226946,40.7915904827778,40.9858874518742,41.1375449758264,41.6368102137223,41.8342114504653,41.8356327782312,42.4930824657958,42.5268772883013,43.7330035605668],[25.4853502673585,25.73685301994,26.0704150023058,27.6422050487997,27.7010079206858,27.7929066052011,28.2088266089477,28.8587625425607,31.7247394315198,31.8730006766401,32.0630124066567,33.192205119802,33.2469422880583,33.2494599091324,33.3684596839629,34.2819563057247,34.3023290607744,34.3786369426185,34.5000480652857,35.205018658477,35.6485850877319,35.7860502876346,35.9080391978235,35.9120761980909,36.1510134852472,37.0313900054887,37.0620130507199,37.1961762783034,37.9374529586881,37.9922516138563,38.3428011548915,38.4297633149578,38.643546319054,38.8512596516916,38.870945394998,40.1059015782082,40.2088003100584,40.3421601647825,40.5589457789702,40.5865795127732,40.9344890411554,41.7971266012386,41.991423570335,42.1430810942872,42.6423463321831,42.8397475689261,42.8411688966919,43.4986185842565,43.532413406762,44.7385396790276],[31.5791081045472,31.8306108571287,32.1641728394945,33.7359628859884,33.7947657578745,33.8866644423898,34.3025844461364,34.9525203797494,37.8184972687084,37.9667585138288,38.1567702438454,39.2859629569907,39.340700125247,39.343217746321,39.4622175211516,40.3757141429133,40.396086897963,40.4723947798072,40.5938059024744,41.2987764956657,41.7423429249206,41.8798081248233,42.0017970350122,42.0058340352796,42.2447713224359,43.1251478426774,43.1557708879086,43.2899341154921,44.0312107958768,44.0860094510449,44.4365589920802,44.5235211521465,44.7373041562427,44.9450174888803,44.9647032321867,46.1996594153969,46.302558147247,46.4359180019711,46.6527036161588,46.6803373499619,47.028246878344,47.8908844384273,48.0851814075237,48.2368389314759,48.7361041693718,48.9335054061147,48.9349267338806,49.5923764214452,49.6261712439507,50.8322975162163],[33.7291834612381,33.9806862138196,34.3142481961854,35.8860382426793,35.9448411145654,36.0367397990807,36.4526598028273,37.1025957364403,39.9685726253994,40.1168338705197,40.3068456005363,41.4360383136816,41.4907754819379,41.493293103012,41.6122928778425,42.5257894996043,42.546162254654,42.6224701364981,42.7438812591653,43.4488518523566,43.8924182816115,44.0298834815142,44.1518723917031,44.1559093919705,44.3948466791268,45.2752231993683,45.3058462445995,45.440009472183,46.1812861525677,46.2360848077358,46.5866343487711,46.6735965088374,46.8873795129336,47.0950928455712,47.1147785888776,48.3497347720878,48.452633503938,48.5859933586621,48.8027789728498,48.8304127066528,49.178322235035,50.0409597951182,50.2352567642146,50.3869142881668,50.8861795260627,51.0835807628057,51.0850020905715,51.7424517781361,51.7762466006416,52.9823728729072],[34.6973210614902,34.9488238140717,35.2823857964375,36.8541758429314,36.9129787148175,37.0048773993328,37.4207974030794,38.0707333366924,40.9367102256514,41.0849714707718,41.2749832007883,42.4041759139337,42.45891308219,42.461430703264,42.5804304780946,43.4939270998563,43.514299854906,43.5906077367502,43.7120188594174,44.4169894526087,44.8605558818636,44.9980210817663,45.1200099919551,45.1240469922225,45.3629842793789,46.2433607996204,46.2739838448516,46.4081470724351,47.1494237528198,47.2042224079879,47.5547719490232,47.6417341090894,47.8555171131857,48.0632304458233,48.0829161891296,49.3178723723399,49.42077110419,49.5541309589141,49.7709165731018,49.7985503069049,50.146459835287,51.0090973953703,51.2033943644667,51.3550518884188,51.8543171263148,52.0517183630577,52.0531396908236,52.7105893783882,52.7443842008937,53.9505104731593],[34.7433542727018,34.9948570252832,35.328419007649,36.9002090541429,36.959011926029,37.0509106105443,37.4668306142909,38.1167665479039,40.982743436863,41.1310046819833,41.3210164119999,42.4502091251452,42.5049462934016,42.5074639144756,42.6264636893061,43.5399603110679,43.5603330661176,43.6366409479617,43.7580520706289,44.4630226638202,44.9065890930751,45.0440542929778,45.1660432031667,45.1700802034341,45.4090174905904,46.2893940108319,46.3200170560631,46.4541802836467,47.1954569640313,47.2502556191995,47.6008051602347,47.687767320301,47.9015503243972,48.1092636570348,48.1289494003412,49.3639055835514,49.4668043154016,49.6001641701257,49.8169497843134,49.8445835181164,50.1924930464986,51.0551306065818,51.2494275756782,51.4010850996304,51.9003503375263,52.0977515742693,52.0991729020351,52.7566225895998,52.7904174121052,53.9965436843708],[35.582789816119,35.8342925687005,36.1678545510663,37.7396445975602,37.7984474694463,37.8903461539616,38.3062661577082,38.9562020913212,41.8221789802803,41.9704402254006,42.1604519554172,43.2896446685625,43.3443818368188,43.3468994578929,43.4658992327234,44.3793958544851,44.3997686095349,44.476076491379,44.5974876140462,45.3024582072375,45.7460246364924,45.8834898363951,46.005478746584,46.0095157468514,46.2484530340077,47.1288295542492,47.1594525994804,47.2936158270639,48.0348925074486,48.0896911626167,48.440240703652,48.5272028637183,48.7409858678145,48.9486992004521,48.9683849437585,50.2033411269687,50.3062398588189,50.4395997135429,50.6563853277307,50.6840190615337,51.0319285899159,51.8945661499991,52.0888631190955,52.2405206430477,52.7397858809436,52.9371871176865,52.9386084454524,53.596058133017,53.6298529555225,54.8359792277881],[35.9528464216756,36.2043491742571,36.5379111566229,38.1097012031168,38.1685040750029,38.2604027595182,38.6763227632648,39.3262586968778,42.1922355858368,42.3404968309572,42.5305085609737,43.6597012741191,43.7144384423754,43.7169560634494,43.83595583828,44.7494524600417,44.7698252150914,44.8461330969356,44.9675442196028,45.6725148127941,46.116081242049,46.2535464419517,46.3755353521405,46.3795723524079,46.6185096395643,47.4988861598058,47.529509205037,47.6636724326205,48.4049491130052,48.4597477681733,48.8102973092086,48.8972594692748,49.1110424733711,49.3187558060087,49.338441549315,50.5733977325253,50.6762964643754,50.8096563190995,51.0264419332872,51.0540756670903,51.4019851954724,52.2646227555557,52.4589197246521,52.6105772486042,53.1098424865002,53.3072437232431,53.308665051009,53.9661147385736,53.9999095610791,55.2060358333447],[36.2810595194829,36.5325622720643,36.8661242544301,38.437914300924,38.4967171728101,38.5886158573255,39.0045358610721,39.6544717946851,42.5204486836441,42.6687099287644,42.858721658781,43.9879143719263,44.0426515401827,44.0451691612567,44.1641689360872,45.077665557849,45.0980383128987,45.1743461947428,45.2957573174101,46.0007279106013,46.4442943398562,46.5817595397589,46.7037484499478,46.7077854502152,46.9467227373715,47.827099257613,47.8577223028442,47.9918855304278,48.7331622108124,48.7879608659806,49.1385104070158,49.2254725670821,49.4392555711783,49.6469689038159,49.6666546471223,50.9016108303325,51.0045095621827,51.1378694169068,51.3546550310945,51.3822887648975,51.7301982932797,52.5928358533629,52.7871328224593,52.9387903464115,53.4380555843074,53.6354568210504,53.6368781488163,54.2943278363809,54.3281226588864,55.5342489311519],[39.2998463558876,39.551349108469,39.8849110908348,41.4567011373288,41.5155040092148,41.6074026937302,42.0233226974768,42.6732586310898,45.5392355200488,45.6874967651691,45.8775084951857,47.006701208331,47.0614383765874,47.0639559976614,47.1829557724919,48.0964523942537,48.1168251493034,48.1931330311475,48.3145441538148,49.019514747006,49.4630811762609,49.6005463761636,49.7225352863525,49.7265722866199,49.9655095737762,50.8458860940178,50.876509139249,51.0106723668325,51.7519490472171,51.8067477023853,52.1572972434205,52.2442594034868,52.458042407583,52.6657557402207,52.685441483527,53.9203976667372,54.0232963985874,54.1566562533115,54.3734418674992,54.4010756013023,54.7489851296844,55.6116226897676,55.805919658864,55.9575771828162,56.4568424207121,56.6542436574551,56.655664985221,57.3131146727856,57.3469094952911,58.5530357675566],[39.9310817315206,40.1825844841021,40.5161464664679,42.0879365129618,42.1467393848479,42.2386380693632,42.6545580731099,43.3044940067228,46.1704708956819,46.3187321408022,46.5087438708188,47.6379365839641,47.6926737522205,47.6951913732945,47.814191148125,48.7276877698868,48.7480605249365,48.8243684067806,48.9457795294479,49.6507501226391,50.094316551894,50.2317817517967,50.3537706619856,50.357807662253,50.5967449494093,51.4771214696508,51.507744514882,51.6419077424656,52.3831844228502,52.4379830780184,52.7885326190536,52.8754947791199,53.0892777832161,53.2969911158537,53.3166768591601,54.5516330423703,54.6545317742205,54.7878916289446,55.0046772431323,55.0323109769353,55.3802205053175,56.2428580654007,56.4371550344971,56.5888125584493,57.0880777963452,57.2854790330882,57.2869003608541,57.9443500484187,57.9781448709242,59.1842711431897],[42.0399912221399,42.2914939747214,42.6250559570872,44.1968460035811,44.2556488754672,44.3475475599825,44.7634675637291,45.4134034973421,48.2793803863012,48.4276416314215,48.6176533614381,49.7468460745834,49.8015832428397,49.8041008639137,49.9231006387443,50.836597260506,50.8569700155558,50.9332778973999,51.0546890200671,51.7596596132584,52.2032260425133,52.340691242416,52.4626801526049,52.4667171528723,52.7056544400286,53.5860309602701,53.6166540055013,53.7508172330848,54.4920939134695,54.5468925686376,54.8974421096729,54.9844042697392,55.1981872738354,55.405900606473,55.4255863497794,56.6605425329896,56.7634412648397,56.8968011195638,57.1135867337516,57.1412204675546,57.4891299959367,58.35176755602,58.5460645251164,58.6977220490686,59.1969872869645,59.3943885237074,59.3958098514733,60.0532595390379,60.0870543615434,61.293180633809],[42.1645003157552,42.4160030683366,42.7495650507024,44.3213550971964,44.3801579690824,44.4720566535978,44.8879766573444,45.5379125909573,48.4038894799164,48.5521507250367,48.7421624550533,49.8713551681986,49.926092336455,49.928609957529,50.0476097323595,50.9611063541213,50.981479109171,51.0577869910151,51.1791981136824,51.8841687068736,52.3277351361285,52.4652003360312,52.5871892462201,52.5912262464875,52.8301635336438,53.7105400538854,53.7411630991165,53.8753263267001,54.6166030070847,54.6714016622529,55.0219512032881,55.1089133633544,55.3226963674506,55.5304097000882,55.5500954433946,56.7850516266048,56.887950358455,57.0213102131791,57.2380958273668,57.2657295611698,57.613639089552,58.4762766496352,58.6705736187316,58.8222311426838,59.3214963805797,59.5188976173227,59.5203189450886,60.1777686326532,60.2115634551587,61.4176897274242],[43.8037789417299,44.0552816943113,44.3888436766772,45.9606337231711,46.0194365950571,46.1113352795725,46.5272552833191,47.1771912169321,50.0431681058911,50.1914293510114,50.381441081028,51.5106337941733,51.5653709624297,51.5678885835037,51.6868883583342,52.600384980096,52.6207577351457,52.6970656169898,52.8184767396571,53.5234473328483,53.9670137621032,54.1044789620059,54.2264678721948,54.2305048724622,54.4694421596185,55.3498186798601,55.3804417250913,55.5146049526748,56.2558816330594,56.3106802882276,56.6612298292628,56.7481919893291,56.9619749934253,57.169688326063,57.1893740693693,58.4243302525795,58.5272289844297,58.6605888391538,58.8773744533415,58.9050081871446,59.2529177155267,60.1155552756099,60.3098522447063,60.4615097686585,60.9607750065544,61.1581762432974,61.1595975710633,61.8170472586279,61.8508420811334,63.0569683533989],[47.1945163414342,47.4460190940157,47.7795810763815,49.3513711228754,49.4101739947615,49.5020726792768,49.9179926830234,50.5679286166364,53.4339055055954,53.5821667507158,53.7721784807323,54.9013711938777,54.956108362134,54.958625983208,55.0776257580386,55.9911223798003,56.01149513485,56.0878030166942,56.2092141393614,56.9141847325527,57.3577511618076,57.4952163617103,57.6172052718991,57.6212422721665,57.8601795593229,58.7405560795644,58.7711791247956,58.9053423523791,59.6466190327638,59.7014176879319,60.0519672289672,60.1389293890334,60.3527123931297,60.5604257257673,60.5801114690736,61.8150676522839,61.917966384134,62.0513262388581,62.2681118530458,62.2957455868489,62.643655115231,63.5062926753143,63.7005896444107,63.8522471683628,64.3515124062588,64.5489136430017,64.5503349707676,65.2077846583322,65.2415794808377,66.4477057531033],[49.4474975249408,49.6990002775222,50.032562259888,51.6043523063819,51.663155178268,51.7550538627833,52.17097386653,52.8209098001429,55.686886689102,55.8351479342223,56.0251596642389,57.1543523773842,57.2090895456406,57.2116071667146,57.3306069415451,58.2441035633069,58.2644763183566,58.3407842002007,58.462195322868,59.1671659160592,59.6107323453141,59.7481975452168,59.8701864554057,59.8742234556731,60.1131607428294,60.9935372630709,61.0241603083021,61.1583235358857,61.8996002162703,61.9543988714385,62.3049484124737,62.39191057254,62.6056935766362,62.8134069092738,62.8330926525802,64.0680488357904,64.1709475676406,64.3043074223647,64.5210930365524,64.5487267703554,64.8966362987376,65.7592738588208,65.9535708279172,66.1052283518694,66.6044935897653,66.8018948265083,66.8033161542742,67.4607658418388,67.4945606643443,68.7006869366098],[50.1498622488843,50.4013650014658,50.7349269838316,52.3067170303255,52.3655199022116,52.4574185867269,52.8733385904735,53.5232745240865,56.3892514130456,56.5375126581659,56.7275243881825,57.8567171013278,57.9114542695841,57.9139718906581,58.0329716654887,58.9464682872504,58.9668410423002,59.0431489241443,59.1645600468115,59.8695306400028,60.3130970692577,60.4505622691604,60.5725511793493,60.5765881796167,60.815525466773,61.6959019870145,61.7265250322457,61.8606882598292,62.6019649402139,62.656763595382,63.0073131364173,63.0942752964836,63.3080583005798,63.5157716332174,63.5354573765238,64.770413559734,64.8733122915842,65.0066721463082,65.223457760496,65.251091494299,65.5990010226811,66.4616385827644,66.6559355518608,66.807593075813,67.3068583137089,67.5042595504518,67.5056808782177,68.1631305657823,68.1969253882878,69.4030516605534],[50.6999688248816,50.951471577463,51.2850335598288,52.8568236063227,52.9156264782088,53.0075251627241,53.4234451664707,54.0733811000837,56.9393579890428,57.0876192341631,57.2776309641797,58.406823677325,58.4615608455814,58.4640784666554,58.5830782414859,59.4965748632477,59.5169476182974,59.5932555001415,59.7146666228087,60.419637216,60.8632036452549,61.0006688451576,61.1226577553465,61.1266947556139,61.3656320427702,62.2460085630117,62.2766316082429,62.4107948358265,63.1520715162111,63.2068701713793,63.5574197124145,63.6443818724808,63.858164876577,64.0658782092146,64.085563952521,65.3205201357312,65.4234188675814,65.5567787223055,65.7735643364932,65.8011980702962,66.1491075986784,67.0117451587616,67.206042127858,67.3576996518102,67.8569648897061,68.0543661264491,68.055787454215,68.7132371417796,68.7470319642851,69.9531582365506],[53.0021447496298,53.2536475022113,53.5872094845771,55.158999531071,55.2178024029571,55.3097010874724,55.725621091219,56.375557024832,59.2415339137911,59.3897951589114,59.579806888928,60.7089996020733,60.7637367703297,60.7662543914037,60.8852541662342,61.798750787996,61.8191235430457,61.8954314248898,62.016842547557,62.7218131407483,63.1653795700032,63.3028447699059,63.4248336800948,63.4288706803622,63.6678079675185,64.54818448776,64.5788075329912,64.7129707605747,65.4542474409594,65.5090460961276,65.8595956371628,65.9465577972291,66.1603408013253,66.3680541339629,66.3877398772693,67.6226960604795,67.7255947923297,67.8589546470537,68.0757402612415,68.1033739950445,68.4512835234267,69.3139210835099,69.5082180526063,69.6598755765585,70.1591408144544,70.3565420511974,70.3579633789632,71.0154130665279,71.0492078890334,72.2553341612989],[54.4376323658566,54.6891351184381,55.0226971008039,56.5944871472978,56.6532900191838,56.7451887036992,57.1611087074458,57.8110446410588,60.6770215300178,60.8252827751381,61.0152945051547,62.1444872183001,62.1992243865564,62.2017420076304,62.320741782461,63.2342384042227,63.2546111592724,63.3309190411166,63.4523301637838,64.1573007569751,64.6008671862299,64.7383323861327,64.8603212963215,64.8643582965889,65.1032955837453,65.9836721039868,66.014295149218,66.1484583768015,66.8897350571862,66.9445337123543,67.2950832533896,67.3820454134558,67.595828417552,67.8035417501897,67.823227493496,69.0581836767062,69.1610824085564,69.2944422632805,69.5112278774682,69.5388616112713,69.8867711396534,70.7494086997367,70.943705668833,71.0953631927852,71.5946284306812,71.7920296674241,71.79345099519,72.4509006827546,72.4846955052601,73.6908217775257],[54.7764622467088,55.0279649992903,55.3615269816561,56.93331702815,56.9921199000361,57.0840185845514,57.499938588298,58.149874521911,61.01585141087,61.1641126559904,61.354124386007,62.4833170991523,62.5380542674086,62.5405718884826,62.6595716633132,63.5730682850749,63.5934410401246,63.6697489219688,63.791160044636,64.4961306378273,64.9396970670822,65.0771622669849,65.1991511771737,65.2031881774412,65.4421254645975,66.322501984839,66.3531250300702,66.4872882576537,67.2285649380384,67.2833635932065,67.6339131342418,67.720875294308,67.9346582984043,68.1423716310419,68.1620573743483,69.3970135575585,69.4999122894086,69.6332721441327,69.8500577583204,69.8776914921235,70.2256010205056,71.0882385805889,71.2825355496853,71.4341930736375,71.9334583115334,72.1308595482763,72.1322808760422,72.7897305636068,72.8235253861123,74.0296516583779],[54.9220585805758,55.1735613331573,55.5071233155231,57.078913362017,57.1377162339031,57.2296149184184,57.645534922165,58.295470855778,61.1614477447371,61.3097089898574,61.499720719874,62.6289134330193,62.6836506012756,62.6861682223496,62.8051679971802,63.7186646189419,63.7390373739916,63.8153452558358,63.936756378503,64.6417269716943,65.0852934009492,65.2227586008519,65.3447475110408,65.3487845113082,65.5877217984645,66.468098318706,66.4987213639372,66.6328845915207,67.3741612719054,67.4289599270735,67.7795094681088,67.866471628175,68.0802546322713,68.2879679649089,68.3076537082153,69.5426098914255,69.6455086232756,69.7788684779997,69.9956540921875,70.0232878259905,70.3711973543726,71.2338349144559,71.4281318835523,71.5797894075045,72.0790546454004,72.2764558821433,72.2778772099092,72.9353268974738,72.9691217199793,74.1752479922449],[56.1860026561725,56.4375054087539,56.7710673911198,58.3428574376137,58.4016603094997,58.4935589940151,58.9094789977617,59.5594149313747,62.4253918203337,62.573653065454,62.7636647954706,63.8928575086159,63.9475946768723,63.9501122979463,64.0691120727768,64.9826086945386,65.0029814495883,65.0792893314324,65.2007004540997,65.9056710472909,66.3492374765458,66.4867026764485,66.6086915866374,66.6127285869048,66.8516658740611,67.7320423943027,67.7626654395338,67.8968286671174,68.638105347502,68.6929040026702,69.0434535437055,69.1304157037717,69.3441987078679,69.5519120405056,69.5715977838119,70.8065539670221,70.9094526988723,71.0428125535964,71.2595981677841,71.2872319015872,71.6351414299693,72.4977789900525,72.6920759591489,72.8437334831011,73.3429987209971,73.54039995774,73.5418212855059,74.1992709730705,74.233065795576,75.4391920678415],[57.9820653937922,58.2335681463736,58.5671301287394,60.1389201752334,60.1977230471194,60.2896217316348,60.7055417353814,61.3554776689944,64.2214545579534,64.3697158030737,64.5597275330903,65.6889202462356,65.743657414492,65.746175035566,65.8651748103965,66.7786714321583,66.799044187208,66.8753520690521,66.9967631917194,67.7017337849106,68.1453002141655,68.2827654140682,68.4047543242571,68.4087913245245,68.6477286116808,69.5281051319224,69.5587281771535,69.6928914047371,70.4341680851217,70.4889667402899,70.8395162813251,70.9264784413914,71.1402614454876,71.3479747781252,71.3676605214316,72.6026167046418,72.705515436492,72.8388752912161,73.0556609054038,73.0832946392069,73.431204167589,74.2938417276722,74.4881386967686,74.6397962207208,75.1390614586167,75.3364626953597,75.3378840231256,75.9953337106902,76.0291285331957,77.2352548054612],[59.6303534034131,59.8818561559945,60.2154181383604,61.7872081848543,61.8460110567403,61.9379097412557,62.3538297450023,63.0037656786153,65.8697425675743,66.0180038126946,66.2080155427112,67.3372082558565,67.3919454241129,67.3944630451869,67.5134628200175,68.4269594417792,68.4473321968289,68.523640078673,68.6450512013403,69.3500217945315,69.7935882237864,69.9310534236891,70.053042333878,70.0570793341454,70.2960166213017,71.1763931415433,71.2070161867745,71.341179414358,72.0824560947426,72.1372547499108,72.4878042909461,72.5747664510123,72.7885494551085,72.9962627877462,73.0159485310525,74.2509047142627,74.3538034461129,74.487163300837,74.7039489150247,74.7315826488278,75.0794921772099,75.9421297372931,76.1364267063895,76.2880842303417,76.7873494682377,76.9847507049806,76.9861720327465,77.6436217203111,77.6774165428166,78.8835428150822],[61.8277439067078,62.0792466592893,62.4128086416551,63.984598688149,64.0434015600351,64.1353002445504,64.551220248297,65.20115618191,68.0671330708691,68.2153943159894,68.405406046006,69.5345987591513,69.5893359274076,69.5918535484816,69.7108533233122,70.6243499450739,70.6447227001237,70.7210305819678,70.842441704635,71.5474122978263,71.9909787270812,72.1284439269839,72.2504328371728,72.2544698374402,72.4934071245965,73.373783644838,73.4044066900692,73.5385699176527,74.2798465980374,74.3346452532055,74.6851947942408,74.7721569543071,74.9859399584033,75.1936532910409,75.2133390343473,76.4482952175575,76.5511939494076,76.6845538041317,76.9013394183195,76.9289731521225,77.2768826805046,78.1395202405879,78.3338172096843,78.4854747336365,78.9847399715324,79.1821412082753,79.1835625360412,79.8410122236058,79.8748070461113,81.0809333183769],[62.5656622235667,62.8171649761482,63.150726958514,64.7225170050079,64.781319876894,64.8732185614093,65.289138565156,65.9390744987689,68.805051387728,68.9533126328483,69.1433243628649,70.2725170760102,70.3272542442666,70.3297718653406,70.4487716401711,71.3622682619329,71.3826410169826,71.4589488988267,71.5803600214939,72.2853306146852,72.7288970439401,72.8663622438428,72.9883511540317,72.9923881542991,73.2313254414554,74.111701961697,74.1423250069281,74.2764882345117,75.0177649148963,75.0725635700645,75.4231131110997,75.510075271166,75.7238582752622,75.9315716078998,75.9512573512062,77.1862135344164,77.2891122662666,77.4224721209907,77.6392577351784,77.6668914689814,78.0148009973636,78.8774385574468,79.0717355265432,79.2233930504954,79.7226582883913,79.9200595251343,79.9214808529002,80.5789305404648,80.6127253629703,81.8188516352358],[63.1590142307808,63.4105169833623,63.7440789657281,65.315869012222,65.3746718841081,65.4665705686234,65.88249057237,66.532426505983,69.398403394942,69.5466646400624,69.7366763700789,70.8658690832243,70.9206062514806,70.9231238725546,71.0421236473852,71.9556202691469,71.9759930241966,72.0523009060408,72.173712028708,72.8786826218993,73.3222490511541,73.4597142510569,73.5817031612457,73.5857401615131,73.8246774486695,74.705053968911,74.7356770141422,74.8698402417257,75.6111169221104,75.6659155772785,76.0164651183138,76.10342727838,76.3172102824763,76.5249236151139,76.5446093584202,77.7795655416305,77.8824642734806,78.0158241282047,78.2326097423924,78.2602434761955,78.6081530045776,79.4707905646609,79.6650875337572,79.8167450577094,80.3160102956054,80.5134115323483,80.5148328601142,81.1722825476788,81.2060773701843,82.4122036424499],[65.2317661348326,65.4832688874141,65.8168308697799,67.3886209162738,67.4474237881599,67.5393224726752,67.9552424764218,68.6051784100348,71.4711552989939,71.6194165441142,71.8094282741308,72.9386209872761,72.9933581555324,72.9958757766065,73.114875551437,74.0283721731987,74.0487449282485,74.1250528100926,74.2464639327598,74.9514345259511,75.395000955206,75.5324661551087,75.6544550652976,75.658492065565,75.8974293527213,76.7778058729628,76.808428918194,76.9425921457776,77.6838688261622,77.7386674813304,78.0892170223656,78.1761791824319,78.3899621865281,78.5976755191657,78.6173612624721,79.8523174456823,79.9552161775325,80.0885760322566,80.3053616464443,80.3329953802473,80.6809049086295,81.5435424687127,81.7378394378091,81.8894969617613,82.3887621996572,82.5861634364002,82.5875847641661,83.2450344517306,83.2788292742361,84.4849555465017],[65.3008755241882,65.5523782767697,65.8859402591355,67.4577303056294,67.5165331775155,67.6084318620308,68.0243518657774,68.6742877993904,71.5402646883494,71.6885259334698,71.8785376634863,73.0077303766317,73.062467544888,73.064985165962,73.1839849407926,74.0974815625543,74.117854317604,74.1941621994482,74.3155733221154,75.0205439153067,75.4641103445615,75.6015755444643,75.7235644546531,75.7276014549205,75.9665387420769,76.8469152623184,76.8775383075496,77.0117015351331,77.7529782155178,77.8077768706859,78.1583264117212,78.2452885717874,78.4590715758837,78.6667849085213,78.6864706518276,79.9214268350379,80.024325566888,80.1576854216121,80.3744710357998,80.4021047696029,80.750014297985,81.6126518580683,81.8069488271646,81.9586063511168,82.4578715890128,82.6552728257557,82.6566941535216,83.3141438410862,83.3479386635917,84.5540649358573],[66.4984223383447,66.7499250909262,67.083487073292,68.6552771197859,68.714079991672,68.8059786761873,69.2218986799339,69.8718346135469,72.7378115025059,72.8860727476262,73.0760844776428,74.2052771907882,74.2600143590445,74.2625319801185,74.3815317549491,75.2950283767108,75.3154011317605,75.3917090136046,75.5131201362719,76.2180907294632,76.661657158718,76.7991223586207,76.9211112688096,76.925148269077,77.1640855562334,78.0444620764749,78.0750851217061,78.2092483492896,78.9505250296743,79.0053236848424,79.3558732258777,79.4428353859439,79.6566183900401,79.8643317226778,79.8840174659841,81.1189736491943,81.2218723810445,81.3552322357686,81.5720178499563,81.5996515837594,81.9475611121415,82.8101986722248,83.0044956413211,83.1561531652733,83.6554184031693,83.8528196399122,83.8542409676781,84.5116906552427,84.5454854777482,85.7516117500138],[66.7622778249142,67.0137805774957,67.3473425598615,68.9191326063554,68.9779354782415,69.0698341627568,69.4857541665034,70.1356901001164,73.0016669890755,73.1499282341958,73.3399399642124,74.4691326773577,74.523869845614,74.526387466688,74.6453872415186,75.5588838632803,75.5792566183301,75.6555645001742,75.7769756228414,76.4819462160327,76.9255126452876,77.0629778451903,77.1849667553792,77.1890037556466,77.4279410428029,78.3083175630444,78.3389406082756,78.4731038358591,79.2143805162438,79.2691791714119,79.6197287124472,79.7066908725135,79.9204738766097,80.1281872092473,80.1478729525537,81.3828291357639,81.4857278676141,81.6190877223382,81.8358733365259,81.8635070703289,82.2114165987111,83.0740541587943,83.2683511278907,83.4200086518429,83.9192738897388,84.1166751264818,84.1180964542476,84.7755461418122,84.8093409643177,86.0154672365833],[66.9928263636827,67.2443291162642,67.57789109863,69.1496811451239,69.20848401701,69.3003827015253,69.7163027052719,70.3662386388849,73.2322155278439,73.3804767729643,73.5704885029808,74.6996812161262,74.7544183843825,74.7569360054565,74.8759357802871,75.7894324020488,75.8098051570985,75.8861130389427,76.0075241616099,76.7124947548012,77.156061184056,77.2935263839588,77.4155152941476,77.419552294415,77.6584895815714,78.5388661018129,78.5694891470441,78.7036523746276,79.4449290550123,79.4997277101804,79.8502772512157,79.9372394112819,80.1510224153782,80.3587357480158,80.3784214913221,81.6133776745324,81.7162764063825,81.8496362611066,82.0664218752943,82.0940556090974,82.4419651374795,83.3046026975628,83.4988996666591,83.6505571906113,84.1498224285073,84.3472236652502,84.3486449930161,85.0060946805807,85.0398895030862,86.2460157753518],[68.9965437972716,69.2480465498531,69.5816085322189,71.1533985787128,71.2122014505989,71.3041001351142,71.7200201388608,72.3699560724738,75.2359329614329,75.3841942065532,75.5742059365698,76.7033986497151,76.7581358179714,76.7606534390454,76.879653213876,77.7931498356377,77.8135225906875,77.8898304725316,78.0112415951988,78.7162121883901,79.159778617645,79.2972438175477,79.4192327277366,79.423269728004,79.6622070151603,80.5425835354018,80.573206580633,80.7073698082165,81.4486464886012,81.5034451437693,81.8539946848046,81.9409568448709,82.1547398489671,82.3624531816047,82.3821389249111,83.6170951081213,83.7199938399715,83.8533536946955,84.0701393088833,84.0977730426863,84.4456825710684,85.3083201311517,85.5026171002481,85.6542746242003,86.1535398620962,86.3509410988391,86.352362426605,87.0098121141696,87.0436069366751,88.2497332089407],[70.8893737073553,71.1408764599368,71.4744384423026,73.0462284887965,73.1050313606826,73.1969300451979,73.6128500489445,74.2627859825575,77.1287628715165,77.2770241166369,77.4670358466534,78.5962285597988,78.6509657280551,78.6534833491291,78.7724831239597,79.6859797457214,79.7063525007711,79.7826603826152,79.9040715052825,80.6090420984738,81.0526085277286,81.1900737276314,81.3120626378202,81.3160996380876,81.555036925244,82.4354134454855,82.4660364907167,82.6001997183002,83.3414763986849,83.396275053853,83.7468245948883,83.8337867549545,84.0475697590507,84.2552830916884,84.2749688349947,85.509925018205,85.6128237500551,85.7461836047792,85.9629692189669,85.99060295277,86.3385124811521,87.2011500412354,87.3954470103317,87.5471045342839,88.0463697721799,88.2437710089228,88.2451923366887,88.9026420242533,88.9364368467588,90.1425631190244],[71.6670774100004,71.9185801625819,72.2521421449477,73.8239321914416,73.8827350633277,73.974633747843,74.3905537515896,75.0404896852026,77.9064665741616,78.054727819282,78.2447395492985,79.3739322624439,79.4286694307002,79.4311870517742,79.5501868266048,80.4636834483665,80.4840562034162,80.5603640852603,80.6817752079276,81.3867458011189,81.8303122303737,81.9677774302765,82.0897663404653,82.0938033407327,82.3327406278891,83.2131171481306,83.2437401933618,83.3779034209453,84.11918010133,84.1739787564981,84.5245282975334,84.6114904575996,84.8252734616958,85.0329867943335,85.0526725376398,86.2876287208501,86.3905274527002,86.5238873074243,86.740672921612,86.7683066554151,87.1162161837972,87.9788537438805,88.1731507129768,88.324808236929,88.824073474825,89.0214747115679,89.0228960393338,89.6803457268984,89.7141405494039,90.9202668216695],[74.1982228883881,74.4497256409696,74.7832876233354,76.3550776698293,76.4138805417154,76.5057792262307,76.9216992299773,77.5716351635903,80.4376120525493,80.5858732976697,80.7758850276862,81.9050777408316,81.9598149090879,81.9623325301619,82.0813323049925,82.9948289267542,83.0152016818039,83.0915095636481,83.2129206863153,83.9178912795066,84.3614577087614,84.4989229086642,84.620911818853,84.6249488191204,84.8638861062768,85.7442626265183,85.7748856717495,85.909048899333,86.6503255797177,86.7051242348858,87.0556737759211,87.1426359359873,87.3564189400836,87.5641322727212,87.5838180160275,88.8187741992378,88.9216729310879,89.055032785812,89.2718183999997,89.2994521338028,89.6473616621849,90.5099992222682,90.7042961913645,90.8559537153167,91.3552189532127,91.5526201899556,91.5540415177215,92.2114912052861,92.2452860277916,93.4514123000572],[74.2390815510788,74.4905843036603,74.8241462860261,76.39593633252,76.4547392044061,76.5466378889214,76.962557892668,77.612493826281,80.4784707152401,80.6267319603604,80.816743690377,81.9459364035223,82.0006735717786,82.0031911928527,82.1221909676832,83.0356875894449,83.0560603444947,83.1323682263388,83.253779349006,83.9587499421973,84.4023163714522,84.5397815713549,84.6617704815438,84.6658074818112,84.9047447689675,85.785121289209,85.8157443344402,85.9499075620238,86.6911842424084,86.7459828975766,87.0965324386118,87.1834945986781,87.3972776027743,87.6049909354119,87.6246766787183,88.8596328619285,88.9625315937787,89.0958914485028,89.3126770626905,89.3403107964935,89.6882203248757,90.5508578849589,90.7451548540553,90.8968123780075,91.3960776159034,91.5934788526464,91.5949001804123,92.2523498679768,92.2861446904823,93.4922709627479],[77.7184028302034,77.9699055827849,78.3034675651507,79.8752576116446,79.9340604835307,80.025959168046,80.4418791717926,81.0918151054056,83.9577919943646,84.106053239485,84.2960649695015,85.4252576826469,85.4799948509032,85.4825124719772,85.6015122468078,86.5150088685695,86.5353816236192,86.6116895054633,86.7331006281306,87.4380712213219,87.8816376505767,88.0191028504795,88.1410917606683,88.1451287609357,88.3840660480921,89.2644425683336,89.2950656135648,89.4292288411483,90.170505521533,90.2253041767011,90.5758537177364,90.6628158778026,90.8765988818988,91.0843122145365,91.1039979578428,92.3389541410531,92.4418528729032,92.5752127276273,92.791998341815,92.8196320756181,93.1675416040002,94.0301791640835,94.2244761331798,94.376133657132,94.875398895028,95.0728001317709,95.0742214595368,95.7316711471014,95.7654659696069,96.9715922418725],[78.2627892573156,78.514292009897,78.8478539922629,80.4196440387568,80.4784469106428,80.5703455951582,80.9862655989048,81.6362015325178,84.5021784214768,84.6504396665971,84.8404513966137,85.969644109759,86.0243812780154,86.0268988990894,86.14589867392,87.0593952956817,87.0797680507314,87.1560759325755,87.2774870552428,87.982457648434,88.4260240776889,88.5634892775916,88.6854781877805,88.6895151880479,88.9284524752043,89.8088289954458,89.839452040677,89.9736152682605,90.7148919486451,90.7696906038133,91.1202401448486,91.2072023049148,91.420985309011,91.6286986416487,91.648384384955,92.8833405681652,92.9862393000154,93.1195991547395,93.3363847689272,93.3640185027303,93.7119280311124,94.5745655911957,94.768862560292,94.9205200842442,95.4197853221402,95.6171865588831,95.618607886649,96.2760575742136,96.3098523967191,97.5159786689847],[84.6171566421933,84.8686593947748,85.2022213771406,86.7740114236345,86.8328142955206,86.9247129800359,87.3406329837825,87.9905689173955,90.8565458063546,91.0048070514749,91.1948187814915,92.3240114946368,92.3787486628931,92.3812662839671,92.5002660587977,93.4137626805594,93.4341354356092,93.5104433174533,93.6318544401205,94.3368250333118,94.7803914625667,94.9178566624694,95.0398455726583,95.0438825729257,95.282819860082,96.1631963803235,96.1938194255547,96.3279826531382,97.0692593335229,97.124057988691,97.4746075297263,97.5615696897925,97.7753526938888,97.9830660265264,98.0027517698328,99.237707953043,99.3406066848931,99.4739665396172,99.690752153805,99.718385887608,100.06629541599,100.928932976073,101.12322994517,101.274887469122,101.774152707018,101.971553943761,101.972975271527,102.630424959091,102.664219781597,103.870346053862]],"type":"surface","x":[-3.68720859601768,-3.39658880982085,-3.05780283806507,-2.8428614617323,-2.65793240843838,-2.63563151534892,-2.61685584917697,-2.35996957865719,-1.4234935907596,-1.35812256167027,-1.26714446436738,-0.715798288657035,-0.521265480318938,-0.43367109640646,-0.429506140129029,-0.3535563585683,-0.320074671287452,-0.290378867625921,-0.0172474728436934,0.0398649395013548,0.230673175344973,0.241938410104874,0.390255758037399,0.697040200103037,0.900883640006907,0.964431636964856,1.01420374229176,1.22249818487302,1.35237709188441,1.3830334723996,1.39620662171399,1.51056474867124,1.6730674847792,1.82219997656081,2.02101372710735,2.08777851377213,2.14146334483501,2.32900014331444,2.33525296768733,2.44360365602448,2.46747656291495,2.48833595026866,2.6696267028272,2.84088466221408,2.91124911937587,3.14026008714997,3.14395686474125,3.45875612860532,3.50801069085019,4.08293608997422],"y":[-3.73786254670458,-3.63747809054158,-3.5043406284154,-2.87697855225446,-2.85350805638705,-2.81682774425337,-2.650818016905,-2.39140349930287,-1.24748150075266,-1.188304715176,-1.11246369957469,-0.661759302708913,-0.639911586089848,-0.638906706343024,-0.591409303005398,-0.226797541738164,-0.218665988841759,-0.188208567834439,-0.13974870250099,0.141632271989487,0.318676754866431,0.373544422086501,0.422234904721515,0.42384622735005,0.519215321254824,0.870607571621903,0.882830410974694,0.936380133591274,1.23225226883807,1.25412452725694,1.39404237974094,1.42875233442496,1.51408138304497,1.59698779150616,1.60484513149822,2.09776381156926,2.13883466717866,2.19206373267812,2.27859123888164,2.28962092866078,2.42848505019461,2.77279699686677,2.85034841678827,2.91088078833771,3.11015681562905,3.1889472685881,3.18951457536047,3.45192812218369,3.46541694027223,3.94682849009638],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">873636</span>)
rho&lt;-<span class="fl">0.99</span>
x&lt;-<span class="dv">2</span><span class="op">*</span><span class="kw">mvrnorm</span>(<span class="dt">n=</span>n,<span class="dt">mu=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>),<span class="dt">Sigma=</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,rho,rho,<span class="dv">1</span>),<span class="dt">ncol=</span><span class="dv">2</span>,<span class="dt">byrow=</span>T))
x1&lt;-x[,<span class="dv">1</span>]
x2&lt;-x[,<span class="dv">2</span>]
y&lt;-beta0<span class="op">+</span>beta1<span class="op">*</span>x1<span class="op">+</span>beta2<span class="op">*</span>x2<span class="op">+</span><span class="kw">rnorm</span>(n,<span class="dt">sd=</span>sigma) 

m2&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x1<span class="op">+</span>x2)

xx &lt;-<span class="st"> </span><span class="kw">sort</span>(x1)
yy &lt;-<span class="st"> </span><span class="kw">sort</span>(x2)
fit &lt;-<span class="st"> </span><span class="kw">outer</span>(xx, yy, <span class="cf">function</span>(a, b){<span class="kw">predict</span>(m2,<span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">x1=</span>a,<span class="dt">x2=</span>b))})

axx&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;x1&quot;</span>)
axy&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;x2&quot;</span>)
axz&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;y&quot;</span>)

<span class="kw">plot_ly</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">add_markers</span>(<span class="dt">x=</span>x1,
              <span class="dt">y=</span>x2,
              <span class="dt">z=</span>y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_surface</span>(<span class="dt">x=</span><span class="op">~</span>xx,
              <span class="dt">y=</span><span class="op">~</span>yy,
              <span class="dt">z=</span><span class="op">~</span>fit,
              <span class="dt">showscale=</span><span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">layout</span>(<span class="dt">scene =</span> <span class="kw">list</span>(<span class="dt">xaxis=</span>axx,<span class="dt">yaxis=</span>axy,<span class="dt">zaxis=</span>axz))</code></pre></div>
<div id="htmlwidget-adef84c94d0b435b5813" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-adef84c94d0b435b5813">{"x":{"visdat":{"dc112aed904":["function () ","plotlyVisDat"]},"cur_data":"dc112aed904","attrs":{"dc112aed904":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[0.853118653957231,-0.853261352273738,0.353507080881254,2.02809044634073,-0.748461580289934,-2.73214618289166,-2.29853927796707,-1.96018442660419,-0.454722470925065,-2.04807598083181,1.19988499344418,-0.49994358955681,-0.423969802044517,2.02928467971549,-0.452452431926527,2.02601727895909,0.460500326950301,-0.77453192186067,0.483705121872882,1.39816972373539,-0.0438096739725239,0.252370993505637,3.5049565576428,-1.49238162732587,0.17418874404757,1.95725589276825,-0.465071952665595,-0.987853308859119,-2.54136756927678,0.285227511214592,1.23982109570761,-1.48889585298465,-1.78161724252153,-2.1021094254153,0.218257037107956,-2.96039892327731,2.44172844471548,0.786071244541534,1.15306569690969,-0.798582679493088,2.10635286058609,-1.03089875877432,0.489524155075263,2.8160310859391,-2.33420760703804,-1.52934930101089,0.546362685903954,3.17470241843918,-2.0956060206783,-2.09856896020911],"y":[0.798140282104294,-0.435095605089509,0.366037622814148,2.01529354164428,-1.03935429943388,-2.96739065955215,-2.44805785324636,-1.68349143086038,-0.130308984218711,-1.72262255180219,0.881903485485141,-0.411981610050177,-0.440167129716625,1.81946134566535,-0.523163933743912,2.28814783885948,0.492833910352589,-0.998117849967946,0.88382555281705,1.6692090586459,-0.105847397513396,0.133116785819328,3.02482963380164,-1.81203543162327,0.0462986786367592,1.88898926684997,-0.181207502094158,-0.897975078198987,-2.24036718573331,0.466556294258567,1.39459445907624,-1.10053939641347,-2.1444613286036,-2.13427727000019,0.334158947616182,-2.13680049051357,2.3642827276904,0.574871193088133,1.33457241765193,-1.02197331724591,1.30737116637587,-1.19182468573189,0.794407315568089,3.15373189365332,-1.80220208993125,-1.24340778810319,0.778994609569731,2.97470866881919,-2.28546018563748,-2.01530932119005],"z":[53.0221153406486,39.7664146403462,52.0479361805679,82.6506213025628,37.5203175730443,16.609106244855,18.6280306060541,22.9691851963071,48.4907714408854,34.2709656315768,63.5199068101962,41.6572047273896,46.0899435645803,76.8089435737867,33.4257632639751,72.0565372585594,51.7210942434003,42.4602927515407,68.7869298404291,69.2295575750801,47.6731452582634,59.3982983155231,94.8410897136315,29.0108667202668,56.4003166374144,80.1945484246352,37.5483388157212,35.378731125902,19.7254580191492,49.3768135281181,67.424362045329,34.9332921260732,29.292206066946,26.1975700701937,55.1548681216452,7.38181320423015,80.4576488867466,64.4246699710494,63.3554120046682,36.4779925952663,65.909197622917,35.2082299720879,57.7384756711734,82.3028309566393,20.3260212546623,23.9903681599536,61.509792140678,89.8358621842948,23.1216472897601,15.4869531296032],"type":"scatter3d","mode":"markers","inherit":true},"dc112aed904.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"showscale":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"x1"},"yaxis":{"title":"x2"},"zaxis":{"title":"y"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[0.853118653957231,-0.853261352273738,0.353507080881254,2.02809044634073,-0.748461580289934,-2.73214618289166,-2.29853927796707,-1.96018442660419,-0.454722470925065,-2.04807598083181,1.19988499344418,-0.49994358955681,-0.423969802044517,2.02928467971549,-0.452452431926527,2.02601727895909,0.460500326950301,-0.77453192186067,0.483705121872882,1.39816972373539,-0.0438096739725239,0.252370993505637,3.5049565576428,-1.49238162732587,0.17418874404757,1.95725589276825,-0.465071952665595,-0.987853308859119,-2.54136756927678,0.285227511214592,1.23982109570761,-1.48889585298465,-1.78161724252153,-2.1021094254153,0.218257037107956,-2.96039892327731,2.44172844471548,0.786071244541534,1.15306569690969,-0.798582679493088,2.10635286058609,-1.03089875877432,0.489524155075263,2.8160310859391,-2.33420760703804,-1.52934930101089,0.546362685903954,3.17470241843918,-2.0956060206783,-2.09856896020911],"y":[0.798140282104294,-0.435095605089509,0.366037622814148,2.01529354164428,-1.03935429943388,-2.96739065955215,-2.44805785324636,-1.68349143086038,-0.130308984218711,-1.72262255180219,0.881903485485141,-0.411981610050177,-0.440167129716625,1.81946134566535,-0.523163933743912,2.28814783885948,0.492833910352589,-0.998117849967946,0.88382555281705,1.6692090586459,-0.105847397513396,0.133116785819328,3.02482963380164,-1.81203543162327,0.0462986786367592,1.88898926684997,-0.181207502094158,-0.897975078198987,-2.24036718573331,0.466556294258567,1.39459445907624,-1.10053939641347,-2.1444613286036,-2.13427727000019,0.334158947616182,-2.13680049051357,2.3642827276904,0.574871193088133,1.33457241765193,-1.02197331724591,1.30737116637587,-1.19182468573189,0.794407315568089,3.15373189365332,-1.80220208993125,-1.24340778810319,0.778994609569731,2.97470866881919,-2.28546018563748,-2.01530932119005],"z":[53.0221153406486,39.7664146403462,52.0479361805679,82.6506213025628,37.5203175730443,16.609106244855,18.6280306060541,22.9691851963071,48.4907714408854,34.2709656315768,63.5199068101962,41.6572047273896,46.0899435645803,76.8089435737867,33.4257632639751,72.0565372585594,51.7210942434003,42.4602927515407,68.7869298404291,69.2295575750801,47.6731452582634,59.3982983155231,94.8410897136315,29.0108667202668,56.4003166374144,80.1945484246352,37.5483388157212,35.378731125902,19.7254580191492,49.3768135281181,67.424362045329,34.9332921260732,29.292206066946,26.1975700701937,55.1548681216452,7.38181320423015,80.4576488867466,64.4246699710494,63.3554120046682,36.4779925952663,65.909197622917,35.2082299720879,57.7384756711734,82.3028309566393,20.3260212546623,23.9903681599536,61.509792140678,89.8358621842948,23.1216472897601,15.4869531296032],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"fit","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[11.3222442786923,12.9087579894234,13.4054788317468,13.5432337775874,13.8362173045825,13.8596204566999,13.8673286620955,14.2307647470743,14.8517476869455,14.8817876367181,15.1248957685411,15.24443772533,16.5888525680381,16.7464341769032,17.0253022964082,17.2122171110933,17.2653144047985,17.3381906518849,17.6441175613983,18.7891309562629,19.0426785194774,19.0581715583487,19.1287826762954,19.8337762409392,19.9892665074596,20.0639943933605,20.5287865900314,20.7940078811042,21.4081730989624,21.5055595044955,21.8126347527654,21.8929104405561,22.1435267549324,22.7671049209893,22.8141893129032,22.825593180557,23.0814820229427,23.0873537609457,24.3812464534004,24.4643437612819,24.6477055487705,25.4866277622223,25.9456346081806,26.1580359795144,26.5438838621285,27.3774285146281,27.6100135613046,29.4748083773219,29.6279232916393,30.0217077784188],[13.5769884393806,15.1635021501117,15.6602229924351,15.7979779382757,16.0909614652708,16.1143646173882,16.1220728227838,16.4855089077626,17.1064918476338,17.1365317974064,17.3796399292294,17.4991818860183,18.8435967287264,19.0011783375915,19.2800464570965,19.4669612717816,19.5200585654868,19.5929348125732,19.8988617220866,21.0438751169512,21.2974226801657,21.312915719037,21.3835268369837,22.0885204016275,22.2440106681479,22.3187385540488,22.7835307507197,23.0487520417925,23.6629172596507,23.7603036651838,24.0673789134537,24.1476546012444,24.3982709156207,25.0218490816776,25.0689334735915,25.0803373412453,25.3362261836309,25.342097921634,26.6359906140887,26.7190879219702,26.9024497094588,27.7413719229106,28.2003787688689,28.4127801402027,28.7986280228168,29.6321726753164,29.8647577219929,31.7295525380102,31.8826674523276,32.2764519391071],[15.4615527355445,17.0480664462756,17.544787288599,17.6825422344397,17.9755257614348,17.9989289135521,18.0066371189477,18.3700732039265,18.9910561437977,19.0210960935704,19.2642042253934,19.3837461821823,20.7281610248904,20.8857426337555,21.1646107532605,21.3515255679455,21.4046228616508,21.4774991087371,21.7834260182506,22.9284394131152,23.1819869763297,23.1974800152009,23.2680911331477,23.9730846977914,24.1285749643118,24.2033028502128,24.6680950468837,24.9333163379565,25.5474815558147,25.6448679613478,25.9519432096176,26.0322188974084,26.2828352117847,26.9064133778415,26.9534977697555,26.9649016374093,27.2207904797949,27.226662217798,28.5205549102527,28.6036522181341,28.7870140056228,29.6259362190746,30.0849430650328,30.2973444363667,30.6831923189808,31.5167369714804,31.7493220181569,33.6141168341741,33.7672317484916,34.1610162352711],[17.5079365572547,19.0944502679857,19.5911711103091,19.7289260561498,20.0219095831449,20.0453127352623,20.0530209406579,20.4164570256367,21.0374399655079,21.0674799152805,21.3105880471035,21.4301300038924,22.7745448466005,22.9321264554656,23.2109945749706,23.3979093896557,23.4510066833609,23.5238829304473,23.8298098399607,24.9748232348253,25.2283707980398,25.2438638369111,25.3144749548578,26.0194685195016,26.174958786022,26.2496866719229,26.7144788685938,26.9797001596666,27.5938653775248,27.6912517830579,27.9983270313278,28.0786027191185,28.3292190334948,28.9527971995517,28.9998815914656,29.0112854591194,29.267174301505,29.2730460395081,30.5669387319628,30.6500360398442,30.8333978273329,31.6723200407847,32.131326886743,32.3437282580768,32.7295761406909,33.5631207931905,33.795705839867,35.6605006558843,35.8136155702017,36.2074000569812],[17.8602782490056,19.4467919597367,19.9435128020601,20.0812677479008,20.3742512748958,20.3976544270132,20.4053626324088,20.7687987173876,21.3897816572588,21.4198216070314,21.6629297388544,21.7824716956433,23.1268865383514,23.2844681472166,23.5633362667215,23.7502510814066,23.8033483751119,23.8762246221982,24.1821515317116,25.3271649265763,25.5807124897907,25.596205528662,25.6668166466088,26.3718102112525,26.5273004777729,26.6020283636738,27.0668205603448,27.3320418514176,27.9462070692758,28.0435934748088,28.3506687230787,28.4309444108695,28.6815607252458,29.3051388913026,29.3522232832166,29.3636271508704,29.619515993256,29.6253877312591,30.9192804237138,31.0023777315952,31.1857395190839,32.0246617325357,32.4836685784939,32.6960699498278,33.0819178324419,33.9154624849415,34.1480475316179,36.0128423476352,36.1659572619526,36.5597417487322],[19.8006670578219,21.387180768553,21.8839016108764,22.0216565567171,22.3146400837122,22.3380432358295,22.3457514412252,22.709187526204,23.3301704660751,23.3602104158478,23.6033185476708,23.7228605044597,25.0672753471678,25.2248569560329,25.5037250755379,25.6906398902229,25.7437371839282,25.8166134310145,26.122540340528,27.2675537353926,27.5211012986071,27.5365943374783,27.6072054554251,28.3121990200688,28.4676892865893,28.5424171724902,29.0072093691611,29.2724306602339,29.8865958780921,29.9839822836252,30.291057531895,30.3713332196858,30.6219495340621,31.245527700119,31.2926120920329,31.3040159596867,31.5599048020723,31.5657765400754,32.8596692325301,32.9427665404115,33.1261283279002,33.965050541352,34.4240573873102,34.6364587586441,35.0223066412582,35.8558512937578,36.0884363404343,37.9532311564515,38.106346070769,38.5001305575485],[19.8356407595015,21.4221544702326,21.918875312556,22.0566302583966,22.3496137853917,22.3730169375091,22.3807251429047,22.7441612278835,23.3651441677547,23.3951841175273,23.6382922493503,23.7578342061392,25.1022490488473,25.2598306577124,25.5386987772174,25.7256135919025,25.7787108856077,25.8515871326941,26.1575140422075,27.3025274370722,27.5560750002866,27.5715680391579,27.6421791571046,28.3471727217484,28.5026629882688,28.5773908741697,29.0421830708406,29.3074043619134,29.9215695797716,30.0189559853047,30.3260312335746,30.4063069213653,30.6569232357416,31.2805014017985,31.3275857937124,31.3389896613662,31.5948785037519,31.6007502417549,32.8946429342096,32.9777402420911,33.1611020295797,34.0000242430315,34.4590310889898,34.6714324603237,35.0572803429377,35.8908249954374,36.1234100421138,37.9882048581311,38.1413197724485,38.535104259228],[19.864909501661,21.4514232123921,21.9481440547155,22.0858990005562,22.3788825275512,22.4022856796686,22.4099938850642,22.773429970043,23.3944129099142,23.4244528596868,23.6675609915098,23.7871029482987,25.1315177910068,25.289099399872,25.5679675193769,25.754882334062,25.8079796277673,25.8808558748536,26.186782784367,27.3317961792317,27.5853437424461,27.6008367813174,27.6714478992642,28.3764414639079,28.5319317304283,28.6066596163292,29.0714518130002,29.336673104073,29.9508383219312,30.0482247274642,30.3552999757341,30.4355756635249,30.6861919779012,31.309770143958,31.356854535872,31.3682584035258,31.6241472459114,31.6300189839145,32.9239116763692,33.0070089842506,33.1903707717393,34.0292929851911,34.4882998311493,34.7007012024832,35.0865490850973,35.9200937375969,36.1526787842733,38.0174736002906,38.170588514608,38.5643730013876],[20.3344244771224,21.9209381878534,22.4176590301768,22.5554139760175,22.8483975030126,22.87180065513,22.8795088605256,23.2429449455044,23.8639278853756,23.8939678351482,24.1370759669712,24.2566179237601,25.6010327664682,25.7586143753333,26.0374824948383,26.2243973095234,26.2774946032286,26.3503708503149,26.6562977598284,27.801311154693,28.0548587179075,28.0703517567788,28.1409628747255,28.8459564393692,29.0014467058897,29.0761745917906,29.5409667884615,29.8061880795343,30.4203532973925,30.5177397029256,30.8248149511955,30.9050906389862,31.1557069533625,31.7792851194194,31.8263695113333,31.8377733789871,32.0936622213727,32.0995339593758,33.3934266518305,33.4765239597119,33.6598857472006,34.4988079606524,34.9578148066106,35.1702161779445,35.5560640605586,36.3896087130582,36.6221937597347,38.486988575752,38.6401034900694,39.0338879768489],[21.2026417363488,22.7891554470799,23.2858762894033,23.423631235244,23.716614762239,23.7400179143564,23.747726119752,24.1111622047308,24.732145144602,24.7621850943746,25.0052932261976,25.1248351829865,26.4692500256946,26.6268316345598,26.9056997540647,27.0926145687498,27.1457118624551,27.2185881095414,27.5245150190548,28.6695284139195,28.9230759771339,28.9385690160052,29.009180133952,29.7141736985957,29.8696639651161,29.944391851017,30.409184047688,30.6744053387608,31.288570556619,31.385956962152,31.6930322104219,31.7733078982127,32.023924212589,32.6475023786458,32.6945867705598,32.7059906382136,32.9618794805992,32.9677512186023,34.261643911057,34.3447412189384,34.5281030064271,35.3670252198789,35.8260320658371,36.038433437171,36.4242813197851,37.2578259722847,37.4904110189611,39.3552058349784,39.5083207492958,39.9021052360754],[22.9665781294921,24.5530918402231,25.0498126825465,25.1875676283872,25.4805511553823,25.5039543074997,25.5116625128953,25.8750985978741,26.4960815377453,26.5261214875179,26.7692296193409,26.8887715761298,28.2331864188379,28.390768027703,28.669636147208,28.8565509618931,28.9096482555983,28.9825245026846,29.2884514121981,30.4334648070627,30.6870123702772,30.7025054091485,30.7731165270952,31.4781100917389,31.6336003582594,31.7083282441603,32.1731204408312,32.438341731904,33.0525069497622,33.1498933552953,33.4569686035652,33.5372442913559,33.7878606057322,34.4114387717891,34.458523163703,34.4699270313568,34.7258158737424,34.7316876117455,36.0255803042002,36.1086776120816,36.2920393995703,37.1309616130221,37.5899684589804,37.8023698303142,38.1882177129283,39.0217623654279,39.2543474121044,41.1191422281217,41.2722571424391,41.6660416292186],[25.4585511375223,27.0450648482534,27.5417856905768,27.6795406364175,27.9725241634126,27.9959273155299,28.0036355209255,28.3670716059043,28.9880545457755,29.0180944955482,29.2612026273712,29.38074458416,30.7251594268681,30.8827410357333,31.1616091552383,31.3485239699233,31.4016212636286,31.4744975107149,31.7804244202283,32.925437815093,33.1789853783074,33.1944784171787,33.2650895351255,33.9700830997692,34.1255733662896,34.2003012521906,34.6650934488615,34.9303147399343,35.5444799577925,35.6418663633256,35.9489416115954,36.0292172993862,36.2798336137625,36.9034117798193,36.9504961717333,36.9619000393871,37.2177888817727,37.2236606197758,38.5175533122305,38.6006506201119,38.7840124076006,39.6229346210524,40.0819414670106,40.2943428383445,40.6801907209586,41.5137353734582,41.7463204201347,43.6111152361519,43.7642301504694,44.1580146372489],[25.8237281173679,27.410241828099,27.9069626704224,28.0447176162631,28.3377011432582,28.3611042953755,28.3688125007711,28.7322485857499,29.3532315256211,29.3832714753938,29.6263796072168,29.7459215640056,31.0903364067137,31.2479180155789,31.5267861350839,31.7137009497689,31.7667982434742,31.8396744905605,32.145601400074,33.2906147949386,33.544162358153,33.5596553970243,33.6302665149711,34.3352600796148,34.4907503461352,34.5654782320362,35.0302704287071,35.2954917197799,35.9096569376381,36.0070433431712,36.314118591441,36.3943942792318,36.6450105936081,37.2685887596649,37.3156731515789,37.3270770192327,37.5829658616183,37.5888375996214,38.8827302920761,38.9658275999575,39.1491893874462,39.988111600898,40.4471184468562,40.6595198181901,41.0453677008042,41.8789123533038,42.1114973999803,43.9762922159975,44.129407130315,44.5231916170945],[25.8581615674465,27.4446752781775,27.9413961205009,28.0791510663416,28.3721345933367,28.3955377454541,28.4032459508497,28.7666820358285,29.3876649756997,29.4177049254723,29.6608130572953,29.7803550140842,31.1247698567923,31.2823514656574,31.5612195851624,31.7481343998475,31.8012316935527,31.874107940639,32.1800348501525,33.3250482450171,33.5785958082316,33.5940888471029,33.6646999650496,34.3696935296933,34.5251837962138,34.5999116821147,35.0647038787856,35.3299251698584,35.9440903877166,36.0414767932497,36.3485520415196,36.4288277293103,36.6794440436866,37.3030222097435,37.3501066016574,37.3615104693112,37.6173993116968,37.6232710496999,38.9171637421546,39.000261050036,39.1836228375247,40.0225450509765,40.4815518969347,40.6939532682686,41.0798011508827,41.9133458033823,42.1459308500588,44.010725666076,44.1638405803935,44.557625067173],[30.3823844619369,31.968898172668,32.4656190149914,32.6033739608321,32.8963574878271,32.9197606399445,32.9274688453401,33.2909049303189,33.9118878701901,33.9419278199627,34.1850359517858,34.3045779085746,35.6489927512827,35.8065743601479,36.0854424796529,36.2723572943379,36.3254545880432,36.3983308351295,36.7042577446429,37.8492711395076,38.102818702722,38.1183117415933,38.1889228595401,38.8939164241838,39.0494066907042,39.1241345766052,39.5889267732761,39.8541480643489,40.4683132822071,40.5656996877402,40.87277493601,40.9530506238008,41.2036669381771,41.8272451042339,41.8743294961479,41.8857333638017,42.1416222061873,42.1474939441904,43.4413866366451,43.5244839445265,43.7078457320152,44.546767945467,45.0057747914252,45.2181761627591,45.6040240453732,46.4375686978728,46.6701537445493,48.5349485605665,48.688063474884,49.0818479616635],[30.8075994086513,32.3941131193824,32.8908339617058,33.0285889075465,33.3215724345416,33.3449755866589,33.3526837920545,33.7161198770333,34.3371028169045,34.3671427666772,34.6102508985002,34.729792855289,36.0742076979972,36.2317893068623,36.5106574263673,36.6975722410523,36.7506695347576,36.8235457818439,37.1294726913573,38.274486086222,38.5280336494364,38.5435266883077,38.6141378062545,39.3191313708982,39.4746216374186,39.5493495233196,40.0141417199905,40.2793630110633,40.8935282289215,40.9909146344546,41.2979898827244,41.3782655705152,41.6288818848915,42.2524600509483,42.2995444428623,42.3109483105161,42.5668371529017,42.5727088909048,43.8666015833595,43.9496988912409,44.1330606787296,44.9719828921814,45.4309897381396,45.6433911094735,46.0292389920876,46.8627836445872,47.0953686912637,48.9601635072809,49.1132784215984,49.5070629083779],[32.1371362530886,33.7236499638197,34.2203708061431,34.3581257519838,34.6511092789789,34.6745124310962,34.6822206364918,35.0456567214706,35.6666396613418,35.6966796111145,35.9397877429375,36.0593296997263,37.4037445424345,37.5613261512996,37.8401942708046,38.0271090854896,38.0802063791949,38.1530826262812,38.4590095357946,39.6040229306593,39.8575704938737,39.873063532745,39.9436746506918,40.6486682153355,40.8041584818559,40.8788863677569,41.3436785644278,41.6088998555006,42.2230650733588,42.3204514788919,42.6275267271617,42.7078024149525,42.9584187293288,43.5819968953856,43.6290812872996,43.6404851549534,43.896373997339,43.9022457353421,45.1961384277968,45.2792357356782,45.4625975231669,46.3015197366187,46.7605265825769,46.9729279539108,47.3587758365249,48.1923204890245,48.424905535701,50.2897003517182,50.4428152660357,50.8365997528152],[32.677267416281,34.2637811270121,34.7605019693355,34.8982569151762,35.1912404421713,35.2146435942886,35.2223517996842,35.585787884663,36.2067708245342,36.2368107743069,36.4799189061299,36.5994608629188,37.9438757056269,38.101457314492,38.380325433997,38.567240248682,38.6203375423873,38.6932137894736,38.9991406989871,40.1441540938517,40.3977016570662,40.4131946959374,40.4838058138842,41.1887993785279,41.3442896450483,41.4190175309493,41.8838097276202,42.149031018693,42.7631962365512,42.8605826420843,43.1676578903541,43.2479335781449,43.4985498925212,44.122128058578,44.169212450492,44.1806163181458,44.4365051605314,44.4423768985345,45.7362695909892,45.8193668988706,46.0027286863593,46.8416508998111,47.3006577457693,47.5130591171032,47.8989069997173,48.7324516522169,48.9650366988934,50.8298315149106,50.9829464292281,51.3767309160076],[32.9148475007094,34.5013612114405,34.9980820537639,35.1358369996046,35.4288205265996,35.452223678717,35.4599318841126,35.8233679690914,36.4443509089626,36.4743908587352,36.7174989905582,36.8370409473471,38.1814557900552,38.3390373989204,38.6179055184253,38.8048203331104,38.8579176268156,38.930793873902,39.2367207834154,40.3817341782801,40.6352817414945,40.6507747803658,40.7213858983125,41.4263794629563,41.5818697294767,41.6565976153776,42.1213898120486,42.3866111031213,43.0007763209796,43.0981627265126,43.4052379747825,43.4855136625732,43.7361299769495,44.3597081430064,44.4067925349203,44.4181964025741,44.6740852449598,44.6799569829628,45.9738496754176,46.056946983299,46.2403087707876,47.0792309842394,47.5382378301977,47.7506392015316,48.1364870841457,48.9700317366453,49.2026167833217,51.067411599339,51.2205265136564,51.6143110004359],[33.172377597936,34.7588913086671,35.2556121509905,35.3933670968312,35.6863506238263,35.7097537759436,35.7174619813392,36.080898066318,36.7018810061892,36.7319209559619,36.9750290877849,37.0945710445737,38.4389858872819,38.596567496147,38.875435615652,39.062350430337,39.1154477240423,39.1883239711286,39.494250880642,40.6392642755067,40.8928118387211,40.9083048775924,40.9789159955392,41.6839095601829,41.8393998267033,41.9141277126043,42.3789199092752,42.644141200348,43.2583064182062,43.3556928237393,43.6627680720091,43.7430437597999,43.9936600741762,44.617238240233,44.664322632147,44.6757264998008,44.9316153421864,44.9374870801895,46.2313797726442,46.3144770805256,46.4978388680143,47.3367610814661,47.7957679274243,48.0081692987582,48.3940171813723,49.2275618338719,49.4601468805484,51.3249416965656,51.4780566108831,51.8718410976626],[35.6273075474506,37.2138212581817,37.7105421005051,37.8482970463457,38.1412805733408,38.1646837254582,38.1723919308538,38.5358280158326,39.1568109557038,39.1868509054764,39.4299590372994,39.5495009940883,40.8939158367964,41.0514974456616,41.3303655651665,41.5172803798516,41.5703776735568,41.6432539206432,41.9491808301566,43.0941942250213,43.3477417882357,43.363234827107,43.4338459450537,44.1388395096975,44.2943297762179,44.3690576621188,44.8338498587898,45.0990711498625,45.7132363677208,45.8106227732538,46.1176980215237,46.1979737093144,46.4485900236907,47.0721681897476,47.1192525816615,47.1306564493153,47.386545291701,47.392417029704,48.6863097221587,48.7694070300402,48.9527688175288,49.7916910309806,50.2506978769389,50.4630992482728,50.8489471308868,51.6824917833865,51.9150768300629,53.7798716460802,53.9329865603976,54.3267710471771],[35.9717792918917,37.5582930026228,38.0550138449462,38.1927687907869,38.4857523177819,38.5091554698993,38.5168636752949,38.8802997602737,39.5012827001449,39.5313226499175,39.7744307817405,39.8939727385294,41.2383875812375,41.3959691901027,41.6748373096076,41.8617521242927,41.9148494179979,41.9877256650843,42.2936525745977,43.4386659694624,43.6922135326768,43.7077065715481,43.7783176894948,44.4833112541386,44.638801520659,44.7135294065599,45.1783216032309,45.4435428943036,46.0577081121619,46.1550945176949,46.4621697659648,46.5424454537556,46.7930617681319,47.4166399341887,47.4637243261026,47.4751281937564,47.7310170361421,47.7368887741451,49.0307814665999,49.1138787744813,49.29724056197,50.1361627754218,50.59516962138,50.8075709927139,51.193418875328,52.0269635278276,52.259548574504,54.1243433905213,54.2774583048387,54.6712427916183],[36.0740143558878,37.6605280666189,38.1572489089423,38.295003854783,38.587987381778,38.6113905338954,38.619098739291,38.9825348242698,39.603517764141,39.6335577139136,39.8766658457366,39.9962078025255,41.3406226452336,41.4982042540988,41.7770723736037,41.9639871882888,42.017084481994,42.0899607290804,42.3958876385938,43.5409010334585,43.7944485966729,43.8099416355442,43.8805527534909,44.5855463181347,44.7410365846551,44.815764470556,45.280556667227,45.5457779582997,46.159943176158,46.257329581691,46.5644048299609,46.6446805177516,46.895296832128,47.5188749981848,47.5659593900987,47.5773632577525,47.8332521001382,47.8391238381412,49.133016530596,49.2161138384774,49.399475625966,50.2383978394178,50.6974046853761,50.90980605671,51.2956539393241,52.1291985918237,52.3617836385001,54.2265784545174,54.3796933688348,54.7734778556144],[36.0964384335477,37.6829521442788,38.1796729866021,38.3174279324428,38.6104114594379,38.6338146115553,38.6415228169509,39.0049589019297,39.6259418418009,39.6559817915735,39.8990899233965,40.0186318801854,41.3630467228935,41.5206283317586,41.7994964512636,41.9864112659487,42.0395085596539,42.1123848067403,42.4183117162537,43.5633251111183,43.8168726743328,43.8323657132041,43.9029768311508,44.6079703957946,44.763460662315,44.8381885482159,45.3029807448868,45.5682020359596,46.1823672538178,46.2797536593509,46.5868289076208,46.6671045954115,46.9177209097878,47.5412990758447,47.5883834677586,47.5997873354124,47.855676177798,47.8615479158011,49.1554406082558,49.2385379161372,49.4218997036259,50.2608219170777,50.719828763036,50.9322301343698,51.3180780169839,52.1516226694836,52.38420771616,54.2490025321773,54.4021174464947,54.7959019332742],[36.377797786776,37.9643114975071,38.4610323398305,38.5987872856712,38.8917708126663,38.9151739647836,38.9228821701792,39.286318255158,39.9073011950292,39.9373411448019,40.1804492766249,40.2999912334137,41.6444060761219,41.801987684987,42.080855804492,42.267770619177,42.3208679128823,42.3937441599686,42.6996710694821,43.8446844643467,44.0982320275611,44.1137250664324,44.1843361843792,44.8893297490229,45.0448200155433,45.1195479014443,45.5843400981152,45.849561389188,46.4637266070462,46.5611130125793,46.8681882608491,46.9484639486399,47.1990802630162,47.822658429073,47.869742820987,47.8811466886408,48.1370355310264,48.1429072690295,49.4367999614842,49.5198972693656,49.7032590568543,50.5421812703061,51.0011881162643,51.2135894875982,51.5994373702123,52.4329820227119,52.6655670693884,54.5303618854056,54.6834767997231,55.0772612865026],[40.1331254443919,41.719639155123,42.2163599974464,42.3541149432871,42.6470984702821,42.6705016223995,42.6782098277951,43.0416459127739,43.6626288526451,43.6926688024177,43.9357769342407,44.0553188910296,45.3997337337377,45.5573153426029,45.8361834621078,46.0230982767929,46.0761955704981,46.1490718175845,46.4549987270979,47.6000121219626,47.853559685177,47.8690527240483,47.939663841995,48.6446574066388,48.8001476731592,48.8748755590601,49.3396677557311,49.6048890468038,50.2190542646621,50.3164406701951,50.623515918465,50.7037916062557,50.9544079206321,51.5779860866889,51.6250704786028,51.6364743462566,51.8923631886423,51.8982349266453,53.1921276191001,53.2752249269815,53.4585867144701,54.2975089279219,54.7565157738802,54.9689171452141,55.3547650278282,56.1883096803278,56.4208947270042,58.2856895430215,58.4388044573389,58.8325889441184],[42.2865745519015,43.8730882626326,44.369809104956,44.5075640507966,44.8005475777917,44.8239507299091,44.8316589353047,45.1950950202835,45.8160779601547,45.8461179099273,46.0892260417503,46.2087679985392,47.5531828412473,47.7107644501124,47.9896325696174,48.1765473843025,48.2296446780077,48.3025209250941,48.6084478346075,49.7534612294722,50.0070087926866,50.0225018315579,50.0931129495046,50.7981065141484,50.9535967806688,51.0283246665697,51.4931168632407,51.7583381543134,52.3725033721716,52.4698897777047,52.7769650259746,52.8572407137653,53.1078570281416,53.7314351941985,53.7785195861124,53.7899234537662,54.0458122961519,54.0516840341549,55.3455767266096,55.4286740344911,55.6120358219797,56.4509580354315,56.9099648813898,57.1223662527237,57.5082141353377,58.3417587878374,58.5743438345138,60.4391386505311,60.5922535648485,60.986038051628],[42.7218934281679,44.3084071388989,44.8051279812223,44.942882927063,45.2358664540581,45.2592696061755,45.2669778115711,45.6304138965499,46.2513968364211,46.2814367861937,46.5245449180167,46.6440868748056,47.9885017175137,48.1460833263788,48.4249514458838,48.6118662605689,48.6649635542741,48.7378398013605,49.0437667108739,50.1887801057385,50.442327668953,50.4578207078243,50.528431825771,51.2334253904148,51.3889156569352,51.4636435428361,51.928435739507,52.1936570305798,52.807822248438,52.9052086539711,53.212283902241,53.2925595900317,53.543175904408,54.1667540704649,54.2138384623788,54.2252423300326,54.4811311724182,54.4870029104213,55.780895602876,55.8639929107574,56.0473546982461,56.8862769116979,57.3452837576562,57.55768512899,57.9435330116041,58.7770776641037,59.0096627107802,60.8744575267975,61.0275724411149,61.4213569278944],[43.0588805936043,44.6453943043353,45.1421151466587,45.2798700924994,45.5728536194945,45.5962567716119,45.6039649770075,45.9674010619863,46.5883840018575,46.6184239516301,46.8615320834531,46.981074040242,48.3254888829501,48.4830704918152,48.7619386113202,48.9488534260053,49.0019507197105,49.0748269667968,49.3807538763103,50.5257672711749,50.7793148343894,50.7948078732607,50.8654189912074,51.5704125558511,51.7259028223716,51.8006307082725,52.2654229049434,52.5306441960162,53.1448094138744,53.2421958194075,53.5492710676774,53.6295467554681,53.8801630698444,54.5037412359013,54.5508256278152,54.562229495469,54.8181183378546,54.8239900758577,56.1178827683124,56.2009800761938,56.3843418636825,57.2232640771343,57.6822709230925,57.8946722944264,58.2805201770405,59.1140648295401,59.3466498762166,61.2114446922338,61.3645596065513,61.7583440933308],[43.3834464293513,44.9699601400824,45.4666809824058,45.6044359282464,45.8974194552415,45.9208226073589,45.9285308127545,46.2919668977333,46.9129498376045,46.9429897873771,47.1860979192001,47.305639875989,48.6500547186971,48.8076363275622,49.0865044470672,49.2734192617523,49.3265165554575,49.3993928025439,49.7053197120573,50.850333106922,51.1038806701364,51.1193737090077,51.1899848269544,51.8949783915982,52.0504686581186,52.1251965440195,52.5899887406905,52.8552100317632,53.4693752496215,53.5667616551545,53.8738369034244,53.9541125912151,54.2047289055914,54.8283070716483,54.8753914635622,54.886795331216,55.1426841736017,55.1485559116047,56.4424486040594,56.5255459119409,56.7089076994295,57.5478299128813,58.0068367588396,58.2192381301735,58.6050860127875,59.4386306652872,59.6712157119636,61.5360105279809,61.6891254422983,62.0829099290778],[44.0579310411903,45.6444447519214,46.1411655942448,46.2789205400855,46.5719040670806,46.5953072191979,46.6030154245935,46.9664515095723,47.5874344494435,47.6174743992162,47.8605825310392,47.980124487828,49.3245393305362,49.4821209394013,49.7609890589063,49.9479038735913,50.0010011672966,50.0738774143829,50.3798043238964,51.524817718761,51.7783652819754,51.7938583208467,51.8644694387935,52.5694630034372,52.7249532699576,52.7996811558586,53.2644733525295,53.5296946436023,54.1438598614605,54.2412462669936,54.5483215152634,54.6285972030542,54.8792135174305,55.5027916834873,55.5498760754013,55.5612799430551,55.8171687854407,55.8230405234438,57.1169332158985,57.2000305237799,57.3833923112686,58.2223145247204,58.6813213706786,58.8937227420125,59.2795706246266,60.1131152771262,60.3457003238027,62.2104951398199,62.3636100541374,62.7573945409169],[45.1148401341531,46.7013538448842,47.1980746872076,47.3358296330483,47.6288131600434,47.6522163121607,47.6599245175563,48.0233606025351,48.6443435424063,48.674383492179,48.917491624002,49.0370335807908,50.381448423499,50.5390300323641,50.8178981518691,51.0048129665541,51.0579102602594,51.1307865073457,51.4367134168591,52.5817268117238,52.8352743749382,52.8507674138095,52.9213785317563,53.6263720964,53.7818623629204,53.8565902488214,54.3213824454923,54.5866037365651,55.2007689544233,55.2981553599564,55.6052306082262,55.685506296017,55.9361226103933,56.5597007764501,56.6067851683641,56.6181890360179,56.8740778784035,56.8799496164066,58.1738423088613,58.2569396167427,58.4403014042314,59.2792236176832,59.7382304636414,59.9506318349753,60.3364797175894,61.170024370089,61.4026094167655,63.2674042327827,63.4205191471002,63.8143036338797],[45.3440635632487,46.9305772739798,47.4272981163032,47.5650530621439,47.8580365891389,47.8814397412563,47.8891479466519,48.2525840316307,48.8735669715019,48.9036069212745,49.1467150530975,49.2662570098864,50.6106718525945,50.7682534614597,51.0471215809646,51.2340363956497,51.2871336893549,51.3600099364413,51.6659368459547,52.8109502408194,53.0644978040338,53.0799908429051,53.1506019608518,53.8555955254956,54.011085792016,54.0858136779169,54.5506058745879,54.8158271656606,55.4299923835189,55.5273787890519,55.8344540373218,55.9147297251126,56.1653460394889,56.7889242055457,56.8360085974596,56.8474124651134,57.1033013074991,57.1091730455021,58.4030657379569,58.4861630458383,58.669524833327,59.5084470467787,59.967453892737,60.1798552640709,60.565703146685,61.3992477991846,61.631832845861,63.4966276618783,63.6497425761957,64.0435270629753],[45.4015455944031,46.9880593051342,47.4847801474575,47.6225350932982,47.9155186202933,47.9389217724107,47.9466299778063,48.3100660627851,48.9310490026563,48.9610889524289,49.2041970842519,49.3237390410408,50.6681538837489,50.825735492614,51.104603612119,51.2915184268041,51.3446157205093,51.4174919675957,51.7234188771091,52.8684322719737,53.1219798351882,53.1374728740595,53.2080839920062,53.91307755665,54.0685678231704,54.1432957090713,54.6080879057422,54.873309196815,55.4874744146732,55.5848608202063,55.8919360684762,55.9722117562669,56.2228280706432,56.8464062367001,56.893490628614,56.9048944962678,57.1607833386534,57.1666550766565,58.4605477691112,58.5436450769926,58.7270068644813,59.5659290779331,60.0249359238914,60.2373372952252,60.6231851778393,61.456729830339,61.6893148770154,63.5541096930327,63.7072246073501,64.1010090941296],[45.96301243713,47.549526147861,48.0462469901844,48.1840019360251,48.4769854630202,48.5003886151376,48.5080968205332,48.871532905512,49.4925158453832,49.5225557951558,49.7656639269788,49.8852058837677,51.2296207264758,51.3872023353409,51.6660704548459,51.852985269531,51.9060825632362,51.9789588103226,52.284885719836,53.4298991147006,53.6834466779151,53.6989397167864,53.7695508347331,54.4745443993769,54.6300346658973,54.7047625517982,55.1695547484691,55.4347760395419,56.0489412574001,56.1463276629332,56.4534029112031,56.5336785989938,56.7842949133701,57.407873079427,57.4549574713409,57.4663613389947,57.7222501813803,57.7281219193834,59.0220146118381,59.1051119197195,59.2884737072082,60.12739592066,60.5864027666183,60.7988041379521,61.1846520205662,62.0181966730658,62.2507817197423,64.1155765357596,64.268691450077,64.6624759368565],[48.3309203620991,49.9174340728301,50.4141549151535,50.5519098609942,50.8448933879893,50.8682965401067,50.8760047455023,51.2394408304811,51.8604237703523,51.8904637201249,52.1335718519479,52.2531138087368,53.5975286514449,53.75511026031,54.033978379815,54.2208931945001,54.2739904882053,54.3468667352916,54.6527936448051,55.7978070396697,56.0513546028842,56.0668476417555,56.1374587597022,56.8424523243459,56.9979425908664,57.0726704767673,57.5374626734382,57.802683964511,58.4168491823692,58.5142355879023,58.8213108361722,58.9015865239629,59.1522028383392,59.7757810043961,59.82286539631,59.8342692639638,60.0901581063494,60.0960298443525,61.3899225368072,61.4730198446886,61.6563816321773,62.4953038456291,62.9543106915873,63.1667120629212,63.5525599455353,64.3861045980349,64.6186896447114,66.4834844607286,66.6365993750461,67.0303838618256],[48.9932333516995,50.5797470624306,51.076467904754,51.2142228505947,51.5072063775897,51.5306095297071,51.5383177351027,51.9017538200815,52.5227367599527,52.5527767097254,52.7958848415484,52.9154267983372,54.2598416410453,54.4174232499105,54.6962913694155,54.8832061841005,54.9363034778058,55.0091797248921,55.3151066344055,56.4601200292702,56.7136675924846,56.7291606313559,56.7997717493027,57.5047653139464,57.6602555804668,57.7349834663678,58.1997756630387,58.4649969541115,59.0791621719697,59.1765485775028,59.4836238257726,59.5638995135634,59.8145158279397,60.4380939939965,60.4851783859105,60.4965822535643,60.7524710959499,60.758342833953,62.0522355264077,62.1353328342891,62.3186946217778,63.1576168352296,63.6166236811878,63.8290250525217,64.2148729351358,65.0484175876354,65.2810026343119,67.1457974503291,67.2989123646466,67.6926968514261],[51.9561938071583,53.5427075178894,54.0394283602128,54.1771833060535,54.4701668330485,54.4935699851659,54.5012781905615,54.8647142755403,55.4856972154115,55.5157371651841,55.7588452970071,55.878387253796,57.2228020965041,57.3803837053693,57.6592518248742,57.8461666395593,57.8992639332646,57.9721401803509,58.2780670898643,59.423080484729,59.6766280479434,59.6921210868147,59.7627322047615,60.4677257694052,60.6232160359256,60.6979439218265,61.1627361184975,61.4279574095703,62.0421226274285,62.1395090329615,62.4465842812314,62.5268599690222,62.7774762833985,63.4010544494553,63.4481388413693,63.4595427090231,63.7154315514087,63.7213032894118,65.0151959818665,65.0982932897479,65.2816550772366,66.1205772906884,66.5795841366466,66.7919855079805,67.1778333905946,68.0113780430942,68.2439630897706,70.1087579057879,70.2618728201053,70.6556573068849],[52.4186878621709,54.0052015729019,54.5019224152253,54.639677361066,54.9326608880611,54.9560640401785,54.9637722455741,55.3272083305529,55.9481912704241,55.9782312201967,56.2213393520197,56.3408813088086,57.6852961515167,57.8428777603818,58.1217458798868,58.3086606945719,58.3617579882771,58.4346342353634,58.7405611448769,59.8855745397415,60.139122102956,60.1546151418273,60.225226259774,60.9302198244177,61.0857100909382,61.1604379768391,61.62523017351,61.8904514645828,62.504616682441,62.6020030879741,62.909078336244,62.9893540240347,63.239970338411,63.8635485044679,63.9106328963818,63.9220367640356,64.1779256064212,64.1837973444243,65.477690036879,65.5607873447604,65.7441491322491,66.5830713457009,67.0420781916591,67.254479562993,67.6403274456071,68.4738720981067,68.7064571447832,70.5712519608005,70.7243668751179,71.1181513618974],[52.8131878065176,54.3997015172486,54.896422359572,55.0341773054127,55.3271608324078,55.3505639845252,55.3582721899208,55.7217082748996,56.3426912147708,56.3727311645434,56.6158392963664,56.7353812531553,58.0797960958634,58.2373777047285,58.5162458242335,58.7031606389186,58.7562579326238,58.8291341797101,59.1350610892236,60.2800744840882,60.5336220473027,60.549115086174,60.6197262041207,61.3247197687644,61.4802100352849,61.5549379211858,62.0197301178567,62.2849514089295,62.8991166267877,62.9965030323208,63.3035782805907,63.3838539683814,63.6344702827577,64.2580484488146,64.3051328407285,64.3165367083823,64.5724255507679,64.578297288771,65.8721899812257,65.9552872891071,66.1386490765958,66.9775712900476,67.4365781360058,67.6489795073397,68.0348273899538,68.8683720424534,69.1009570891299,70.9657519051472,71.1188668194646,71.5126513062441],[54.3773996700658,55.9639133807969,56.4606342231203,56.598389168961,56.891372695956,56.9147758480734,56.922484053469,57.2859201384478,57.906903078319,57.9369430280916,58.1800511599146,58.2995931167035,59.6440079594116,59.8015895682768,60.0804576877817,60.2673725024668,60.320469796172,60.3933460432584,60.6992729527718,61.8442863476365,62.0978339108509,62.1133269497222,62.1839380676689,62.8889316323127,63.0444218988331,63.119149784734,63.583941981405,63.8491632724777,64.463328490336,64.560714895869,64.8677901441389,64.9480658319296,65.1986821463059,65.8222603123628,65.8693447042767,65.8807485719305,66.1366374143162,66.1425091523192,67.436401844774,67.5194991526554,67.702860940144,68.5417831535958,69.0007899995541,69.213191370888,69.5990392535021,70.4325839060017,70.6651689526781,72.5299637686954,72.6830786830128,73.0768631697923],[59.9002086090453,61.4867223197764,61.9834431620998,62.1211981079405,62.4141816349356,62.4375847870529,62.4452929924485,62.8087290774273,63.4297120172985,63.4597519670712,63.7028600988942,63.822402055683,65.1668168983911,65.3243985072563,65.6032666267613,65.7901814414463,65.8432787351516,65.9161549822379,66.2220818917513,67.367095286616,67.6206428498304,67.6361358887017,67.7067470066485,68.4117405712922,68.5672308378126,68.6419587237136,69.1067509203845,69.3719722114573,69.9861374293155,70.0835238348486,70.3905990831184,70.4708747709092,70.7214910852855,71.3450692513424,71.3921536432563,71.4035575109101,71.6594463532957,71.6653180912988,72.9592107837535,73.0423080916349,73.2256698791236,74.0645920925754,74.5235989385336,74.7360003098675,75.1218481924816,75.9553928449812,76.1879778916577,78.0527727076749,78.2058876219924,78.5996721087719],[60.5794527387442,62.1659664494753,62.6626872917987,62.8004422376394,63.0934257646344,63.1168289167518,63.1245371221474,63.4879732071262,64.1089561469974,64.13899609677,64.382104228593,64.5016461853819,65.84606102809,66.0036426369552,66.2825107564601,66.4694255711452,66.5225228648504,66.5953991119368,66.9013260214502,68.0463394163149,68.2998869795293,68.3153800184006,68.3859911363473,69.0909847009911,69.2464749675115,69.3212028534124,69.7859950500834,70.0512163411561,70.6653815590144,70.7627679645474,71.0698432128173,71.150118900608,71.4007352149844,72.0243133810412,72.0713977729551,72.0828016406089,72.3386904829946,72.3445622209976,73.6384549134524,73.7215522213338,73.9049140088224,74.7438362222742,75.2028430682325,75.4152444395664,75.8010923221805,76.6346369746801,76.8672220213565,78.7320168373738,78.8851317516912,79.2789162384707],[60.599932063724,62.1864457744551,62.6831666167784,62.8209215626191,63.1139050896142,63.1373082417316,63.1450164471272,63.508452532106,64.1294354719772,64.1594754217498,64.4025835535728,64.5221255103617,65.8665403530698,66.0241219619349,66.3029900814399,66.489904896125,66.5430021898302,66.6158784369166,66.92180534643,68.0668187412946,68.3203663045091,68.3358593433804,68.4064704613271,69.1114640259709,69.2669542924913,69.3416821783922,69.8064743750631,70.0716956661359,70.6858608839941,70.7832472895272,71.0903225377971,71.1705982255878,71.4212145399641,72.044792706021,72.0918770979349,72.1032809655887,72.3591698079743,72.3650415459774,73.6589342384321,73.7420315463135,73.9253933338022,74.764315547254,75.2233223932123,75.4357237645461,75.8215716471602,76.6551162996598,76.8877013463363,78.7524961623536,78.905611076671,79.2993955634505],[60.6117290337129,62.198242744444,62.6949635867674,62.8327185326081,63.1257020596031,63.1491052117205,63.1568134171161,63.5202495020949,64.1412324419661,64.1712723917387,64.4143805235617,64.5339224803506,65.8783373230587,66.0359189319239,66.3147870514288,66.5017018661139,66.5547991598191,66.6276754069055,66.9336023164189,68.0786157112836,68.332163274498,68.3476563133693,68.418267431316,69.1232609959598,69.2787512624802,69.3534791483811,69.8182713450521,70.0834926361248,70.6976578539831,70.7950442595161,71.102119507786,71.1823951955767,71.433011509953,72.0565896760099,72.1036740679238,72.1150779355776,72.3709667779633,72.3768385159663,73.6707312084211,73.7538285163025,73.9371903037911,74.7761125172429,75.2351193632012,75.4475207345351,75.8333686171492,76.6669132696488,76.8994983163252,78.7642931323425,78.9174080466599,79.3111925334395],[61.3730299955294,62.9595437062605,63.4562645485839,63.5940194944246,63.8870030214197,63.9104061735371,63.9181143789327,64.2815504639115,64.9025334037827,64.9325733535553,65.1756814853783,65.2952234421672,66.6396382848753,66.7972198937404,67.0760880132454,67.2630028279304,67.3161001216357,67.388976368722,67.6949032782355,68.8399166731001,69.0934642363146,69.1089572751858,69.1795683931326,69.8845619577763,70.0400522242968,70.1147801101977,70.5795723068686,70.8447935979414,71.4589588157996,71.5563452213327,71.8634204696026,71.9436961573933,72.1943124717696,72.8178906378265,72.8649750297404,72.8763788973942,73.1322677397798,73.1381394777829,74.4320321702376,74.515129478119,74.6984912656077,75.5374134790595,75.9964203250177,76.2088216963516,76.5946695789657,77.4282142314653,77.6607992781418,79.525594094159,79.6787090084765,80.072493495256],[64.6859634511171,66.2724771618482,66.7691980041716,66.9069529500123,67.1999364770073,67.2233396291247,67.2310478345203,67.5944839194991,68.2154668593703,68.2455068091429,68.4886149409659,68.6081568977548,69.9525717404629,70.1101533493281,70.389021468833,70.5759362835181,70.6290335772233,70.7019098243097,71.0078367338231,72.1528501286878,72.4063976919022,72.4218907307735,72.4925018487202,73.197495413364,73.3529856798844,73.4277135657853,73.8925057624563,74.157727053529,74.7718922713873,74.8692786769203,75.1763539251902,75.2566296129809,75.5072459273572,76.1308240934141,76.177908485328,76.1893123529818,76.4452011953675,76.4510729333705,77.7449656258253,77.8280629337067,78.0114247211953,78.8503469346471,79.3093537806054,79.5217551519393,79.9076030345534,80.741147687053,80.9737327337294,82.8385275497467,82.9916424640641,83.3854269508437],[68.3834292217502,69.9699429324813,70.4666637748047,70.6044187206454,70.8974022476404,70.9208053997578,70.9285136051534,71.2919496901322,71.9129326300034,71.942972579776,72.186080711599,72.3056226683879,73.650037511096,73.8076191199612,74.0864872394662,74.2734020541512,74.3264993478565,74.3993755949428,74.7053025044562,75.8503158993209,76.1038634625353,76.1193565014066,76.1899676193534,76.8949611839971,77.0504514505175,77.1251793364184,77.5899715330894,77.8551928241622,78.4693580420204,78.5667444475534,78.8738196958233,78.9540953836141,79.2047116979904,79.8282898640472,79.8753742559612,79.886778123615,80.1426669660006,80.1485387040037,81.4424313964584,81.5255287043398,81.7088904918285,82.5478127052803,83.0068195512385,83.2192209225724,83.6050688051865,84.4386134576861,84.6711985043626,86.5359933203798,86.6891082346972,87.0828927214768],[71.9264845699335,73.5129982806646,74.009719122988,74.1474740688286,74.4404575958237,74.4638607479411,74.4715689533367,74.8350050383155,75.4559879781867,75.4860279279593,75.7291360597823,75.8486780165712,77.1930928592793,77.3506744681444,77.6295425876494,77.8164574023345,77.8695546960397,77.9424309431261,78.2483578526395,79.3933712475041,79.6469188107186,79.6624118495899,79.7330229675366,80.4380165321804,80.5935067987008,80.6682346846017,81.1330268812726,81.3982481723454,82.0124133902036,82.1097997957367,82.4168750440066,82.4971507317973,82.7477670461736,83.3713452122305,83.4184296041444,83.4298334717982,83.6857223141838,83.6915940521869,84.9854867446416,85.0685840525231,85.2519458400117,86.0908680534635,86.5498748994218,86.7622762707556,87.1481241533697,87.9816688058693,88.2142538525458,90.0790486685631,90.2321635828805,90.62594806966],[75.188826965711,76.7753406764421,77.2720615187655,77.4098164646061,77.7027999916012,77.7262031437186,77.7339113491142,78.097347434093,78.7183303739642,78.7483703237368,78.9914784555598,79.1110204123487,80.4554352550568,80.6130168639219,80.8918849834269,81.078799798112,81.1318970918172,81.2047733389036,81.510700248417,82.6557136432816,82.9092612064961,82.9247542453674,82.9953653633141,83.7003589279579,83.8558491944783,83.9305770803792,84.3953692770501,84.6605905681229,85.2747557859811,85.3721421915142,85.6792174397841,85.7594931275748,86.0101094419511,86.633687608008,86.6807719999219,86.6921758675757,86.9480647099613,86.9539364479644,88.2478291404191,88.3309264483005,88.5142882357892,89.353210449241,89.8122172951993,90.0246186665331,90.4104665491472,91.2440112016468,91.4765962483233,93.3413910643406,93.494505978658,93.8882904654375]],"type":"surface","x":[-2.96039892327731,-2.73214618289166,-2.54136756927678,-2.33420760703804,-2.29853927796707,-2.1021094254153,-2.09856896020911,-2.0956060206783,-2.04807598083181,-1.96018442660419,-1.78161724252153,-1.52934930101089,-1.49238162732587,-1.48889585298465,-1.03089875877432,-0.987853308859119,-0.853261352273738,-0.798582679493088,-0.77453192186067,-0.748461580289934,-0.49994358955681,-0.465071952665595,-0.454722470925065,-0.452452431926527,-0.423969802044517,-0.0438096739725239,0.17418874404757,0.218257037107956,0.252370993505637,0.285227511214592,0.353507080881254,0.460500326950301,0.483705121872882,0.489524155075263,0.546362685903954,0.786071244541534,0.853118653957231,1.15306569690969,1.19988499344418,1.23982109570761,1.39816972373539,1.95725589276825,2.02601727895909,2.02809044634073,2.02928467971549,2.10635286058609,2.44172844471548,2.8160310859391,3.17470241843918,3.5049565576428],"y":[-2.96739065955215,-2.44805785324636,-2.28546018563748,-2.24036718573331,-2.1444613286036,-2.13680049051357,-2.13427727000019,-2.01530932119005,-1.81203543162327,-1.80220208993125,-1.72262255180219,-1.68349143086038,-1.24340778810319,-1.19182468573189,-1.10053939641347,-1.03935429943388,-1.02197331724591,-0.998117849967946,-0.897975078198987,-0.523163933743912,-0.440167129716625,-0.435095605089509,-0.411981610050177,-0.181207502094158,-0.130308984218711,-0.105847397513396,0.0462986786367592,0.133116785819328,0.334158947616182,0.366037622814148,0.466556294258567,0.492833910352589,0.574871193088133,0.778994609569731,0.794407315568089,0.798140282104294,0.881903485485141,0.88382555281705,1.30737116637587,1.33457241765193,1.39459445907624,1.6692090586459,1.81946134566535,1.88898926684997,2.01529354164428,2.28814783885948,2.3642827276904,2.97470866881919,3.02482963380164,3.15373189365332],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">983</span>)
rho&lt;-<span class="fl">0.99</span>
x&lt;-<span class="dv">2</span><span class="op">*</span><span class="kw">mvrnorm</span>(<span class="dt">n=</span>n,<span class="dt">mu=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>),<span class="dt">Sigma=</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,rho,rho,<span class="dv">1</span>),<span class="dt">ncol=</span><span class="dv">2</span>,<span class="dt">byrow=</span>T))
x1&lt;-x[,<span class="dv">1</span>]
x2&lt;-x[,<span class="dv">2</span>]
y&lt;-beta0<span class="op">+</span>beta1<span class="op">*</span>x1<span class="op">+</span>beta2<span class="op">*</span>x2<span class="op">+</span><span class="kw">rnorm</span>(n,<span class="dt">sd=</span>sigma) 

m3&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x1<span class="op">+</span>x2)

xx &lt;-<span class="st"> </span><span class="kw">sort</span>(x1)
yy &lt;-<span class="st"> </span><span class="kw">sort</span>(x2)
fit &lt;-<span class="st"> </span><span class="kw">outer</span>(xx, yy, <span class="cf">function</span>(a, b){<span class="kw">predict</span>(m3,<span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">x1=</span>a,<span class="dt">x2=</span>b))})

axx&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;x1&quot;</span>)
axy&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;x2&quot;</span>)
axz&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;y&quot;</span>)

<span class="kw">plot_ly</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">add_markers</span>(<span class="dt">x=</span>x1,
              <span class="dt">y=</span>x2,
              <span class="dt">z=</span>y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_surface</span>(<span class="dt">x=</span><span class="op">~</span>xx,
              <span class="dt">y=</span><span class="op">~</span>yy,
              <span class="dt">z=</span><span class="op">~</span>fit,
              <span class="dt">showscale=</span><span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">layout</span>(<span class="dt">scene =</span> <span class="kw">list</span>(<span class="dt">xaxis=</span>axx,<span class="dt">yaxis=</span>axy,<span class="dt">zaxis=</span>axz))</code></pre></div>
<div id="htmlwidget-ef0e22a4b0a1243b16aa" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-ef0e22a4b0a1243b16aa">{"x":{"visdat":{"dc15f9254d1":["function () ","plotlyVisDat"]},"cur_data":"dc15f9254d1","attrs":{"dc15f9254d1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[2.05567570010434,1.64546737333353,-0.98585636159149,0.145436500810527,2.23080449186914,1.4158553175941,0.396154307276448,5.33433948212546,-1.48078830684263,0.512188899192018,0.394486532393321,-1.51158866544863,-0.414059299619805,1.40682626830658,-1.59611574077461,1.97492388577371,0.742678031713295,-1.65689485118863,2.75101125208859,2.18990436397892,-1.44485803668215,1.47691249674086,-0.755733252895172,-0.09401621207894,-1.62443935718435,0.371981710378547,1.18312406031599,-0.851693229515672,-0.371847705946829,3.61714700692691,-3.79160974822952,0.163903990695111,0.125891042360704,0.646035706452884,3.42993996456778,1.26496305113097,-0.0687014707382249,-0.149982316825593,-3.21180106444372,0.427033533055787,2.26435337007621,1.02513796314026,3.70878577582133,-1.1194680259074,-1.34982293283585,-2.93442533408609,-1.21162327066926,1.5489509747353,1.2953802240997,-2.23948570656194],"y":[2.01929167320148,2.63832015677903,-1.09614889076506,-0.103072564567768,2.02879998919901,1.94249706331783,0.666055586690686,4.76108097024044,-1.36538851182604,0.0807013958788015,0.599834176427104,-1.64818561174969,-0.128709935204398,1.59950770057417,-1.54655160856301,2.01204727691861,0.833835248073225,-1.22426879368609,2.86413548426478,2.02224967478038,-1.81476622854792,1.60794728094207,-1.29261197705307,0.043340365586029,-1.68805158423357,0.281833453138403,1.2408689176852,-1.1241332016812,-0.521936896100953,3.71411632837187,-3.96625309707395,0.386159158000361,0.23726124785331,0.429586249039983,3.81642321445712,0.943841538120979,-0.35957397272494,-0.317063149401063,-3.25357877831655,0.306301030285228,1.96882310451407,0.71247874224825,3.27890051936193,-1.1779059314858,-1.39072935224815,-3.11276389825864,-1.22602967025068,1.43401509400912,1.19911678208591,-2.24357337219482],"z":[72.4112283037445,80.1357940223725,37.0673223603711,51.705495470334,78.8752505297219,73.3573259158224,53.7042664462778,117.156336201732,25.5404408635633,52.3328220462206,51.9942984233549,33.177183721453,55.0833167832042,71.2226609082324,26.9287442980817,82.2824905062434,63.3613389960669,40.2902821986686,85.230854135455,70.2558702036732,29.5134623752076,67.3618315631795,38.4577206056567,61.4990741471532,26.2605936101416,52.0177298700554,63.8321581818994,35.7541723426673,46.3865511742847,101.331828322834,-11.4063643942466,52.7532010886446,59.7645641018044,52.747803571642,96.6374716908503,76.2990365379506,50.6798105691903,43.7543635134124,11.6061812520325,55.3972407582171,72.2609901493509,58.8890099584912,90.8505437370928,34.0836932623783,30.4735690629398,5.79829772235815,43.5057423094769,67.8456192535648,69.2379631381695,17.3828611502725],"type":"scatter3d","mode":"markers","inherit":true},"dc15f9254d1.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"showscale":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"x1"},"yaxis":{"title":"x2"},"zaxis":{"title":"y"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[2.05567570010434,1.64546737333353,-0.98585636159149,0.145436500810527,2.23080449186914,1.4158553175941,0.396154307276448,5.33433948212546,-1.48078830684263,0.512188899192018,0.394486532393321,-1.51158866544863,-0.414059299619805,1.40682626830658,-1.59611574077461,1.97492388577371,0.742678031713295,-1.65689485118863,2.75101125208859,2.18990436397892,-1.44485803668215,1.47691249674086,-0.755733252895172,-0.09401621207894,-1.62443935718435,0.371981710378547,1.18312406031599,-0.851693229515672,-0.371847705946829,3.61714700692691,-3.79160974822952,0.163903990695111,0.125891042360704,0.646035706452884,3.42993996456778,1.26496305113097,-0.0687014707382249,-0.149982316825593,-3.21180106444372,0.427033533055787,2.26435337007621,1.02513796314026,3.70878577582133,-1.1194680259074,-1.34982293283585,-2.93442533408609,-1.21162327066926,1.5489509747353,1.2953802240997,-2.23948570656194],"y":[2.01929167320148,2.63832015677903,-1.09614889076506,-0.103072564567768,2.02879998919901,1.94249706331783,0.666055586690686,4.76108097024044,-1.36538851182604,0.0807013958788015,0.599834176427104,-1.64818561174969,-0.128709935204398,1.59950770057417,-1.54655160856301,2.01204727691861,0.833835248073225,-1.22426879368609,2.86413548426478,2.02224967478038,-1.81476622854792,1.60794728094207,-1.29261197705307,0.043340365586029,-1.68805158423357,0.281833453138403,1.2408689176852,-1.1241332016812,-0.521936896100953,3.71411632837187,-3.96625309707395,0.386159158000361,0.23726124785331,0.429586249039983,3.81642321445712,0.943841538120979,-0.35957397272494,-0.317063149401063,-3.25357877831655,0.306301030285228,1.96882310451407,0.71247874224825,3.27890051936193,-1.1779059314858,-1.39072935224815,-3.11276389825864,-1.22602967025068,1.43401509400912,1.19911678208591,-2.24357337219482],"z":[72.4112283037445,80.1357940223725,37.0673223603711,51.705495470334,78.8752505297219,73.3573259158224,53.7042664462778,117.156336201732,25.5404408635633,52.3328220462206,51.9942984233549,33.177183721453,55.0833167832042,71.2226609082324,26.9287442980817,82.2824905062434,63.3613389960669,40.2902821986686,85.230854135455,70.2558702036732,29.5134623752076,67.3618315631795,38.4577206056567,61.4990741471532,26.2605936101416,52.0177298700554,63.8321581818994,35.7541723426673,46.3865511742847,101.331828322834,-11.4063643942466,52.7532010886446,59.7645641018044,52.747803571642,96.6374716908503,76.2990365379506,50.6798105691903,43.7543635134124,11.6061812520325,55.3972407582171,72.2609901493509,58.8890099584912,90.8505437370928,34.0836932623783,30.4735690629398,5.79829772235815,43.5057423094769,67.8456192535648,69.2379631381695,17.3828611502725],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"fit","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[-1.01501733498186,5.92575118072583,7.29715388224757,15.7622410597593,19.9384138831665,21.1724937294413,21.5607502996326,22.5505686091704,24.0681288498178,24.3149244796506,25.0236985549045,25.672146754305,25.6892960135079,26.1408260964481,26.6645212264791,26.9370617390643,32.5293392057974,34.1105993088012,34.5246142049989,36.3589949656358,36.6086785170938,38.0346008902696,38.3984617113282,39.9232054305956,40.3572962221059,40.595587100066,41.3733291401257,41.7962676104527,43.4543201362309,44.0992535473641,44.5513708301302,45.7332675151053,46.8046239178792,49.2907614557669,49.6973874567381,51.5784470996557,53.1901873335845,53.2723808014303,56.530576775854,56.7869673220949,57.2079295583462,57.2784830725,57.3072911881957,57.3710850066177,63.3072304204226,65.5064564541459,69.5458725257648,73.7844596408181,74.7808311883696,83.980897245039],[0.938776621993597,7.87954513770129,9.25094783922303,17.7160350167347,21.892207840142,23.1262876864168,23.5145442566081,24.5043625661458,26.0219228067932,26.268718436626,26.97749251188,27.6259407112805,27.6430899704833,28.0946200534235,28.6183151834545,28.8908556960398,34.4831331627728,36.0643932657766,36.4784081619744,38.3127889226113,38.5624724740692,39.988394847245,40.3522556683037,41.8769993875711,42.3110901790813,42.5493810570414,43.3271230971012,43.7500615674281,45.4081140932064,46.0530475043396,46.5051647871056,47.6870614720808,48.7584178748547,51.2445554127424,51.6511814137136,53.5322410566312,55.1439812905599,55.2261747584058,58.4843707328294,58.7407612790704,59.1617235153216,59.2322770294754,59.2610851451712,59.3248789635932,65.2610243773981,67.4602504111214,71.4996664827403,75.7382535977935,76.7346251453451,85.9346912020144],[1.8734556651667,8.81422418087439,10.1856268823961,18.6507140599078,22.8268868833151,24.0609667295899,24.4492232997812,25.4390416093189,26.9566018499663,27.2033974797992,27.9121715550531,28.5606197544536,28.5777690136564,29.0292990965966,29.5529942266276,29.8255347392129,35.4178122059459,36.9990723089497,37.4130872051475,39.2474679657844,39.4971515172424,40.9230738904181,41.2869347114768,42.8116784307442,43.2457692222544,43.4840601002145,44.2618021402743,44.6847406106012,46.3427931363795,46.9877265475127,47.4398438302787,48.6217405152539,49.6930969180278,52.1792344559155,52.5858604568867,54.4669200998043,56.078660333733,56.1608538015789,59.4190497760025,59.6754403222435,60.0964025584947,60.1669560726485,60.1957641883443,60.2595580067663,66.1957034205712,68.3949294542945,72.4343455259134,76.6729326409666,77.6693041885182,86.8693702451875],[4.21520887219708,11.1559773879048,12.5273800894265,20.9924672669382,25.1686400903454,26.4027199366203,26.7909765068115,27.7807948163493,29.2983550569967,29.5451506868295,30.2539247620835,30.902372961484,30.9195222206868,31.371052303627,31.894747433658,32.1672879462432,37.7595654129763,39.3408255159801,39.7548404121779,41.5892211728147,41.8389047242727,43.2648270974485,43.6286879185071,45.1534316377745,45.5875224292848,45.8258133072449,46.6035553473046,47.0264938176316,48.6845463434099,49.3294797545431,49.7815970373091,50.9634937222842,52.0348501250581,54.5209876629459,54.927613663917,56.8086733068347,58.4204135407634,58.5026070086093,61.7608029830329,62.0171935292739,62.4381557655251,62.5087092796789,62.5375173953747,62.6013112137967,68.5374566276016,70.7366826613249,74.7760987329438,79.014685847997,80.0110573955486,89.2111234522179],[6.17837797340018,13.1191464891079,14.4905491906296,22.9556363681413,27.1318091915485,28.3658890378234,28.7541456080146,29.7439639175524,31.2615241581998,31.5083197880326,32.2170938632866,32.8655420626871,32.8826913218899,33.3342214048301,33.8579165348611,34.1304570474463,39.7227345141794,41.3039946171832,41.718009513381,43.5523902740178,43.8020738254758,45.2279961986516,45.5918570197102,47.1166007389776,47.5506915304879,47.788982408448,48.5667244485077,48.9896629188347,50.647715444613,51.2926488557462,51.7447661385122,52.9266628234873,53.9980192262612,56.484156764149,56.8907827651201,58.7718424080378,60.3835826419665,60.4657761098124,63.723972084236,63.980362630477,64.4013248667282,64.471878380882,64.5006864965778,64.5644803149998,70.5006257288046,72.699851762528,76.7392678341469,80.9778549492001,81.9742264967517,91.174292553421],[6.28774395881832,13.228512474526,14.5999151760478,23.0650023535594,27.2411751769667,28.4752550232415,28.8635115934328,29.8533299029706,31.3708901436179,31.6176857734508,32.3264598487047,32.9749080481052,32.992057307308,33.4435873902482,33.9672825202792,34.2398230328645,39.8321004995975,41.4133606026013,41.8273754987991,43.661756259436,43.911439810894,45.3373621840697,45.7012230051284,47.2259667243958,47.660057515906,47.8983483938662,48.6760904339259,49.0990289042529,50.7570814300311,51.4020148411643,51.8541321239304,53.0360288089055,54.1073852116794,56.5935227495671,57.0001487505383,58.8812083934559,60.4929486273846,60.5751420952305,63.8333380696541,64.0897286158951,64.5106908521464,64.5812443663001,64.6100524819959,64.6738463004179,70.6099917142228,72.8092177479461,76.848633819565,81.0872209346182,82.0835924821698,91.2836585388391],[6.38318666626401,13.3239551819717,14.6953578834934,23.1604450610051,27.3366178844124,28.5706977306872,28.9589543008785,29.9487726104163,31.4663328510636,31.7131284808965,32.4219025561504,33.0703507555509,33.0875000147537,33.5390300976939,34.0627252277249,34.3352657403102,39.9275432070432,41.508803310047,41.9228182062448,43.7571989668817,44.0068825183397,45.4328048915154,45.7966657125741,47.3214094318415,47.7555002233517,47.9937911013118,48.7715331413716,49.1944716116985,50.8525241374768,51.49745754861,51.9495748313761,53.1314715163512,54.2028279191251,56.6889654570128,57.095591457984,58.9766511009016,60.5883913348303,60.6705848026762,63.9287807770998,64.1851713233408,64.606133559592,64.6766870737458,64.7054951894416,64.7692890078636,70.7054344216685,72.9046604553918,76.9440765270107,81.1826636420639,82.1790351896155,91.3791012462848],[6.6680193940086,13.6087879097163,14.980190611238,23.4452777887497,27.621450612157,28.8555304584318,29.2437870286231,30.2336053381608,31.7511655788082,31.997961208641,32.706735283895,33.3551834832955,33.3723327424983,33.8238628254385,34.3475579554695,34.6200984680548,40.2123759347878,41.7936360377916,42.2076509339894,44.0420316946263,44.2917152460842,45.71763761926,46.0814984403187,47.6062421595861,48.0403329510963,48.2786238290564,49.0563658691162,49.4793043394431,51.1373568652214,51.7822902763546,52.2344075591206,53.4163042440958,54.4876606468696,56.9737981847574,57.3804241857286,59.2614838286462,60.8732240625749,60.9554175304208,64.2136135048444,64.4700040510854,64.8909662873366,64.9615198014904,64.9903279171862,65.0541217356082,70.9902671494131,73.1894931831364,77.2289092547553,81.4674963698085,82.4638679173601,91.6639339740294],[6.77180803360631,13.712576549314,15.0839792508357,23.5490664283474,27.7252392517547,28.9593190980295,29.3475756682208,30.3373939777586,31.8549542184059,32.1017498482388,32.8105239234927,33.4589721228932,33.476121382096,33.9276514650362,34.4513465950672,34.7238871076525,40.3161645743855,41.8974246773893,42.3114395735871,44.145820334224,44.395503885682,45.8214262588577,46.1852870799164,47.7100307991838,48.144121590694,48.3824124686541,49.1601545087139,49.5830929790408,51.2411455048191,51.8860789159523,52.3381961987184,53.5200928836935,54.5914492864674,57.0775868243551,57.4842128253263,59.3652724682439,60.9770127021726,61.0592061700185,64.3174021444421,64.5737926906831,64.9947549269343,65.0653084410881,65.0941165567839,65.1579103752059,71.0940557890108,73.2932818227341,77.332697894353,81.5712850094062,82.5676565569578,91.7677226136271],[6.89288304796048,13.8336515636682,15.2050542651899,23.6701414427016,27.8463142661088,29.0803941123837,29.4686506825749,30.4584689921127,31.9760292327601,32.2228248625929,32.9315989378469,33.5800471372474,33.5971963964502,34.0487264793904,34.5724216094214,34.8449621220066,40.4372395887397,42.0184996917435,42.4325145879413,44.2668953485781,44.5165789000361,45.9425012732119,46.3063620942705,47.8311058135379,48.2651966050482,48.5034874830083,49.281229523068,49.704167993395,51.3622205191733,52.0071539303065,52.4592712130725,53.6411678980476,54.7125243008215,57.1986618387093,57.6052878396804,59.4863474825981,61.0980877165268,61.1802811843727,64.4384771587963,64.6948677050373,65.1158299412885,65.1863834554423,65.2151915711381,65.2789853895601,71.215130803365,73.4143568370883,77.4537729087072,81.6923600237604,82.688731571312,91.8887976279813],[7.21312490840153,14.1538934241092,15.525296125631,23.9903833031426,28.1665561265499,29.4006359728247,29.788892543016,30.7787108525538,32.2962710932012,32.543066723034,33.2518407982879,33.9002889976884,33.9174382568913,34.3689683398315,34.8926634698625,35.1652039824477,40.7574814491808,42.3387415521846,42.7527564483823,44.5871372090192,44.8368207604772,46.262743133653,46.6266039547116,48.151347673979,48.5854384654892,48.8237293434494,49.6014713835091,50.0244098538361,51.6824623796143,52.3273957907475,52.7795130735136,53.9614097584887,55.0327661612626,57.5189036991503,57.9255297001215,59.8065893430391,61.4183295769679,61.5005230448137,64.7587190192374,65.0151095654783,65.4360718017296,65.5066253158834,65.5354334315791,65.5992272500011,71.535372663806,73.7345986975293,77.7740147691483,82.0126018842015,83.008973431753,92.2090394884223],[7.67881932165886,14.6195878373665,15.9909905388883,24.4560777164,28.6322505398072,29.8663303860821,30.2545869562733,31.2444052658111,32.7619655064585,33.0087611362913,33.7175352115453,34.3659834109458,34.3831326701486,34.8346627530888,35.3583578831198,35.630898395705,41.2231758624381,42.8044359654419,43.2184508616397,45.0528316222765,45.3025151737345,46.7284375469103,47.0922983679689,48.6170420872363,49.0511328787466,49.2894237567067,50.0671657967664,50.4901042670934,52.1481567928717,52.7930902040048,53.2452074867709,54.427104171746,55.4984605745199,57.9845981124077,58.3912241133788,60.2722837562965,61.8840239902252,61.966217458071,65.2244134324947,65.4808039787357,65.9017662149869,65.9723197291407,66.0011278448365,66.0649216632585,72.0010670770633,74.2002931107866,78.2397091824056,82.4782962974588,83.4746678450103,92.6747339016797],[7.98935685818502,14.9301253738927,16.3015280754145,24.7666152529261,28.9427880763334,30.1768679226082,30.5651244927995,31.5549428023373,33.0725030429846,33.3192986728175,34.0280727480714,34.6765209474719,34.6936702066747,35.1452002896149,35.6688954196459,35.9414359322312,41.5337133989642,43.114973501968,43.5289883981658,45.3633691588027,45.6130527102607,47.0389750834364,47.4028359044951,48.9275796237625,49.3616704152727,49.5999612932329,50.3777033332926,50.8006418036196,52.4586943293978,53.103627740531,53.5557450232971,54.7376417082722,55.8089981110461,58.2951356489338,58.701761649905,60.5828212928226,62.1945615267514,62.2767549945972,65.5349509690209,65.7913415152618,66.212303751513,66.2828572656668,66.3116653813626,66.3754591997846,72.3116046135895,74.5108306473128,78.5502467189317,82.7888338339849,83.7852053815365,92.9852714382058],[8.439590995103,15.3803595108107,16.7517622123324,25.2168493898441,29.3930222132514,30.6271020595262,31.0153586297175,32.0051769392553,33.5227371799026,33.7695328097355,34.4783068849894,35.1267550843899,35.1439043435927,35.5954344265329,36.1191295565639,36.3916700691492,41.9839475358822,43.565207638886,43.9792225350838,45.8136032957207,46.0632868471787,47.4892092203544,47.8530700414131,49.3778137606805,49.8119045521907,50.0501954301508,50.8279374702106,51.2508759405375,52.9089284663158,53.553861877449,54.005979160215,55.1878758451902,56.2592322479641,58.7453697858518,59.151995786823,61.0330554297406,62.6447956636693,62.7269891315152,65.9851851059388,66.2415756521798,66.662537888431,66.7330914025848,66.7618995182806,66.8256933367026,72.7618387505075,74.9610647842308,79.0004808558497,83.2390679709029,84.2354395184545,93.4355055751238],[8.89168342494917,15.8324519406569,17.2038546421786,25.6689418196903,29.8451146430975,31.0791944893724,31.4674510595636,32.4572693691014,33.9748296097488,34.2216252395816,34.9303993148356,35.5788475142361,35.5959967734389,36.0475268563791,36.5712219864101,36.8437624989953,42.4360399657284,44.0173000687322,44.43131496493,46.2656957255668,46.5153792770248,47.9413016502006,48.3051624712592,49.8299061905266,50.2639969820369,50.502287859997,51.2800299000567,51.7029683703837,53.361020896162,54.0059543072952,54.4580715900612,55.6399682750363,56.7113246778102,59.197462215698,59.6040882166691,61.4851478595868,63.0968880935155,63.1790815613614,66.437277535785,66.693668082026,67.1146303182772,67.185183832431,67.2139919481268,67.2777857665488,73.2139311803536,75.413157214077,79.4525732856959,83.6911604007491,84.6875319483007,93.88759800497],[9.21504184933231,16.15581036504,17.5272130665617,25.9923002440734,30.1684730674807,31.4025529137555,31.7908094839468,32.7806277934846,34.2981880341319,34.5449836639648,35.2537577392187,35.9022059386192,35.919355197822,36.3708852807622,36.8945804107932,37.1671209233785,42.7593983901115,44.3406584931153,44.7546733893131,46.58905414995,46.838737701408,48.2646600745837,48.6285208956424,50.1532646149098,50.58735540642,50.8256462843801,51.6033883244399,52.0263267947668,53.6843793205451,54.3293127316783,54.7814300144444,55.9633266994195,57.0346831021934,59.5208206400811,59.9274466410523,61.8085062839699,63.4202465178986,63.5024399857445,66.7606359601681,67.0170265064091,67.4379887426603,67.5085422568141,67.5373503725099,67.6011441909319,73.5372896047368,75.7365156384601,79.775931710079,84.0145188251322,85.0108903726838,94.2109564293531],[10.3663880154617,17.3071565311694,18.6785592326912,27.1436464102028,31.3198192336101,32.5538990798849,32.9421556500762,33.931973959614,35.4495342002614,35.6963298300942,36.4051039053481,37.0535521047486,37.0707013639514,37.5222314468916,38.0459265769226,38.3184670895079,43.9107445562409,45.4920046592447,45.9060195554425,47.7404003160794,47.9900838675374,49.4160062407131,49.7798670617718,51.3046107810392,51.7387015725494,51.9769924505096,52.7547344905693,53.1776729608963,54.8357254866745,55.4806588978077,55.9327761805738,57.1146728655489,58.1860292683228,60.6721668062105,61.0787928071817,62.9598524500993,64.5715926840281,64.6537861518739,67.9119821262975,68.1683726725385,68.5893349087898,68.6598884229435,68.6886965386393,68.7524903570613,74.6886357708662,76.8878618045895,80.9272778762084,85.1658649912617,86.1622365388132,95.3623025954825],[10.5086293425389,17.4493978582466,18.8208005597683,27.28588773728,31.4620605606873,32.6961404069621,33.0843969771534,34.0742152866911,35.5917755273385,35.8385711571713,36.5473452324253,37.1957934318258,37.2129426910286,37.6644727739688,38.1881679039998,38.4607084165851,44.0529858833181,45.6342459863219,46.0482608825197,47.8826416431566,48.1323251946145,49.5582475677903,49.922108388849,51.4468521081164,51.8809428996266,52.1192337775867,52.8969758176465,53.3199142879734,54.9779668137517,55.6229002248849,56.0750175076509,57.2569141926261,58.3282705953999,60.8144081332877,61.2210341342589,63.1020937771765,64.7138340111052,64.7960274789511,68.0542234533747,68.3106139996157,68.7315762358669,68.8021297500207,68.8309378657165,68.8947316841385,74.8308770979434,77.0301031316667,81.0695192032856,85.3081063183388,86.3044778658904,95.5045439225597],[11.2562539791634,18.1970224948711,19.5684251963928,28.0335123739045,32.2096851973117,33.4437650435866,33.8320216137778,34.8218399233156,36.339400163963,36.5861957937958,37.2949698690498,37.9434180684503,37.9605673276531,38.4120974105933,38.9357925406243,39.2083330532095,44.8006105199426,46.3818706229464,46.7958855191442,48.630266279781,48.879949831239,50.3058722044148,50.6697330254734,52.1944767447408,52.6285675362511,52.8668584142112,53.6446004542709,54.0675389245979,55.7255914503762,56.3705248615094,56.8226421442754,58.0045388292505,59.0758952320244,61.5620327699122,61.9686587708833,63.849718413801,65.4614586477297,65.5436521155756,68.8018480899992,69.0582386362402,69.4792008724914,69.5497543866452,69.578562502341,69.642356320763,75.5785017345679,77.7777277682912,81.8171438399101,86.0557309549633,87.0521025025149,96.2521685591842],[11.4448441819446,18.3856126976523,19.757015399174,28.2221025766857,32.3982754000929,33.6323552463678,34.020611816559,35.0104301260968,36.5279903667442,36.774785996577,37.483560071831,38.1320082712315,38.1491575304343,38.6006876133745,39.1243827434055,39.3969232559907,44.9892007227238,46.5704608257276,46.9844757219254,48.8188564825622,49.0685400340202,50.494462407196,50.8583232282546,52.383066947522,52.8171577390323,53.0554486169924,53.8331906570521,54.2561291273791,55.9141816531574,56.5591150642906,57.0112323470566,58.1931290320317,59.2644854348056,61.7506229726934,62.1572489736645,64.0383086165822,65.6500488505109,65.7322423183568,68.9904382927804,69.2468288390214,69.6677910752726,69.7383445894264,69.7671527051222,69.8309465235442,75.7670919373491,77.9663179710724,82.0057340426913,86.2443211577445,87.2406927052961,96.4407587619654],[11.5301478175006,18.4709163332083,19.84231903473,28.3074062122417,32.483579035649,33.7176588819238,34.1059154521151,35.0957337616529,36.6132940023002,36.8600896321331,37.568863707387,38.2173119067875,38.2344611659903,38.6859912489305,39.2096863789615,39.4822268915468,45.0745043582798,46.6557644612836,47.0697793574814,48.9041601181183,49.1538436695763,50.579766042752,50.9436268638107,52.4683705830781,52.9024613745883,53.1407522525484,53.9184942926082,54.3414327629351,55.9994852887134,56.6444186998466,57.0965359826127,58.2784326675878,59.3497890703617,61.8359266082494,62.2425526092206,64.1236122521382,65.7353524860669,65.8175459539128,69.0757419283364,69.3321324745774,69.7530947108286,69.8236482249824,69.8524563406782,69.9162501591002,75.8523955729051,78.0516216066284,82.0910376782473,86.3296247933005,87.3259963408521,96.5260623975214],[12.1858704494607,19.1266389651684,20.4980416666901,28.9631288442018,33.1393016676091,34.3733815138839,34.7616380840752,35.751456393613,37.2690166342603,37.5158122640932,38.2245863393471,38.8730345387476,38.8901837979504,39.3417138808906,39.8654090109216,40.1379495235069,45.7302269902399,47.3114870932437,47.7255019894415,49.5598827500784,49.8095663015364,51.2354886747121,51.5993494957708,53.1240932150382,53.5581840065484,53.7964748845085,54.5742169245683,54.9971553948953,56.6552079206735,57.3001413318067,57.7522586145728,58.9341552995479,60.0055117023218,62.4916492402095,62.8982752411807,64.7793348840983,66.391075118027,66.4732685858729,69.7314645602965,69.9878551065375,70.4088173427888,70.4793708569425,70.5081789726383,70.5719727910603,76.5081182048652,78.7073442385885,82.7467603102074,86.9853474252606,87.9817189728122,97.1817850294815],[12.2517332067348,19.1925017224425,20.5639044239642,29.0289916014759,33.2051644248832,34.439244271158,34.8275008413492,35.817319150887,37.3348793915344,37.5816750213672,38.2904490966212,38.9388972960217,38.9560465552245,39.4075766381647,39.9312717681957,40.203812280781,45.796089747514,47.3773498505178,47.7913647467156,49.6257455073524,49.8754290588104,51.3013514319862,51.6652122530448,53.1899559723122,53.6240467638225,53.8623376417826,54.6400796818423,55.0630181521693,56.7210706779476,57.3660040890808,57.8181213718468,59.0000180568219,60.0713744595958,62.5575119974836,62.9641379984547,64.8451976413724,66.4569378753011,66.539131343147,69.7973273175706,70.0537178638116,70.4746801000628,70.5452336142166,70.5740417299124,70.6378355483344,76.5739809621393,78.7732069958626,82.8126230674815,87.0512101825347,88.0475817300863,97.2476477867556],[12.3139635098434,19.2547320255511,20.6261347270728,29.0912219045845,33.2673947279918,34.5014745742666,34.8897311444579,35.8795494539957,37.397109694643,37.6439053244759,38.3526793997298,39.0011275991303,39.0182768583331,39.4698069412733,39.9935020713043,40.2660425838896,45.8583200506226,47.4395801536264,47.8535950498242,49.6879758104611,49.9376593619191,51.3635817350948,51.7274425561535,53.2521862754209,53.6862770669311,53.9245679448912,54.702309984951,55.1252484552779,56.7833009810562,57.4282343921894,57.8803516749555,59.0622483599306,60.1336047627045,62.6197423005922,63.0263683015634,64.907427944481,66.5191681784097,66.6013616462556,69.8595576206792,70.1159481669202,70.5369104031714,70.6074639173252,70.636272033021,70.700065851443,76.6362112652479,78.8354372989712,82.8748533705901,87.1134404856433,88.1098120331949,97.3098780898642],[13.0151275363017,19.9558960520094,21.3272987535311,29.7923859310428,33.9685587544501,35.2026386007249,35.5908951709162,36.5807134804539,38.0982737211013,38.3450693509341,39.0538434261881,39.7022916255886,39.7194408847914,40.1709709677316,40.6946660977626,40.9672066103479,46.5594840770809,48.1407441800847,48.5547590762825,50.3891398369193,50.6388233883773,52.0647457615531,52.4286065826117,53.9533503018791,54.3874410933894,54.6257319713495,55.4034740114093,55.8264124817362,57.4844650075145,58.1293984186477,58.5815157014137,59.7634123863889,60.8347687891627,63.3209063270505,63.7275323280216,65.6085919709393,67.220332204868,67.3025256727139,70.5607216471375,70.8171121933785,71.2380744296297,71.3086279437835,71.3374360594793,71.4012298779013,77.3373752917062,79.5366013254295,83.5760173970484,87.8146045121016,88.8109760596532,98.0110421163225],[13.0909625254712,20.0317310411789,21.4031337427006,29.8682209202123,34.0443937436196,35.2784735898944,35.6667301600856,36.6565484696234,38.1741087102708,38.4209043401036,39.1296784153576,39.7781266147581,39.7952758739609,40.2468059569011,40.7705010869321,41.0430415995174,46.6353190662504,48.2165791692542,48.630594065452,50.4649748260888,50.7146583775468,52.1405807507226,52.5044415717812,54.0291852910486,54.4632760825589,54.701566960519,55.4793090005787,55.9022474709057,57.560299996684,58.2052334078172,58.6573506905832,59.8392473755583,60.9106037783322,63.39674131622,63.8033673171911,65.6844269601088,67.2961671940375,67.3783606618834,70.636556636307,70.892947182548,71.3139094187992,71.384462932953,71.4132710486488,71.4770648670708,77.4132102808757,79.612436314599,83.6518523862179,87.8904395012711,88.8868110488227,98.086877105492],[13.0965824628391,20.0373509785468,21.4087536800685,29.8738408575802,34.0500136809875,35.2840935272623,35.6723500974536,36.6621684069913,38.1797286476387,38.4265242774715,39.1352983527255,39.783746552126,39.8008958113288,40.252425894269,40.7761210243,41.0486615368853,46.6409390036183,48.2221991066221,48.6362140028199,50.4705947634568,50.7202783149147,52.1462006880905,52.5100615091492,54.0348052284166,54.4688960199268,54.7071868978869,55.4849289379467,55.9078674082736,57.5659199340519,58.2108533451851,58.6629706279511,59.8448673129263,60.9162237157001,63.4023612535879,63.808987254559,65.6900468974767,67.3017871314054,67.3839805992513,70.6421765736749,70.8985671199159,71.3195293561671,71.3900828703209,71.4188909860167,71.4826848044387,77.4188302182436,79.6180562519669,83.6574723235858,87.896059438639,88.8924309861906,98.0924970428599],[13.2006368628695,20.1414053785772,21.5128080800989,29.9778952576106,34.1540680810179,35.3881479272927,35.776404497484,36.7662228070218,38.2837830476691,38.530578677502,39.2393527527559,39.8878009521564,39.9049502113592,40.3564802942994,40.8801754243304,41.1527159369157,46.7449934036487,48.3262535066525,48.7402684028503,50.5746491634872,50.8243327149452,52.2502550881209,52.6141159091796,54.138859628447,54.5729504199572,54.8112412979173,55.5889833379771,56.011921808304,57.6699743340823,58.3149077452155,58.7670250279816,59.9489217129567,61.0202781157306,63.5064156536183,63.9130416545895,65.7941012975071,67.4058415314358,67.4880349992817,70.7462309737053,71.0026215199463,71.4235837561975,71.4941372703513,71.5229453860471,71.5867392044691,77.522884618274,79.7221106519973,83.7615267236162,88.0001138386694,88.996485386221,98.1965514428903],[13.4875867558491,20.4283552715568,21.7997579730786,30.2648451505903,34.4410179739975,35.6750978202723,36.0633543904636,37.0531727000014,38.5707329406488,38.8175285704816,39.5263026457355,40.174750845136,40.1919001043389,40.6434301872791,41.1671253173101,41.4396658298953,47.0319432966284,48.6132033996322,49.02721829583,50.8615990564668,51.1112826079248,52.5372049811006,52.9010658021592,54.4258095214266,54.8599003129369,55.098191190897,55.8759332309567,56.2988717012837,57.956924227062,58.6018576381951,59.0539749209612,60.2358716059363,61.3072280087102,63.793365546598,64.1999915475691,66.0810511904867,67.6927914244155,67.7749848922613,71.033180866685,71.2895714129259,71.7105336491772,71.781087163331,71.8098952790267,71.8736890974487,77.8098345112536,80.0090605449769,84.0484766165959,88.2870637316491,89.2834352792006,98.48350133587],[13.938613259073,20.8793817747807,22.2507844763024,30.7158716538141,34.8920444772214,36.1261243234962,36.5143808936875,37.5041992032252,39.0217594438726,39.2685550737055,39.9773291489594,40.6257773483599,40.6429266075627,41.0944566905029,41.6181518205339,41.8906923331192,47.4829697998522,49.064229902856,49.4782447990538,51.3126255596907,51.5623091111487,52.9882314843244,53.3520923053831,54.8768360246505,55.3109268161607,55.5492176941208,56.3269597341806,56.7498982045075,58.4079507302858,59.052884141419,59.505001424185,60.6868981091602,61.7582545119341,64.2443920498218,64.651018050793,66.5320776937106,68.1438179276393,68.2260113954852,71.4842073699088,71.7405979161498,72.161560152401,72.2321136665548,72.2609217822506,72.3247156006726,78.2608610144775,80.4600870482008,84.4995031198197,88.7380902348729,89.7344617824245,98.9345278390938],[14.2642710085378,21.2050395242455,22.5764422257673,31.0415294032789,35.2177022266862,36.451782072961,36.8400386431523,37.8298569526901,39.3474171933375,39.5942128231703,40.3029868984242,40.9514350978247,40.9685843570276,41.4201144399677,41.9438095699987,42.216350082584,47.8086275493171,49.3898876523208,49.8039025485186,51.6382833091555,51.8879668606135,53.3138892337892,53.6777500548479,55.2024937741153,55.6365845656255,55.8748754435857,56.6526174836454,57.0755559539724,58.7336084797506,59.3785418908838,59.8306591736499,61.012555858625,62.0839122613989,64.5700497992866,64.9766758002578,66.8577354431754,68.4694756771042,68.55166914495,71.8098651193737,72.0662556656146,72.4872179018659,72.5577714160196,72.5865795317154,72.6503733501374,78.5865187639423,80.7857447976656,84.8251608692845,89.0637479843378,90.0601195318893,99.2601855885586],[15.2160823948495,22.1568509105572,23.5282536120789,31.9933407895906,36.1695136129979,37.4035934592727,37.791850029464,38.7816683390017,40.2992285796491,40.546024209482,41.2547982847359,41.9032464841364,41.9203957433392,42.3719258262794,42.8956209563104,43.1681614688957,48.7604389356287,50.3416990386325,50.7557139348303,52.5900946954672,52.8397782469251,54.2657006201009,54.6295614411596,56.154305160427,56.5883959519372,56.8266868298973,57.6044288699571,58.027367340284,59.6854198660623,60.3303532771955,60.7824705599615,61.9643672449367,63.0357236477106,65.5218611855983,65.9284871865695,67.8095468294871,69.4212870634158,69.5034805312617,72.7616765056853,73.0180670519263,73.4390292881775,73.5095828023313,73.5383909180271,73.6021847364491,79.538330150254,81.7375561839773,85.7769722555962,90.0155593706494,91.011930918201,100.21199697487],[15.7484515893442,22.6892201050519,24.0606228065736,32.5257099840853,36.7018828074926,37.9359626537674,38.3242192239587,39.3140375334964,40.8315977741438,41.0783934039766,41.7871674792306,42.4356156786311,42.4527649378339,42.9042950207741,43.4279901508051,43.7005306633904,49.2928081301234,50.8740682331272,51.288083129325,53.1224638899619,53.3721474414198,54.7980698145956,55.1619306356543,56.6866743549217,57.1207651464319,57.359056024392,58.1367980644518,58.5597365347787,60.217789060557,60.8627224716902,61.3148397544562,62.4967364394314,63.5680928422053,66.054230380093,66.4608563810642,68.3419160239818,69.9536562579105,70.0358497257564,73.29404570018,73.550436246421,73.9713984826722,74.041951996826,74.0707601125218,74.1345539309438,80.0706993447487,82.269925378472,86.3093414500909,90.5479285651441,91.5443001126957,100.744366169365],[16.0242262201332,22.9649947358409,24.3363974373626,32.8014846148743,36.9776574382815,38.2117372845564,38.5999938547476,39.5898121642854,41.1073724049328,41.3541680347656,42.0629421100196,42.7113903094201,42.7285395686229,43.1800696515631,43.7037647815941,43.9763052941793,49.5685827609124,51.1498428639162,51.563857760114,53.3982385207508,53.6479220722088,55.0738444453846,55.4377052664432,56.9624489857106,57.3965397772209,57.634830655181,58.4125726952407,58.8355111655677,60.493563691346,61.1384971024792,61.5906143852452,62.7725110702203,63.8438674729942,66.330005010882,66.7366310118531,68.6176906547708,70.2294308886995,70.3116243565454,73.569820330969,73.82621087721,74.2471731134612,74.317726627615,74.3465347433108,74.4103285617328,80.3464739755376,82.545700009261,86.5851160808799,90.8237031959331,91.8200747434847,101.020140800154],[16.1267236307386,23.0674921464463,24.438894847968,32.9039820254797,37.0801548488869,38.3142346951618,38.702491265353,39.6923095748908,41.2098698155382,41.456665445371,42.165439520625,42.8138877200255,42.8310369792283,43.2825670621685,43.8062621921995,44.0788027047847,49.6710801715178,51.2523402745216,51.6663551707194,53.5007359313562,53.7504194828142,55.17634185599,55.5402026770486,57.064946396316,57.4990371878263,57.7373280657864,58.5150701058461,58.9380085761731,60.5960611019514,61.2409945130846,61.6931117958506,62.8750084808257,63.9463648835996,66.4325024214874,66.8391284224585,68.7201880653762,70.3319282993049,70.4141217671508,73.6723177415744,73.9287082878154,74.3496705240666,74.4202240382204,74.4490321539162,74.5128259723382,80.4489713861431,82.6481974198664,86.6876134914853,90.9262006065385,91.9225721540901,101.122638210759],[16.5022657945408,23.4430343102485,24.8144370117702,33.2795241892819,37.4556970126892,38.689776858964,39.0780334291553,40.067851738693,41.5854119793404,41.8322076091732,42.5409816844272,43.1894298838277,43.2065791430305,43.6581092259707,44.1818043560017,44.454344868587,50.04662233532,51.6278824383238,52.0418973345216,53.8762780951584,54.1259616466164,55.5518840197922,55.9157448408508,57.4404885601182,57.8745793516285,58.1128702295886,58.8906122696484,59.3135507399753,60.9716032657536,61.6165366768868,62.0686539596528,63.2505506446279,64.3219070474018,66.8080445852896,67.2146705862608,69.0957302291784,70.7074704631071,70.789663930953,74.0478599053766,74.3042504516176,74.7252126878688,74.7957662020226,74.8245743177184,74.8883681361404,80.8245135499453,83.0237395836686,87.0631556552875,91.3017427703407,92.2981143178923,101.498180374562],[16.5326911786865,23.4734596943941,24.8448623959159,33.3099495734276,37.4861223968348,38.7202022431097,39.1084588133009,40.0982771228387,41.6158373634861,41.8626329933189,42.5714070685729,43.2198552679734,43.2370045271762,43.6885346101164,44.2122297401474,44.4847702527326,50.0770477194657,51.6583078224695,52.0723227186673,53.9067034793041,54.1563870307621,55.5823094039379,55.9461702249965,57.4709139442639,57.9050047357742,58.1432956137343,58.921037653794,59.343976124121,61.0020286498993,61.6469620610324,62.0990793437985,63.2809760287736,64.3523324315475,66.8384699694353,67.2450959704064,69.1261556133241,70.7378958472528,70.8200893150986,74.0782852895223,74.3346758357632,74.7556380720145,74.8261915861683,74.8549997018641,74.9187935202861,80.8549389340909,83.0541649678142,87.0935810394332,91.3321681544864,92.3285397020379,101.528605758707],[16.7384368858187,23.6792054015264,25.0506081030481,33.5156952805598,37.6918681039671,38.9259479502419,39.3142045204332,40.3040228299709,41.8215830706183,42.0683787004511,42.7771527757051,43.4256009751056,43.4427502343084,43.8942803172486,44.4179754472796,44.6905159598649,50.2827934265979,51.8640535296017,52.2780684257995,54.1124491864363,54.3621327378943,55.7880551110701,56.1519159321287,57.6766596513961,58.1107504429064,58.3490413208665,59.1267833609262,59.5497218312532,61.2077743570315,61.8527077681647,62.3048250509307,63.4867217359058,64.5580781386797,67.0442156765675,67.4508416775386,69.3319013204563,70.943641554385,71.0258350222309,74.2840309966545,74.5404215428955,74.9613837791467,75.0319372933005,75.0607454089963,75.1245392274183,81.0606846412232,83.2599106749465,87.2993267465654,91.5379138616186,92.5342854091702,101.73435146584],[16.981186514983,23.9219550306907,25.2933577322124,33.7584449097241,37.9346177331313,39.1686975794062,39.5569541495974,40.5467724591352,42.0643326997826,42.3111283296154,43.0199024048694,43.6683506042699,43.6854998634727,44.1370299464129,44.6607250764439,44.9332655890291,50.5255430557622,52.106803158766,52.5208180549638,54.3551988156006,54.6048823670586,56.0308047402344,56.394665561293,57.9194092805604,58.3535000720707,58.5917909500308,59.3695329900905,59.7924714604175,61.4505239861958,62.095457397329,62.547574680095,63.7294713650701,64.800827767844,67.2869653057318,67.6935913067029,69.5746509496206,71.1863911835493,71.2685846513952,74.5267806258188,74.7831711720598,75.204133408311,75.2746869224648,75.3034950381606,75.3672888565826,81.3034342703874,83.5026603041108,87.5420763757297,91.7806634907829,92.7770350383345,101.977101095004],[17.3064199266301,24.2471884423378,25.6185911438595,34.0836783213712,38.2598511447784,39.4939309910533,39.8821875612445,40.8720058707823,42.3895661114297,42.6363617412625,43.3451358165165,43.993584015917,44.0107332751198,44.46226335806,44.985958488091,45.2584990006762,50.8507764674093,52.4320365704131,52.8460514666109,54.6804322272477,54.9301157787057,56.3560381518815,56.7198989729401,58.2446426922075,58.6787334837178,58.9170243616779,59.6947664017376,60.1177048720646,61.7757573978429,62.4206908089761,62.8728080917421,64.0547047767172,65.1260611794911,67.6121987173789,68.01882471835,69.8998843612677,71.5116245951964,71.5938180630423,74.8520140374659,75.1084045837069,75.5293668199581,75.5999203341119,75.6287284498077,75.6925222682297,81.6286676820345,83.8278937157579,87.8673097873768,92.10589690243,93.1022684499816,102.302334506651],[18.416596716134,25.3573652318417,26.7287679333634,35.1938551108751,39.3700279342824,40.6041077805572,40.9923643507485,41.9821826602862,43.4997429009336,43.7465385307665,44.4553126060204,45.1037608054209,45.1209100646237,45.5724401475639,46.0961352775949,46.3686757901802,51.9609532569132,53.542213359917,53.9562282561148,55.7906090167517,56.0402925682097,57.4662149413854,57.8300757624441,59.3548194817115,59.7889102732217,60.0272011511818,60.8049431912416,61.2278816615685,62.8859341873468,63.53086759848,63.982984881246,65.1648815662212,66.2362379689951,68.7223755068828,69.129001507854,71.0100611507716,72.6218013847003,72.7039948525462,75.9621908269698,76.2185813732108,76.639543609462,76.7100971236158,76.7389052393116,76.8026990577336,82.7388444715385,84.9380705052618,88.9774865768807,93.2160736919339,94.2124452394855,103.412511296155],[18.6887078646319,25.6294763803396,27.0008790818613,35.465966259373,39.6421390827802,40.8762189290551,41.2644754992463,42.2542938087841,43.7718540494315,44.0186496792643,44.7274237545183,45.3758719539188,45.3930212131216,45.8445512960618,46.3682464260928,46.640786938678,52.2330644054111,53.8143245084149,54.2283394046127,56.0627201652495,56.3124037167075,57.7383260898833,58.1021869109419,59.6269306302093,60.0610214217196,60.2993122996797,61.0770543397394,61.4999928100664,63.1580453358447,63.8029787469779,64.2550960297439,65.436992714719,66.5083491174929,68.9944866553807,69.4011126563518,71.2821722992695,72.8939125331982,72.9761060010441,76.2343019754677,76.4906925217087,76.9116547579599,76.9822082721137,77.0110163878095,77.0748102062315,83.0109556200364,85.2101816537597,89.2495977253786,93.4881848404318,94.4845563879834,103.684622444653],[19.1410211184084,26.0817896341161,27.4531923356378,35.9182795131495,40.0944523365568,41.3285321828316,41.7167887530229,42.7066070625606,44.224167303208,44.4709629330409,45.1797370082948,45.8281852076953,45.8453344668981,46.2968645498383,46.8205596798693,47.0931001924546,52.6853776591876,54.2666377621914,54.6806526583892,56.5150334190261,56.7647169704841,58.1906393436598,58.5545001647185,60.0792438839859,60.5133346754961,60.7516255534562,61.529367593516,61.9523060638429,63.6103585896212,64.2552920007544,64.7074092835204,65.8893059684956,66.9606623712694,69.4467999091572,69.8534259101284,71.734485553046,73.3462257869747,73.4284192548206,76.6866152292442,76.9430057754852,77.3639680117364,77.4345215258902,77.463329641586,77.527123460008,83.4632688738129,85.6624949075362,89.7019109791551,93.9404980942083,94.9368696417599,104.136935698429],[19.2788431706579,26.2196116863656,27.5910143878873,36.056101565399,40.2322743888062,41.4663542350811,41.8546108052723,42.8444291148101,44.3619893554575,44.6087849852903,45.3175590605443,45.9660072599448,45.9831565191476,46.4346866020878,46.9583817321188,47.230922244704,52.8231997114371,54.4044598144409,54.8184747106387,56.6528554712755,56.9025390227335,58.3284613959093,58.6923222169679,60.2170659362353,60.6511567277456,60.8894476057057,61.6671896457654,62.0901281160924,63.7481806418707,64.3931140530039,64.8452313357699,66.027128020745,67.0984844235189,69.5846219614067,69.9912479623778,71.8723076052955,73.4840478392242,73.5662413070701,76.8244372814937,77.0808278277347,77.5017900639859,77.5723435781397,77.6011516938355,77.6649455122575,83.6010909260623,85.8003169597857,89.8397330314046,94.0783201464578,95.0746916940094,104.274757750679],[19.3918935566512,26.3326620723589,27.7040647738807,36.1691519513924,40.3453247747996,41.5794046210744,41.9676611912657,42.9574795008035,44.4750397414509,44.7218353712837,45.4306094465376,46.0790576459381,46.096206905141,46.5477369880812,47.0714321181122,47.3439726306974,52.9362500974305,54.5175102004343,54.9315250966321,56.7659058572689,57.0155894087269,58.4415117819027,58.8053726029613,60.3301163222287,60.764207113739,61.0024979916991,61.7802400317588,62.2031785020858,63.8612310278641,64.5061644389972,64.9582817217633,66.1401784067384,67.2115348095123,69.6976723474,70.1042983483712,71.9853579912888,73.5970982252176,73.6792916930634,76.9374876674871,77.193878213728,77.6148404499793,77.6853939641331,77.7142020798288,77.7779958982509,83.7141413120557,85.913367345779,89.952783417398,94.1913705324512,95.1877420800027,104.387808136672],[21.0317952269434,27.9725637426511,29.3439664441729,37.8090536216845,41.9852264450918,43.2193062913666,43.6075628615579,44.5973811710957,46.1149414117431,46.3617370415759,47.0705111168298,47.7189593162303,47.7361085754331,48.1876386583733,48.7113337884043,48.9838743009896,54.5761517677226,56.1574118707264,56.5714267669242,58.4058075275611,58.6554910790191,60.0814134521948,60.4452742732535,61.9700179925209,62.4041087840311,62.6423996619913,63.420141702051,63.843080172378,65.5011326981562,66.1460661092894,66.5981833920555,67.7800800770306,68.8514364798045,71.3375740176922,71.7442000186634,73.625259661581,75.2369998955098,75.3191933633556,78.5773893377793,78.8337798840202,79.2547421202715,79.3252956344252,79.354103750121,79.417897568543,85.3540429823479,87.5532690160712,91.5926850876901,95.8312722027433,96.8276437502949,106.027709806964],[23.3195961046361,30.2603646203437,31.6317673218655,40.0968544993772,44.2730273227844,45.5071071690593,45.8953637392505,46.8851820487883,48.4027422894357,48.6495379192685,49.3583119945225,50.006760193923,50.0239094531258,50.475439536066,50.999134666097,51.2716751786822,56.8639526454153,58.4452127484191,58.8592276446169,60.6936084052537,60.9432919567117,62.3692143298875,62.7330751509461,64.2578188702135,64.6919096617238,64.9302005396839,65.7079425797436,66.1308810500706,67.7889335758489,68.4338669869821,68.8859842697481,70.0678809547232,71.1392373574971,73.6253748953849,74.032000896356,75.9130605392737,77.5248007732024,77.6069942410482,80.8651902154719,81.1215807617129,81.5425429979641,81.6130965121179,81.6419046278137,81.7056984462357,87.6418438600405,89.8410698937639,93.8804859653828,98.119073080436,99.1154446279875,108.315510684657],[23.9504317548997,30.8912002706074,32.2626029721291,40.7276901496408,44.9038629730481,46.1379428193229,46.5261993895142,47.5160176990519,49.0335779396993,49.2803735695321,49.9891476447861,50.6375958441866,50.6547451033894,51.1062751863296,51.6299703163606,51.9025108289459,57.4947882956789,59.0760483986827,59.4900632948805,61.3244440555174,61.5741276069753,63.0000499801511,63.3639108012098,64.8886545204771,65.3227453119874,65.5610361899475,66.3387782300073,66.7617167003342,68.4197692261125,69.0647026372457,69.5168199200117,70.6987166049869,71.7700730077607,74.2562105456485,74.6628365466197,76.5438961895373,78.155636423466,78.2378298913119,81.4960258657355,81.7524164119765,82.1733786482277,82.2439321623815,82.2727402780773,82.3365340964993,88.2726795103042,90.4719055440275,94.5113216156464,98.7499087306996,99.7462802782512,108.946346334921],[24.2592289114403,31.199997427148,32.5714001286697,41.0364873061814,45.2126601295886,46.4467399758635,46.8349965460547,47.8248148555925,49.3423750962399,49.5891707260727,50.2979448013267,50.9463930007272,50.96354225993,51.4150723428702,51.9387674729012,52.2113079854864,57.8035854522195,59.3848455552233,59.7988604514211,61.6332412120579,61.8829247635159,63.3088471366917,63.6727079577503,65.1974516770177,65.631542468528,65.8698333464881,66.6475753865478,67.0705138568748,68.7285663826531,69.3734997937863,69.8256170765523,71.0075137615274,72.0788701643013,74.5650077021891,74.9716337031602,76.8526933460779,78.4644335800066,78.5466270478525,81.8048230222761,82.0612135685171,82.4821758047683,82.5527293189221,82.5815374346179,82.6453312530399,88.5814766668448,90.7807027005681,94.820118772187,99.0587058872402,100.055077434792,109.255143491461],[29.7368926597282,36.6776611754359,38.0490638769577,46.5141510544693,50.6903238778766,51.9244037241514,52.3126602943427,53.3024786038805,54.8200388445279,55.0668344743607,55.7756085496146,56.4240567490151,56.4412060082179,56.8927360911581,57.4164312211891,57.6889717337744,63.2812492005074,64.8625093035112,65.276524199709,67.1109049603459,67.3605885118039,68.7865108849796,69.1503717060383,70.6751154253057,71.1092062168159,71.3474970947761,72.1252391348358,72.5481776051628,74.206230130941,74.8511635420742,75.3032808248403,76.4851775098154,77.5565339125893,80.042671450477,80.4492974514482,82.3303570943658,83.9420973282946,84.0242907961404,87.282486770564,87.538877316805,87.9598395530563,88.03039306721,88.0592011829058,88.1229950013278,94.0591404151327,96.258366448856,100.297782520475,104.536369635528,105.53274118308,114.732807239749]],"type":"surface","x":[-3.79160974822952,-3.21180106444372,-2.93442533408609,-2.23948570656194,-1.65689485118863,-1.62443935718435,-1.59611574077461,-1.51158866544863,-1.48078830684263,-1.44485803668215,-1.34982293283585,-1.21162327066926,-1.1194680259074,-0.98585636159149,-0.851693229515672,-0.755733252895172,-0.414059299619805,-0.371847705946829,-0.149982316825593,-0.09401621207894,-0.0687014707382249,0.125891042360704,0.145436500810527,0.163903990695111,0.371981710378547,0.394486532393321,0.396154307276448,0.427033533055787,0.512188899192018,0.646035706452884,0.742678031713295,1.02513796314026,1.18312406031599,1.26496305113097,1.2953802240997,1.40682626830658,1.4158553175941,1.47691249674086,1.5489509747353,1.64546737333353,1.97492388577371,2.05567570010434,2.18990436397892,2.23080449186914,2.26435337007621,2.75101125208859,3.42993996456778,3.61714700692691,3.70878577582133,5.33433948212546],"y":[-3.96625309707395,-3.25357877831655,-3.11276389825864,-2.24357337219482,-1.81476622854792,-1.68805158423357,-1.64818561174969,-1.54655160856301,-1.39072935224815,-1.36538851182604,-1.29261197705307,-1.22602967025068,-1.22426879368609,-1.1779059314858,-1.1241332016812,-1.09614889076506,-0.521936896100953,-0.35957397272494,-0.317063149401063,-0.128709935204398,-0.103072564567768,0.043340365586029,0.0807013958788015,0.23726124785331,0.281833453138403,0.306301030285228,0.386159158000361,0.429586249039983,0.599834176427104,0.666055586690686,0.71247874224825,0.833835248073225,0.943841538120979,1.19911678208591,1.2408689176852,1.43401509400912,1.59950770057417,1.60794728094207,1.94249706331783,1.96882310451407,2.01204727691861,2.01929167320148,2.02224967478038,2.02879998919901,2.63832015677903,2.86413548426478,3.27890051936193,3.71411632837187,3.81642321445712,4.76108097024044],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">7272</span>)
rho&lt;-<span class="fl">0.99</span>
x&lt;-<span class="dv">2</span><span class="op">*</span><span class="kw">mvrnorm</span>(<span class="dt">n=</span>n,<span class="dt">mu=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>),<span class="dt">Sigma=</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,rho,rho,<span class="dv">1</span>),<span class="dt">ncol=</span><span class="dv">2</span>,<span class="dt">byrow=</span>T))
x1&lt;-x[,<span class="dv">1</span>]
x2&lt;-x[,<span class="dv">2</span>]
y&lt;-beta0<span class="op">+</span>beta1<span class="op">*</span>x1<span class="op">+</span>beta2<span class="op">*</span>x2<span class="op">+</span><span class="kw">rnorm</span>(n,<span class="dt">sd=</span>sigma) 

m4&lt;-<span class="kw">lm</span>(y<span class="op">~</span>x1<span class="op">+</span>x2)

xx &lt;-<span class="st"> </span><span class="kw">sort</span>(x1)
yy &lt;-<span class="st"> </span><span class="kw">sort</span>(x2)
fit &lt;-<span class="st"> </span><span class="kw">outer</span>(xx, yy, <span class="cf">function</span>(a, b){<span class="kw">predict</span>(m4,<span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">x1=</span>a,<span class="dt">x2=</span>b))})

axx&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;x1&quot;</span>)
axy&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;x2&quot;</span>)
axz&lt;-<span class="kw">list</span>(<span class="dt">title=</span><span class="st">&quot;y&quot;</span>)

<span class="kw">plot_ly</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">add_markers</span>(<span class="dt">x=</span>x1,
              <span class="dt">y=</span>x2,
              <span class="dt">z=</span>y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_surface</span>(<span class="dt">x=</span><span class="op">~</span>xx,
              <span class="dt">y=</span><span class="op">~</span>yy,
              <span class="dt">z=</span><span class="op">~</span>fit,
              <span class="dt">showscale=</span><span class="ot">FALSE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">layout</span>(<span class="dt">scene =</span> <span class="kw">list</span>(<span class="dt">xaxis=</span>axx,<span class="dt">yaxis=</span>axy,<span class="dt">zaxis=</span>axz))</code></pre></div>
<div id="htmlwidget-166227c9b64406eb14b6" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-166227c9b64406eb14b6">{"x":{"visdat":{"dc123417203":["function () ","plotlyVisDat"]},"cur_data":"dc123417203","attrs":{"dc123417203":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[0.670447232350085,-1.52081986853802,0.551097448896429,2.75809179715889,3.29818281382937,0.180002104549399,-1.99567685626452,-0.585334790655924,0.0917772684483983,0.427746220865478,-0.48893767800831,2.25426561957652,-2.42707453768341,0.101677188693447,3.59313410685851,-2.05796397926283,4.75179660687796,2.16836518421425,-0.548086147958929,-0.592730451548333,2.81320184249814,0.281096441858762,-1.62586985030409,-0.591212166502939,-2.06039261721247,2.56729633087264,2.39687738613548,2.36337331778596,1.9883323826121,2.09722080475462,-1.74115622451856,-4.07597421713483,-3.264857723815,-0.349608871758296,0.845580921775133,-4.44585033319053,-2.11519570831633,-0.543066183934002,3.38067849605031,0.891601535342018,-1.45176684910048,2.41968648637947,-1.00606930056544,-4.22639470950259,1.51694312999049,-1.24040682971251,5.29724181435705,-2.48735664250955,-0.0589047620367544,-0.434049739259119],"y":[0.523420770970757,-1.14148814085572,0.636497404945093,2.8456096015782,2.90455764172021,-0.221381784385977,-1.96519755158619,-0.432422158485776,-0.0444837009711016,0.746512772728603,-0.581798781345011,1.9193599893433,-2.29885890070173,-0.287019362858232,3.46908876110364,-2.60529114605309,4.97966142633536,1.9191957639727,-0.0849868851701209,-0.308006124432938,2.75045974920866,-0.117183796875793,-1.10319298762528,-0.67325988137785,-2.01388979806851,2.71241988398414,2.68480722207595,2.43649694013844,1.85462155057681,1.44377479614985,-1.55068272497971,-4.07102184072386,-2.85151353513735,-0.051043524092399,0.867949000287586,-4.41912104116896,-2.4528864773632,-0.364096589148385,3.05516993496982,0.886023056899303,-1.59716252921171,2.30660907946819,-1.09351877394224,-3.76140022222062,0.979389194830804,-1.21252199204031,5.40128873293331,-2.62781024418413,-0.0586197593960398,-0.761237398137361],"z":[50.6409862363115,37.6188942379965,55.1156164424235,77.5102510295675,87.1233239485785,50.3561833320536,25.1372620699713,39.0191394242595,46.4224241922122,52.2997015553769,46.5263752457263,80.2300454004135,27.087399361868,52.2015264072309,102.070846785859,22.3725435957777,112.989353473564,82.2552873486129,44.3145697493114,45.8100286592282,86.2179419362683,47.5966772030684,28.4384247702086,46.0745083433043,19.5298434735908,82.5305414889222,92.2471871987802,78.2822766455684,70.0870401735438,65.9996396891847,36.4889292390901,-6.50693325250774,11.7862377584625,46.5978064369615,61.6455456355576,0.313379426531433,20.3124310783652,44.8697061109606,90.2140176457216,62.1155582100481,22.950649980682,91.1833364918415,39.2156225795799,-3.08027946925855,66.2656019434205,31.2092572462291,119.677556198975,11.4840757625055,55.3765479957603,36.1724188004595],"type":"scatter3d","mode":"markers","inherit":true},"dc123417203.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"showscale":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"x1"},"yaxis":{"title":"x2"},"zaxis":{"title":"y"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[0.670447232350085,-1.52081986853802,0.551097448896429,2.75809179715889,3.29818281382937,0.180002104549399,-1.99567685626452,-0.585334790655924,0.0917772684483983,0.427746220865478,-0.48893767800831,2.25426561957652,-2.42707453768341,0.101677188693447,3.59313410685851,-2.05796397926283,4.75179660687796,2.16836518421425,-0.548086147958929,-0.592730451548333,2.81320184249814,0.281096441858762,-1.62586985030409,-0.591212166502939,-2.06039261721247,2.56729633087264,2.39687738613548,2.36337331778596,1.9883323826121,2.09722080475462,-1.74115622451856,-4.07597421713483,-3.264857723815,-0.349608871758296,0.845580921775133,-4.44585033319053,-2.11519570831633,-0.543066183934002,3.38067849605031,0.891601535342018,-1.45176684910048,2.41968648637947,-1.00606930056544,-4.22639470950259,1.51694312999049,-1.24040682971251,5.29724181435705,-2.48735664250955,-0.0589047620367544,-0.434049739259119],"y":[0.523420770970757,-1.14148814085572,0.636497404945093,2.8456096015782,2.90455764172021,-0.221381784385977,-1.96519755158619,-0.432422158485776,-0.0444837009711016,0.746512772728603,-0.581798781345011,1.9193599893433,-2.29885890070173,-0.287019362858232,3.46908876110364,-2.60529114605309,4.97966142633536,1.9191957639727,-0.0849868851701209,-0.308006124432938,2.75045974920866,-0.117183796875793,-1.10319298762528,-0.67325988137785,-2.01388979806851,2.71241988398414,2.68480722207595,2.43649694013844,1.85462155057681,1.44377479614985,-1.55068272497971,-4.07102184072386,-2.85151353513735,-0.051043524092399,0.867949000287586,-4.41912104116896,-2.4528864773632,-0.364096589148385,3.05516993496982,0.886023056899303,-1.59716252921171,2.30660907946819,-1.09351877394224,-3.76140022222062,0.979389194830804,-1.21252199204031,5.40128873293331,-2.62781024418413,-0.0586197593960398,-0.761237398137361],"z":[50.6409862363115,37.6188942379965,55.1156164424235,77.5102510295675,87.1233239485785,50.3561833320536,25.1372620699713,39.0191394242595,46.4224241922122,52.2997015553769,46.5263752457263,80.2300454004135,27.087399361868,52.2015264072309,102.070846785859,22.3725435957777,112.989353473564,82.2552873486129,44.3145697493114,45.8100286592282,86.2179419362683,47.5966772030684,28.4384247702086,46.0745083433043,19.5298434735908,82.5305414889222,92.2471871987802,78.2822766455684,70.0870401735438,65.9996396891847,36.4889292390901,-6.50693325250774,11.7862377584625,46.5978064369615,61.6455456355576,0.313379426531433,20.3124310783652,44.8697061109606,90.2140176457216,62.1155582100481,22.950649980682,91.1833364918415,39.2156225795799,-3.08027946925855,66.2656019434205,31.2092572462291,119.677556198975,11.4840757625055,55.3765479957603,36.1724188004595],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"fit","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[-7.30511331684119,-4.52306614368943,-2.04853604594089,5.22337863370135,7.01123998881322,7.19121510285338,8.40925001118325,9.64025537887877,11.9177598528984,12.3069136530431,15.2482901121345,15.6197618299866,18.3223796755377,18.8900900426787,19.1961491277151,19.2734665096424,21.9290958850242,22.6322218947615,23.3631890859368,24.5570235182742,25.103088998419,25.5513701740231,25.7190986854298,26.2436814463433,27.0764432459399,27.3337645126376,27.544493487683,27.6050435951888,27.6574704245169,32.196225551693,33.0999464723259,33.9792014161867,34.9497331300393,35.0941829823794,35.8403754463905,39.5517963509778,42.8353288908129,43.3514131208451,43.3527256281058,46.4476630341958,47.4857411313082,49.4702642702452,49.690947707342,49.9949665025005,50.7554146088794,51.2265338422193,52.4302438884969,55.7383287639791,67.8109919811732,71.1806838820583],[-6.21841594876296,-3.4363687756112,-0.961838677862659,6.31007600177958,8.09793735689144,8.27791247093161,9.49594737926147,10.726952746957,13.0044572209766,13.3936110211213,16.3349874802127,16.7064591980648,19.4090770436159,19.9767874107569,20.2828464957933,20.3601638777206,23.0157932531024,23.7189192628397,24.449886454015,25.6437208863524,26.1897863664972,26.6380675421013,26.8057960535081,27.3303788144215,28.1631406140182,28.4204618807158,28.6311908557612,28.691740963267,28.7441677925951,33.2829229197712,34.1866438404041,35.0658987842649,36.0364304981175,36.1808803504576,36.9270728144687,40.638493719056,43.9220262588912,44.4381104889234,44.439422996184,47.5343604022741,48.5724384993864,50.5569616383234,50.7776450754202,51.0816638705787,51.8421119769576,52.3132312102975,53.5169412565751,56.8250261320573,68.8976893492514,72.2673812501366],[-5.47356580359873,-2.69151863044697,-0.216988532698426,7.05492614694381,8.84278750205567,9.02276261609584,10.2407975244257,11.4718028921212,13.7493073661408,14.1384611662855,17.079837625377,17.4513093432291,20.1539271887802,20.7216375559211,21.0276966409576,21.1050140228848,23.7606433982667,24.463769408004,25.1947365991792,26.3885710315167,26.9346365116615,27.3829176872656,27.5506461986723,28.0752289595858,28.9079907591824,29.1653120258801,29.3760410009255,29.4365911084313,29.4890179377593,34.0277730649354,34.9314939855683,35.8107489294292,36.7812806432817,36.9257304956218,37.671922959633,41.3833438642202,44.6668764040554,45.1829606340876,45.1842731413483,48.2792105474383,49.3172886445507,51.3018117834877,51.5224952205844,51.8265140157429,52.5869621221219,53.0580813554617,54.2617914017393,57.5698762772216,69.6425394944157,73.0122313953008],[-1.45709020054522,1.32495697260654,3.79948707035509,11.0714017499973,12.8592631051092,13.0392382191494,14.2572731274792,15.4882784951747,17.7657829691944,18.1549367693391,21.0963132284305,21.4677849462826,24.1704027918337,24.7381131589746,25.0441722440111,25.1214896259383,27.7771190013202,28.4802450110575,29.2112122022328,30.4050466345702,30.951112114715,31.3993932903191,31.5671218017258,32.0917045626393,32.9244663622359,33.1817876289336,33.392516603979,33.4530667114848,33.5054935408129,38.0442486679889,38.9479695886218,39.8272245324827,40.7977562463352,40.9422060986754,41.6883985626865,45.3998194672737,48.6833520071089,49.1994362371411,49.2007487444018,52.2956861504918,53.3337642476042,55.3182873865412,55.5389708236379,55.8429896187965,56.6034377251754,57.0745569585153,58.2782670047929,61.5863518802751,73.6590150974692,77.0287069983543],[2.39292906318555,5.17497623633731,7.64950633408585,14.9214210137281,16.70928236884,16.8892574828801,18.10729239121,19.3382977589055,21.6158022329251,22.0049560330698,24.9463324921613,25.3178042100134,28.0204220555644,28.5881324227054,28.8941915077419,28.9715088896691,31.627138265051,32.3302642747883,33.0612314659635,34.255065898301,34.8011313784458,35.2494125540499,35.4171410654566,35.94172382637,36.7744856259667,37.0318068926644,37.2425358677098,37.3030859752155,37.3555128045436,41.8942679317197,42.7979888523526,43.6772437962135,44.647775510066,44.7922253624061,45.5384178264173,49.2498387310045,52.5333712708397,53.0494555008719,53.0507680081326,56.1457054142226,57.183783511335,59.168306650272,59.3889900873687,59.6930088825272,60.4534569889062,60.924576222246,62.1282862685236,65.4363711440059,77.5090343612,80.8787262620851],[2.69143316873508,5.47348034188684,7.94801043963539,15.2199251192776,17.0077864743895,17.1877615884297,18.4057964967595,19.636801864455,21.9143063384747,22.3034601386194,25.2448365977108,25.6163083155629,28.318926161114,28.8866365282549,29.1926956132914,29.2700129952186,31.9256423706005,32.6287683803378,33.3597355715131,34.5535700038505,35.0996354839953,35.5479166595994,35.7156451710061,36.2402279319196,37.0729897315162,37.3303109982139,37.5410399732593,37.6015900807651,37.6540169100932,42.1927720372692,43.0964929579021,43.975747901763,44.9462796156155,45.0907294679557,45.8369219319668,49.548342836554,52.8318753763892,53.3479596064214,53.3492721136821,56.4442095197721,57.4822876168845,59.4668107558215,59.6874941929182,59.9915129880768,60.7519610944557,61.2230803277956,62.4267903740732,65.7348752495554,77.8075384667495,81.1772303676346],[4.23579050775892,7.01783768091068,9.49236777865923,16.7642824583015,18.5521438134133,18.7321189274535,19.9501538357834,21.1811592034789,23.4586636774985,23.8478174776432,26.7891939367346,27.1606656545867,29.8632835001378,30.4309938672788,30.7370529523152,30.8143703342425,33.4699997096243,34.1731257193616,34.9040929105369,36.0979273428743,36.6439928230191,37.0922739986232,37.2600025100299,37.7845852709434,38.61734707054,38.8746683372377,39.0853973122831,39.1459474197889,39.198374249117,43.7371293762931,44.640850296926,45.5201052407868,46.4906369546394,46.6350868069795,47.3812792709906,51.0927001755779,54.3762327154131,54.8923169454452,54.8936294527059,57.9885668587959,59.0266449559083,61.0111680948453,61.2318515319421,61.5358703271006,62.2963184334795,62.7674376668194,63.971147713097,67.2792325885792,79.3518958057733,82.7215877066584],[4.50716370779047,7.28921088094223,9.76374097869077,17.035655658333,18.8235170134449,19.003492127485,20.2215270358149,21.4525324035104,23.73003687753,24.1191906776747,27.0605671367662,27.4320388546183,30.1346567001694,30.7023670673103,31.0084261523468,31.085743534274,33.7413729096559,34.4444989193932,35.1754661105684,36.3693005429059,36.9153660230507,37.3636471986548,37.5313757100615,38.055958470975,38.8887202705716,39.1460415372693,39.3567705123147,39.4173206198205,39.4697474491485,44.0085025763246,44.9122234969575,45.7914784408184,46.7620101546709,46.906460007011,47.6526524710222,51.3640733756094,54.6476059154446,55.1636901454768,55.1650026527375,58.2599400588275,59.2980181559399,61.2825412948769,61.5032247319736,61.8072435271321,62.5676916335111,63.0388108668509,64.2425209131285,67.5506057886108,79.6232690058049,82.99296090669],[4.51918980410797,7.30123697725973,9.77576707500828,17.0476817546505,18.8355431097624,19.0155182238025,20.2335531321324,21.4645584998279,23.7420629738475,24.1312167739923,27.0725932330837,27.4440649509358,30.1466827964869,30.7143931636278,31.0204522486643,31.0977696305915,33.7533990059734,34.4565250157107,35.1874922068859,36.3813266392234,36.9273921193682,37.3756732949723,37.543401806379,38.0679845672925,38.9007463668891,39.1580676335868,39.3687966086322,39.429346716138,39.4817735454661,44.0205286726421,44.924249593275,45.8035045371359,46.7740362509884,46.9184861033286,47.6646785673397,51.3760994719269,54.6596320117621,55.1757162417943,55.177028749055,58.271966155145,59.3100442522574,61.2945673911944,61.5152508282911,61.8192696234496,62.5797177298286,63.0508369631685,64.254547009446,67.5626318849283,79.6352951021224,83.0049870030075],[4.82762233133123,7.60966950448299,10.0841996022315,17.3561142818738,19.1439756369856,19.3239507510258,20.5419856593557,21.7729910270512,24.0504955010708,24.4396493012155,27.3810257603069,27.752497478159,30.4551153237101,31.0228256908511,31.3288847758875,31.4062021578148,34.0618315331966,34.7649575429339,35.4959247341092,36.6897591664466,37.2358246465914,37.6841058221955,37.8518343336022,38.3764170945157,39.2091788941124,39.46650016081,39.6772291358554,39.7377792433612,39.7902060726893,44.3289611998654,45.2326821204983,46.1119370643591,47.0824687782117,47.2269186305518,47.9731110945629,51.6845319991502,54.9680645389854,55.4841487690176,55.4854612762782,58.5803986823682,59.6184767794806,61.6029999184176,61.8236833555144,62.1277021506729,62.8881502570518,63.3592694903917,64.5629795366693,67.8710644121515,79.9437276293456,83.3134195302307],[6.08795412867556,8.87000130182732,11.3445313995759,18.6164460792181,20.40430743433,20.5842825483701,21.8023174567,23.0333228243955,25.3108272984151,25.6999810985598,28.6413575576513,29.0128292755034,31.7154471210544,32.2831574881954,32.5892165732319,32.6665339551591,35.322163330541,36.0252893402783,36.7562565314535,37.950090963791,38.4961564439358,38.9444376195399,39.1121661309466,39.6367488918601,40.4695106914567,40.7268319581544,40.9375609331998,40.9981110407055,41.0505378700336,45.5892929972097,46.4930139178426,47.3722688617035,48.342800575556,48.4872504278961,49.2334428919073,52.9448637964945,56.2283963363297,56.7444805663619,56.7457930736226,59.8407304797126,60.878808576825,62.863331715762,63.0840151528587,63.3880339480172,64.1484820543962,64.619601287736,65.8233113340136,69.1313962094959,81.20405942669,84.5737513275751],[6.65882762614537,9.44087479929713,11.9154048970457,19.1873195766879,20.9751809317998,21.1551560458399,22.3731909541698,23.6041963218653,25.8817007958849,26.2708545960296,29.2122310551211,29.5837027729732,32.2863206185243,32.8540309856652,33.1600900707017,33.2374074526289,35.8930368280108,36.5961628377481,37.3271300289233,38.5209644612608,39.0670299414056,39.5153111170097,39.6830396284164,40.2076223893299,41.0403841889265,41.2977054556242,41.5084344306696,41.5689845381753,41.6214113675034,46.1601664946795,47.0638874153124,47.9431423591733,48.9136740730258,49.0581239253659,49.8043163893771,53.5157372939643,56.7992698337995,57.3153540638317,57.3166665710924,60.4116039771824,61.4496820742948,63.4342052132318,63.6548886503285,63.958907445487,64.719355551866,65.1904747852059,66.3941848314834,69.7022697069657,81.7749329241598,85.1446248250449],[7.17901269492344,9.9610598680752,12.4355899658237,19.707504645466,21.4953660005778,21.675341114618,22.8933760229479,24.1243813906434,26.401885864663,26.7910396648077,29.7324161238991,30.1038878417513,32.8065056873023,33.3742160544433,33.6802751394798,33.757592521407,36.4132218967888,37.1163479065262,37.8473150977014,39.0411495300389,39.5872150101836,40.0354961857878,40.2032246971945,40.7278074581079,41.5605692577046,41.8178905244023,42.0286194994477,42.0891696069534,42.1415964362815,46.6803515634576,47.5840724840905,48.4633274279514,49.4338591418039,49.578308994144,50.3245014581552,54.0359223627424,57.3194549025776,57.8355391326098,57.8368516398705,60.9317890459605,61.9698671430728,63.9543902820098,64.1750737191066,64.4790925142651,65.239540620644,65.7106598539839,66.9143699002615,70.2224547757438,82.2951179929378,85.664809893823],[7.52094849597332,10.3029956691251,12.7775257668736,20.0494404465159,21.8373018016277,22.0172769156679,23.2353118239978,24.4663171916933,26.7438216657129,27.1329754658576,30.074351924949,30.4458236428011,33.1484414883522,33.7161518554932,34.0222109405296,34.0995283224569,36.7551576978387,37.458283707576,38.1892508987513,39.3830853310887,39.9291508112335,40.3774319868376,40.5451604982443,41.0697432591578,41.9025050587544,42.1598263254521,42.3705553004975,42.4311054080033,42.4835322373314,47.0222873645075,47.9260082851404,48.8052632290012,49.7757949428538,49.9202447951939,50.666437259205,54.3778581637923,57.6613907036275,58.1774749336596,58.1787874409203,61.2737248470103,62.3118029441227,64.2963260830597,64.5170095201565,64.821028315315,65.5814764216939,66.0525956550338,67.2563057013114,70.5643905767936,82.6370537939877,86.0067456948728],[8.56755816095269,11.3496053341044,13.824135431853,21.0960501114952,22.8839114666071,23.0638865806473,24.2819214889771,25.5129268566726,27.7904313306923,28.179585130837,31.1209615899284,31.4924333077805,34.1950511533316,34.7627615204725,35.068820605509,35.1461379874362,37.8017673628181,38.5048933725554,39.2358605637307,40.4296949960681,40.9757604762129,41.424041651817,41.5917701632237,42.1163529241372,42.9491147237338,43.2064359904315,43.4171649654769,43.4777150729827,43.5301419023108,48.0688970294868,48.9726179501197,49.8518728939806,50.8224046078331,50.9668544601733,51.7130469241844,55.4244678287716,58.7080003686068,59.224084598639,59.2253971058997,62.3203345119897,63.3584126091021,65.3429357480391,65.5636191851358,65.8676379802944,66.6280860866733,67.0992053200132,68.3029153662908,71.611000241773,83.6836634589671,87.0533553598522],[9.72794754577059,12.5099947189223,14.9845248166709,22.2564394963131,24.044300851425,24.2242759654652,25.442310873795,26.6733162414906,28.9508207155102,29.3399745156549,32.2813509747463,32.6528226925984,35.3554405381495,35.9231509052904,36.2292099903269,36.3065273722541,38.962156747636,39.6652827573733,40.3962499485486,41.590084380886,42.1361498610308,42.5844310366349,42.7521595480416,43.2767423089551,44.1095041085517,44.3668253752494,44.5775543502948,44.6381044578006,44.6905312871287,49.2292864143048,50.1330073349376,51.0122622787985,51.982793992651,52.1272438449912,52.8734363090023,56.5848572135895,59.8683897534247,60.3844739834569,60.3857864907176,63.4807238968076,64.51880199392,66.503325132857,66.7240085699538,67.0280273651123,67.7884754714912,68.2595947048311,69.4633047511087,72.7713896265909,84.844052843785,88.2137447446701],[11.7747132280667,14.5567604012185,17.031290498967,24.3032051786093,26.0910665337211,26.2710416477613,27.4890765560912,28.7200819237867,30.9975863978063,31.386740197951,34.3281166570424,34.6995883748945,37.4022062204456,37.9699165875866,38.275975672623,38.3532930545503,41.0089224299321,41.7120484396694,42.4430156308447,43.6368500631821,44.1829155433269,44.631196718931,44.7989252303377,45.3235079912512,46.1562697908479,46.4135910575455,46.6243200325909,46.6848701400967,46.7372969694248,51.2760520966009,52.1797730172338,53.0590279610946,54.0295596749472,54.1740095272873,54.9202019912984,58.6316228958857,61.9151554357209,62.4312396657531,62.4325521730137,65.5274895791038,66.5655676762161,68.5500908151531,68.7707742522499,69.0747930474084,69.8352411537873,70.3063603871272,71.5100704334048,74.818155308887,86.8908185260811,90.2605104269662],[11.7822314512733,14.5642786244251,17.0388087221736,24.3107234018158,26.0985847569277,26.2785598709679,27.4965947792977,28.7276001469933,31.0051046210129,31.3942584211576,34.335634880249,34.7071065981011,37.4097244436522,37.9774348107932,38.2834938958296,38.3608112777568,41.0164406531387,41.719566662876,42.4505338540513,43.6443682863887,44.1904337665335,44.6387149421376,44.8064434535443,45.3310262144578,46.1637880140544,46.4211092807521,46.6318382557975,46.6923883633033,46.7448151926314,51.2835703198075,52.1872912404404,53.0665461843012,54.0370778981538,54.1815277504939,54.927720214505,58.6391411190922,61.9226736589274,62.4387578889596,62.4400703962203,65.5350078023103,66.5730858994227,68.5576090383597,68.7782924754565,69.082311270615,69.8427593769939,70.3138786103338,71.5175886566114,74.8256735320936,86.8983367492877,90.2680286501728],[11.8113349609343,14.593382134086,17.0679122318346,24.3398269114768,26.1276882665887,26.3076633806288,27.5256982889587,28.7567036566542,31.0342081306738,31.4233619308186,34.36473838991,34.7362101077621,37.4388279533132,38.0065383204541,38.3125974054906,38.3899147874178,41.0455441627997,41.748670172537,42.4796373637122,43.6734717960497,44.2195372761945,44.6678184517986,44.8355469632053,45.3601297241188,46.1928915237154,46.4502127904131,46.6609417654585,46.7214918729643,46.7739187022924,51.3126738294684,52.2163947501013,53.0956496939622,54.0661814078147,54.2106312601549,54.956823724166,58.6682446287532,61.9517771685884,62.4678613986206,62.4691739058813,65.5641113119713,66.6021894090837,68.5867125480207,68.8073959851174,69.111414780276,69.8718628866549,70.3429821199948,71.5466921662724,74.8547770417546,86.9274402589487,90.2971321598338],[11.9957822824614,14.7778294556132,17.2523595533617,24.524274233004,26.3121355881158,26.492110702156,27.7101456104859,28.9411509781814,31.218655452201,31.6078092523457,34.5491857114371,34.9206574292892,37.6232752748403,38.1909856419813,38.4970447270177,38.574362108945,41.2299914843268,41.9331174940641,42.6640846852394,43.8579191175768,44.4039845977216,44.8522657733257,45.0199942847324,45.5445770456459,46.3773388452425,46.6346601119402,46.8453890869856,46.9059391944914,46.9583660238195,51.4971211509956,52.4008420716285,53.2800970154893,54.2506287293419,54.395078581682,55.1412710456931,58.8526919502804,62.1362244901156,62.6523087201477,62.6536212274084,65.7485586334984,66.7866367306108,68.7711598695478,68.9918433066446,69.2958621018031,70.056310208182,70.5274294415219,71.7311394877995,75.0392243632817,87.1118875804758,90.4815794813609],[12.020640071941,14.8026872450927,17.2772173428413,24.5491320224835,26.3369933775954,26.5169684916355,27.7350033999654,28.9660087676609,31.2435132416805,31.6326670418252,34.5740435009167,34.9455152187688,37.6481330643199,38.2158434314608,38.5219025164973,38.5992198984245,41.2548492738064,41.9579752835437,42.6889424747189,43.8827769070564,44.4288423872012,44.8771235628053,45.044852074212,45.5694348351255,46.4021966347221,46.6595179014198,46.8702468764652,46.930796983971,46.983223813299,51.5219789404751,52.425699861108,53.3049548049689,54.2754865188214,54.4199363711616,55.1661288351727,58.8775497397599,62.1610822795951,62.6771665096273,62.678479016888,65.773416422978,66.8114945200904,68.7960176590274,69.0167010961241,69.3207198912826,70.0811679976616,70.5522872310014,71.7559972772791,75.0640821527613,87.1367453699554,90.5064372708405],[12.2886728702761,15.0707200434278,17.5452501411764,24.8171648208186,26.6050261759305,26.7850012899707,28.0030361983005,29.234041565996,31.5115460400157,31.9006998401604,34.8420762992518,35.2135480171039,37.916165862655,38.4838762297959,38.7899353148324,38.8672526967596,41.5228820721415,42.2260080818788,42.9569752730541,44.1508097053915,44.6968751855363,45.1451563611404,45.3128848725471,45.8374676334606,46.6702294330572,46.9275506997549,47.1382796748003,47.1988297823061,47.2512566116342,51.7900117388102,52.6937326594431,53.572987603304,54.5435193171565,54.6879691694967,55.4341616335078,59.145582538095,62.4291150779302,62.9451993079624,62.9465118152231,66.0414492213131,67.0795273184255,69.0640504573625,69.2847338944592,69.5887526896178,70.3492007959967,70.8203200293366,72.0240300756141,75.3321149510964,87.4047781682905,90.7744700691756],[12.560466217722,15.3425133908738,17.8170434886223,25.0889581682646,26.8768195233764,27.0567946374166,28.2748295457465,29.505834913442,31.7833393874616,32.1724931876063,35.1138696466977,35.4853413645498,38.1879592101009,38.7556695772419,39.0617286622783,39.1390460442056,41.7946754195874,42.4978014293247,43.2287686205,44.4226030528374,44.9686685329822,45.4169497085863,45.584678219993,46.1092609809065,46.9420227805032,47.1993440472008,47.4100730222462,47.470623129752,47.5230499590801,52.0618050862562,52.9655260068891,53.8447809507499,54.8153126646025,54.9597625169426,55.7059549809537,59.417375885541,62.7009084253762,63.2169926554084,63.218305162669,66.3132425687591,67.3513206658714,69.3358438048084,69.5565272419052,69.8605460370637,70.6209941434426,71.0921133767825,72.2958234230601,75.6039082985423,87.6765715157364,91.0462634166215],[12.9785993552071,15.7606465283589,18.2351766261074,25.5070913057497,27.2949526608615,27.4749277749017,28.6929626832316,29.9239680509271,32.2014725249467,32.5906263250914,35.5320027841828,35.9034745020349,38.606092347586,39.173802714727,39.4798617997634,39.5571791816907,42.2128085570725,42.9159345668098,43.6469017579851,44.8407361903225,45.3868016704673,45.8350828460714,46.0028113574781,46.5273941183916,47.3601559179883,47.6174771846859,47.8282061597313,47.8887562672371,47.9411830965652,52.4799382237413,53.3836591443742,54.262914088235,55.2334458020876,55.3778956544277,56.1240881184388,59.8355090230261,63.1190415628613,63.6351257928935,63.6364383001541,66.7313757062442,67.7694538033565,69.7539769422935,69.9746603793903,70.2786791745488,71.0391272809277,71.5102465142676,72.7139565605452,76.0220414360274,88.0947046532215,91.4643965541066],[14.4181040059202,17.200151179072,19.6746812768205,26.9465959564627,28.7344573115746,28.9144324256148,30.1324673339446,31.3634727016402,33.6409771756598,34.0301309758045,36.9715074348959,37.342979152748,40.0455969982991,40.6133073654401,40.9193664504765,40.9966838324037,43.6523132077856,44.3554392175229,45.0864064086982,46.2802408410356,46.8263063211804,47.2745874967845,47.4423160081912,47.9668987691047,48.7996605687013,49.056981835399,49.2677108104444,49.3282609179502,49.3806877472783,53.9194428744544,54.8231637950872,55.7024187389481,56.6729504528006,56.8174003051408,57.5635927691519,61.2750136737391,64.5585462135743,65.0746304436065,65.0759429508672,68.1708803569572,69.2089584540696,71.1934815930066,71.4141650301034,71.7181838252619,72.4786319316408,72.9497511649807,74.1534612112583,77.4615460867405,89.5342093039346,92.9039012048197],[15.1642492319716,17.9462964051234,20.4208265028719,27.6927411825142,29.480602537626,29.6605776516662,30.8786125599961,32.1096179276916,34.3871224017112,34.7762762018559,37.7176526609473,38.0891243787995,40.7917422243505,41.3594525914915,41.6655116765279,41.7428290584552,44.398458433837,45.1015844435743,45.8325516347496,47.026386067087,47.5724515472318,48.020732722836,48.1884612342427,48.7130439951561,49.5458057947528,49.8031270614504,50.0138560364958,50.0744061440016,50.1268329733297,54.6655881005058,55.5693090211387,56.4485639649996,57.4190956788521,57.5635455311922,58.3097379952033,62.0211588997906,65.3046914396258,65.820775669658,65.8220881769187,68.9170255830087,69.955103680121,71.939626819058,72.1603102561548,72.4643290513133,73.2247771576922,73.6958963910321,74.8996064373097,78.207691312792,90.280354529986,93.6500464308712],[15.21327152219,17.9953186953418,20.4698487930903,27.7417634727325,29.5296248278444,29.7095999418846,30.9276348502144,32.15864021791,34.4361446919296,34.8252984920743,37.7666749511657,38.1381466690178,40.8407645145689,41.4084748817099,41.7145339667463,41.7918513486735,44.4474807240554,45.1506067337927,45.881573924968,47.0754083573054,47.6214738374502,48.0697550130543,48.237483524461,48.7620662853745,49.5948280849711,49.8521493516688,50.0628783267142,50.12342843422,50.1758552635481,54.7146103907242,55.618331311357,56.4975862552179,57.4681179690704,57.6125678214106,58.3587602854217,62.0701811900089,65.3537137298441,65.8697979598763,65.871110467137,68.966047873227,70.0041259703394,71.9886491092764,72.2093325463732,72.5133513415317,73.2737994479106,73.7449186812505,74.9486287275281,78.2567136030103,90.3293768202044,93.6990687210895],[15.601119773628,18.3831669467798,20.8576970445283,28.1296117241706,29.9174730792824,30.0974481933226,31.3154831016525,32.546488469348,34.8239929433676,35.2131467435123,38.1545232026037,38.5259949204558,41.2286127660069,41.7963231331479,42.1023822181843,42.1796996001116,44.8353289754934,45.5384549852307,46.269422176406,47.4632566087434,48.0093220888882,48.4576032644923,48.625331775899,49.1499145368125,49.9826763364092,50.2399976031068,50.4507265781522,50.511276685658,50.5637035149861,55.1024586421622,56.0061795627951,56.8854345066559,57.8559662205085,58.0004160728486,58.7466085368597,62.458029441447,65.7415619812822,66.2576462113144,66.258958718575,69.3538961246651,70.3919742217774,72.3764973607144,72.5971807978112,72.9011995929697,73.6616476993486,74.1327669326885,75.3364769789661,78.6445618544483,90.7172250716424,94.0869169725275],[16.101717336067,18.8837645092187,21.3582946069673,28.6302092866095,30.4180706417214,30.5980457557615,31.8160806640914,33.0470860317869,35.3245905058065,35.7137443059512,38.6551207650427,39.0265924828948,41.7292103284458,42.2969206955868,42.6029797806233,42.6802971625505,45.3359265379324,46.0390525476697,46.7700197388449,47.9638541711824,48.5099196513272,48.9582008269313,49.125929338338,49.6505120992514,50.4832738988481,50.7405951655458,50.9513241405912,51.0118742480969,51.064301077425,55.6030562046011,56.506777125234,57.3860320690949,58.3565637829474,58.5010136352875,59.2472060992987,62.9586270038859,66.2421595437211,66.7582437737533,66.759556281014,69.854493687104,70.8925717842164,72.8770949231534,73.0977783602501,73.4017971554086,74.1622452617876,74.6333644951274,75.837074541405,79.1451594168873,91.2178226340814,94.5875145349665],[16.8278957141725,19.6099428873243,22.0844729850728,29.3563876647151,31.1442490198269,31.3242241338671,32.5422590421969,33.7732644098925,36.0507688839121,36.4399226840568,39.3812991431482,39.7527708610003,42.4553887065514,43.0230990736924,43.3291581587288,43.4064755406561,46.0621049160379,46.7652309257752,47.4961981169505,48.6900325492879,49.2360980294327,49.6843792050368,49.8521077164435,50.376690477357,51.2094522769536,51.4667735436513,51.6775025186967,51.7380526262025,51.7904794555306,56.3292345827067,57.2329555033396,58.1122104472004,59.082742161053,59.2271920133931,59.9733844774042,63.6848053819915,66.9683379218266,67.4844221518588,67.4857346591195,70.5806720652095,71.6187501623219,73.6032733012589,73.8239567383557,74.1279755335142,74.8884236398931,75.359542873233,76.5632529195106,79.8713377949928,91.9440010121869,95.313692913072],[17.4387046449159,20.2207518180677,22.6952819158162,29.9671965954584,31.7550579505703,31.9350330646105,33.1530679729403,34.3840733406359,36.6615778146555,37.0507316148002,39.9921080738916,40.3635797917437,43.0661976372948,43.6339080044358,43.9399670894722,44.0172844713994,46.6729138467813,47.3760398565186,48.1070070476939,49.3008414800313,49.8469069601761,50.2951881357802,50.4629166471869,50.9874994081004,51.820261207697,52.0775824743947,52.2883114494401,52.3488615569459,52.401288386274,56.9400435134501,57.8437644340829,58.7230193779438,59.6935510917964,59.8380009441365,60.5841934081476,64.2956143127348,67.57914685257,68.0952310826022,68.0965435898629,71.1914809959529,72.2295590930653,74.2140822320023,74.4347656690991,74.7387844642576,75.4992325706365,75.9703518039764,77.174061850254,80.4821467257362,92.5548099429303,95.9245018438154],[18.0296992769068,20.8117464500586,23.2862765478071,30.5581912274494,32.3460525825612,32.5260276966014,33.7440626049313,34.9750679726268,37.2525724466464,37.6417262467911,40.5831027058825,40.9545744237346,43.6571922692857,44.2249026364267,44.5309617214631,44.6082791033904,47.2639084787722,47.9670344885095,48.6980016796848,49.8918361120222,50.437901592167,50.8861827677711,51.0539112791778,51.5784940400913,52.411255839688,52.6685771063856,52.879306081431,52.9398561889368,52.9922830182649,57.531038145441,58.4347590660739,59.3140140099347,60.2845457237873,60.4289955761274,61.1751880401385,64.8866089447258,68.170141484561,68.6862257145932,68.6875382218539,71.7824756279439,72.8205537250562,74.8050768639932,75.02576030109,75.3297790962485,76.0902272026274,76.5613464359673,77.7650564822449,81.0731413577271,93.1458045749212,96.5154964758063],[18.8969238946303,21.678971067782,24.1535011655306,31.4254158451728,33.2132772002847,33.3932523143248,34.6112872226547,35.8422925903502,38.1197970643698,38.5089508645145,41.450327323606,41.8217990414581,44.5244168870091,45.0921272541501,45.3981863391866,45.4755037211138,48.1311330964957,48.834259106233,49.5652262974082,50.7590607297457,51.3051262098905,51.7534073854946,51.9211358969013,52.4457186578148,53.2784804574114,53.5358017241091,53.7465306991545,53.8070808066602,53.8595076359883,58.3982627631644,59.3019836837973,60.1812386276582,61.1517703415107,61.2962201938508,62.042412657862,65.7538335624492,69.0373661022844,69.5534503323166,69.5547628395773,72.6497002456673,73.6877783427797,75.6723014817167,75.8929849188134,76.1970037139719,76.9574518203509,77.4285710536907,78.6322810999683,81.9403659754506,94.0130291926447,97.3827210935298],[19.1248081420241,21.9068553151759,24.3813854129244,31.6533000925667,33.4411614476785,33.6211365617187,34.8391714700486,36.0701768377441,38.3476813117637,38.7368351119084,41.6782115709998,42.049683288852,44.752301134403,45.320011501544,45.6260705865804,45.7033879685077,48.3590173438895,49.0621433536268,49.7931105448021,50.9869449771395,51.5330104572843,51.9812916328884,52.1490201442952,52.6736029052086,53.5063647048053,53.7636859715029,53.9744149465483,54.0349650540541,54.0873918833822,58.6261470105583,59.5298679311912,60.409122875052,61.3796545889046,61.5241044412447,62.2702969052558,65.9817178098431,69.2652503496783,69.7813345797105,69.7826470869712,72.8775844930612,73.9156625901735,75.9001857291105,76.1208691662073,76.4248879613658,77.1853360677447,77.6564553010846,78.8601653473622,82.1682502228444,94.2409134400385,97.6106053409237],[22.2213661323568,25.0034133055086,27.4779434032571,34.7498580828994,36.5377194380113,36.7176945520514,37.9357294603813,39.1667348280768,41.4442393020964,41.8333931022411,44.7747695613325,45.1462412791847,47.8488591247357,48.4165694918767,48.7226285769132,48.7999459588404,51.4555753342222,52.1587013439595,52.8896685351348,54.0835029674722,54.629568447617,55.0778496232212,55.2455781346279,55.7701608955413,56.602922695138,56.8602439618356,57.070972936881,57.1315230443868,57.1839498737149,61.722705000891,62.6264259215239,63.5056808653848,64.4762125792373,64.6206624315774,65.3668548955885,69.0782758001758,72.361808340011,72.8778925700432,72.8792050773039,75.9741424833939,77.0122205805062,78.9967437194432,79.21742715654,79.5214459516985,80.2818940580775,80.7530132914173,81.9567233376949,85.2648082131771,97.3374714303712,100.707163331256],[24.5555850124922,27.337632185644,29.8121622833925,37.0840769630348,38.8719383181466,39.0519134321868,40.2699483405167,41.5009537082122,43.7784581822318,44.1676119823765,47.1089884414679,47.48046015932,50.1830780048711,50.7507883720121,51.0568474570485,51.1341648389758,53.7897942143576,54.4929202240949,55.2238874152702,56.4177218476076,56.9637873277524,57.4120685033565,57.5797970147632,58.1043797756767,58.9371415752734,59.194462841971,59.4051918170164,59.4657419245222,59.5181687538503,64.0569238810264,64.9606448016593,65.8398997455201,66.8104314593727,66.9548813117128,67.7010737757239,71.4124946803112,74.6960272201464,75.2121114501786,75.2134239574392,78.3083613635292,79.3464394606416,81.3309625995786,81.5516460366754,81.8556648318339,82.6161129382128,83.0872321715527,84.2909422178303,87.5990270933125,99.6716903105066,103.041382211392],[25.0947772180414,27.8768243911931,30.3513544889417,37.6232691685839,39.4111305236958,39.5911056377359,40.8091405460658,42.0401459137613,44.3176503877809,44.7068041879256,47.6481806470171,48.0196523648692,50.7222702104202,51.2899805775612,51.5960396625977,51.6733570445249,54.3289864199068,55.0321124296441,55.7630796208193,56.9569140531568,57.5029795333016,57.9512607089057,58.1189892203124,58.6435719812258,59.4763337808225,59.7336550475202,59.9443840225656,60.0049341300713,60.0573609593994,64.5961160865755,65.4998370072084,66.3790919510693,67.3496236649218,67.4940735172619,68.2402659812731,71.9516868858603,75.2352194256955,75.7513036557277,75.7526161629884,78.8475535690784,79.8856316661908,81.8701548051278,82.0908382422245,82.394857037383,83.155305143762,83.6264243771018,84.8301344233794,88.1382192988617,100.210882516056,103.580574416941],[25.4470689871613,28.2291161603131,30.7036462580616,37.9755609377038,39.7634222928157,39.9433974068559,41.1614323151857,42.3924376828813,44.6699421569009,45.0590959570456,48.000472416137,48.3719441339891,51.0745619795402,51.6422723466812,51.9483314317176,52.0256488136448,54.6812781890267,55.384404198764,56.1153713899393,57.3092058222767,57.8552713024215,58.3035524780256,58.4712809894323,58.9958637503458,59.8286255499424,60.0859468166401,60.2966757916855,60.3572258991913,60.4096527285194,64.9484078556955,65.8521287763284,66.7313837201892,67.7019154340418,67.8463652863819,68.592557750393,72.3039786549803,75.5875111948154,76.1035954248476,76.1049079321083,79.1998453381983,80.2379234353107,82.2224465742477,82.4431300113445,82.747148806503,83.5075969128819,83.9787161462218,85.1824261924994,88.4905110679816,100.563174285176,103.932866186061],[25.8724295929013,28.654476766053,31.1290068638016,38.4009215434438,40.1887828985557,40.3687580125958,41.5867929209257,42.8177982886212,45.0953027626408,45.4844565627856,48.425833021877,48.7973047397291,51.4999225852802,52.0676329524211,52.3736920374576,52.4510094193848,55.1066387947667,55.809764804504,56.5407319956793,57.7345664280167,58.2806319081615,58.7289130837656,58.8966415951723,59.4212243560858,60.2539861556824,60.5113074223801,60.7220363974255,60.7825865049313,60.8350133342594,65.3737684614354,66.2774893820683,67.1567443259292,68.1272760397817,68.2717258921219,69.017918356133,72.7293392607202,76.0128718005554,76.5289560305876,76.5302685378483,79.6252059439383,80.6632840410507,82.6478071799877,82.8684906170844,83.172509412243,83.9329575186219,84.4040767519618,85.6077867982393,88.9158716737216,100.988534890916,104.358226791801],[26.4127076066918,29.1947547798435,31.6692848775921,38.9411995572343,40.7290609123462,40.9090360263863,42.1270709347162,43.3580763024117,45.6355807764313,46.0247345765761,48.9661110356675,49.3375827535196,52.0402005990707,52.6079109662116,52.9139700512481,52.9912874331753,55.6469168085572,56.3500428182945,57.0810100094698,58.2748444418072,58.820909921952,59.2691910975561,59.4369196089628,59.9615023698763,60.7942641694729,61.0515854361706,61.262314411216,61.3228645187218,61.3752913480499,65.9140464752259,66.8177673958588,67.6970223397197,68.6675540535722,68.8120039059124,69.5581963699235,73.2696172745107,76.5531498143459,77.0692340443781,77.0705465516388,80.1654839577288,81.2035620548412,83.1880851937782,83.4087686308749,83.7127874260335,84.4732355324124,84.9443547657523,86.1480648120298,89.4561496875121,101.528812904706,104.898504805591],[26.5786125959716,29.3606597691233,31.8351898668719,39.1071045465141,40.894965901626,41.0749410156661,42.292975923996,43.5239812916915,45.8014857657111,46.1906395658558,49.1320160249473,49.5034877427994,52.2061055883505,52.7738159554914,53.0798750405279,53.1571924224551,55.812821797837,56.5159478075743,57.2469149987495,58.440749431087,58.9868149112318,59.4350960868359,59.6028245982426,60.1274073591561,60.9601691587527,61.2174904254504,61.4282194004958,61.4887695080016,61.5411963373297,66.0799514645057,66.9836723851386,67.8629273289995,68.833459042852,68.9779088951922,69.7241013592033,73.4355222637905,76.7190548036257,77.2351390336579,77.2364515409186,80.3313889470086,81.369467044121,83.353990183058,83.5746736201547,83.8786924153133,84.6391405216922,85.110259755032,86.3139698013096,89.6220546767919,101.694717893986,105.064409794871],[26.6915583878672,29.4736055610189,31.9481356587675,39.2200503384097,41.0079116935216,41.1878868075618,42.4059217158916,43.6369270835871,45.9144315576068,46.3035853577515,49.2449618168429,49.616433534695,52.3190513802461,52.886761747387,53.1928208324235,53.2701382143507,55.9257675897326,56.6288935994699,57.3598607906452,58.5536952229826,59.0997607031274,59.5480418787315,59.7157703901382,60.2403531510517,61.0731149506483,61.330436217346,61.5411651923914,61.6017152998972,61.6541421292253,66.1928972564014,67.0966181770342,67.9758731208951,68.9464048347476,69.0908546870878,69.8370471510989,73.5484680556861,76.8320005955213,77.3480848255535,77.3493973328142,80.4443347389042,81.4824128360166,83.4669359749536,83.6876194120503,83.9916382072089,84.7520863135878,85.2232055469277,86.4269155932053,89.7350004686875,101.807663685882,105.177355586767],[27.4224908051711,30.2045379783228,32.6790680760714,39.9509827557136,41.7388441108255,41.9188192248657,43.1368541331955,44.3678595008911,46.6453639749107,47.0345177750554,49.9758942341468,50.3473659519989,53.04998379755,53.6176941646909,53.9237532497274,54.0010706316546,56.6567000070365,57.3598260167738,58.0907932079491,59.2846276402865,59.8306931204313,60.2789742960354,60.4467028074421,60.9712855683556,61.8040473679522,62.0613686346499,62.2720976096953,62.3326477172011,62.3850745465292,66.9238296737053,67.8275505943381,68.706805538199,69.6773372520515,69.8217871043917,70.5679795684028,74.27940047299,77.5629330128252,78.0790172428574,78.0803297501181,81.1752671562081,82.2133452533205,84.1978683922575,84.4185518293542,84.7225706245128,85.4830187308917,85.9541379642316,87.1578480105092,90.4659328859914,102.538596103185,105.908288004071],[28.36726919622,31.1493163693717,33.6238464671203,40.8957611467625,42.6836225018744,42.8635976159146,44.0816325242444,45.3126378919399,47.5901423659596,47.9792961661043,50.9206726251957,51.2921443430478,53.9947621885989,54.5624725557398,54.8685316407763,54.9458490227035,57.6014783980854,58.3046044078227,59.035571598998,60.2294060313354,60.7754715114802,61.2237526870843,61.391481198491,61.9160639594045,62.7488257590011,63.0061470256988,63.2168760007442,63.27742610825,63.3298529375781,67.8686080647542,68.772328985387,69.6515839292479,70.6221156431004,70.7665654954406,71.5127579594517,75.2241788640389,78.5077114038741,79.0237956339063,79.025108141167,82.120045547257,83.1581236443694,85.1426467833064,85.3633302204032,85.6673490155617,86.4277971219406,86.8989163552805,88.1026264015581,91.4107112770403,103.483374494234,106.853066395119],[28.6401623680532,31.4222095412049,33.8967396389535,41.1686543185957,42.9565156737076,43.1364907877477,44.3545256960776,45.5855310637731,47.8630355377927,48.2521893379374,51.1935657970289,51.565037514881,54.2676553604321,54.835365727573,55.1414248126095,55.2187421945367,57.8743715699186,58.5774975796559,59.3084647708311,60.5022992031686,61.0483646833134,61.4966458589175,61.6643743703242,62.1889571312377,63.0217189308343,63.279040197532,63.4897691725774,63.5503192800832,63.6027461094112,68.1415012365873,69.0452221572202,69.9244771010811,70.8950088149336,71.0394586672737,71.7856511312849,75.4970720358721,78.7806045757073,79.2966888057395,79.2980013130002,82.3929387190902,83.4310168162026,85.4155399551396,85.6362233922363,85.9402421873948,86.7006902937738,87.1718095271137,88.3755195733912,91.6836044488735,103.756267666068,107.125959566953],[31.0416845357562,33.8237317089079,36.2982618066565,43.5701764862987,45.3580378414106,45.5380129554507,46.7560478637806,47.9870532314761,50.2645577054957,50.6537115056404,53.5950879647319,53.966559682584,56.669177528135,57.236887895276,57.5429469803125,57.6202643622397,60.2758937376216,60.9790197473589,61.7099869385341,62.9038213708716,63.4498868510164,63.8981680266205,64.0658965380272,64.5904792989407,65.4232410985373,65.680562365235,65.8912913402804,65.9518414477862,66.0042682771142,70.5430234042903,71.4467443249232,72.3259992687841,73.2965309826366,73.4409808349767,74.1871732989879,77.8985942035751,81.1821267434103,81.6982109734425,81.6995234807032,84.7944608867932,85.8325389839056,87.8170621228426,88.0377455599393,88.3417643550978,89.1022124614768,89.5733316948166,90.7770417410942,94.0851266165765,106.157789833771,109.527481734656],[31.4501855312685,34.2322327044203,36.7067628021689,43.9786774818111,45.7665388369229,45.9465139509631,47.164548859293,48.3955542269885,50.6730587010081,51.0622125011528,54.0035889602443,54.3750606780964,57.0776785236474,57.6453888907884,57.9514479758249,58.0287653577521,60.6843947331339,61.3875207428713,62.1184879340465,63.312322366384,63.8583878465287,64.3066690221329,64.4743975335396,64.998980294453,65.8317420940497,66.0890633607473,66.2997923357927,66.3603424432985,66.4127692726266,70.9515243998027,71.8552453204356,72.7345002642965,73.705031978149,73.8494818304891,74.5956742945003,78.3070951990875,81.5906277389227,82.1067119689549,82.1080244762156,85.2029618823056,86.2410399794179,88.225563118355,88.4462465554517,88.7502653506102,89.5107134569892,89.981832690329,91.1855427366066,94.4936276120889,106.566290829283,109.935982730168],[32.5022203309191,35.2842675040708,37.7587976018194,45.0307122814616,46.8185736365735,46.9985487506136,48.2165836589435,49.447589026639,51.7250935006586,52.1142473008033,55.0556237598948,55.4270954777469,58.129713323298,58.6974236904389,59.0034827754754,59.0808001574026,61.7364295327845,62.4395555425218,63.170522733697,64.3643571660345,64.9104226461793,65.3587038217834,65.5264323331901,66.0510150941036,66.8837768937002,67.1410981603979,67.3518271354433,67.4123772429491,67.4648040722771,72.0035591994532,72.9072801200861,73.786535063947,74.7570667777995,74.9015166301397,75.6477090941508,79.359129998738,82.6426625385732,83.1587467686054,83.1600592758661,86.2549966819561,87.2930747790685,89.2775979180055,89.4982813551022,89.8023001502607,90.5627482566397,91.0338674899795,92.2375775362571,95.5456624117394,107.618325628933,110.988017529819],[38.2396695158737,41.0217166890255,43.496246786774,50.7681614664163,52.5560228215281,52.7359979355683,53.9540328438981,55.1850382115937,57.4625426856133,57.851696485758,60.7930729448494,61.1645446627015,63.8671625082526,64.4348728753936,64.74093196043,64.8182493423573,67.4738787177391,68.1770047274764,68.9079719186517,70.1018063509891,70.6478718311339,71.096153006738,71.2638815181447,71.7884642790582,72.6212260786548,72.8785473453525,73.0892763203979,73.1498264279037,73.2022532572318,77.7410083844079,78.6447293050408,79.5239842489016,80.4945159627542,80.6389658150943,81.3851582791054,85.0965791836927,88.3801117235278,88.89619595356,88.8975084608207,91.9924458669107,93.0305239640231,95.0150471029601,95.2357305400569,95.5397493352154,96.3001974415943,96.7713166749342,97.9750267212118,101.283111596694,113.355774813888,116.725466714773],[40.9405976645242,43.722644837676,46.1971749354245,53.4690896150668,55.2569509701786,55.4369260842188,56.6549609925487,57.8859663602442,60.1634708342638,60.5526246344085,63.4940010934999,63.865472811352,66.5680906569031,67.1358010240441,67.4418601090805,67.5191774910078,70.1748068663896,70.8779328761269,71.6089000673022,72.8027344996396,73.3487999797844,73.7970811553885,73.9648096667952,74.4893924277087,75.3221542273053,75.579475494003,75.7902044690484,75.8507545765542,75.9031814058823,80.4419365330584,81.3456574536913,82.2249123975521,83.1954441114047,83.3398939637448,84.0860864277559,87.7975073323432,91.0810398721784,91.5971241022105,91.5984366094712,94.6933740155612,95.7314521126736,97.7159752516106,97.9366586887074,98.2406774838659,99.0011255902448,99.4722448235847,100.675954869862,103.984039745345,116.056702962539,119.426394863424]],"type":"surface","x":[-4.44585033319053,-4.22639470950259,-4.07597421713483,-3.264857723815,-2.48735664250955,-2.42707453768341,-2.11519570831633,-2.06039261721247,-2.05796397926283,-1.99567685626452,-1.74115622451856,-1.62586985030409,-1.52081986853802,-1.45176684910048,-1.24040682971251,-1.00606930056544,-0.592730451548333,-0.591212166502939,-0.585334790655924,-0.548086147958929,-0.543066183934002,-0.48893767800831,-0.434049739259119,-0.349608871758296,-0.0589047620367544,0.0917772684483983,0.101677188693447,0.180002104549399,0.281096441858762,0.427746220865478,0.551097448896429,0.670447232350085,0.845580921775133,0.891601535342018,1.51694312999049,1.9883323826121,2.09722080475462,2.16836518421425,2.25426561957652,2.36337331778596,2.39687738613548,2.41968648637947,2.56729633087264,2.75809179715889,2.81320184249814,3.29818281382937,3.38067849605031,3.59313410685851,4.75179660687796,5.29724181435705],"y":[-4.41912104116896,-4.07102184072386,-3.76140022222062,-2.85151353513735,-2.62781024418413,-2.60529114605309,-2.4528864773632,-2.29885890070173,-2.01388979806851,-1.96519755158619,-1.59716252921171,-1.55068272497971,-1.21252199204031,-1.14148814085572,-1.10319298762528,-1.09351877394224,-0.761237398137361,-0.67325988137785,-0.581798781345011,-0.432422158485776,-0.364096589148385,-0.308006124432938,-0.287019362858232,-0.221381784385977,-0.117183796875793,-0.0849868851701209,-0.0586197593960398,-0.051043524092399,-0.0444837009711016,0.523420770970757,0.636497404945093,0.746512772728603,0.867949000287586,0.886023056899303,0.979389194830804,1.44377479614985,1.85462155057681,1.9191957639727,1.9193599893433,2.30660907946819,2.43649694013844,2.68480722207595,2.71241988398414,2.75045974920866,2.8456096015782,2.90455764172021,3.05516993496982,3.46908876110364,4.97966142633536,5.40128873293331],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(m1)</code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##   48.855298   11.052508    2.505395</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(m2)</code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##   49.630994    9.878279    3.054908</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(m3)</code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##   50.389177    3.369722    9.739047</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(m4)</code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##   50.027890    4.951786    7.992110</code></pre>
<p></details></p>
<div id="variance-inflation-factor" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Variance Inflation Factor</h3>
<p>In this section we introduce the <strong>Variance Inflation Factor (VIF)</strong>, which can be used for measuring the effect of multicollinearity on the variances (or standard errors) of the parameter estimators. We will present the results without proof.</p>
<p>We will be looking for an alternative expression for <span class="math inline">\(\var{\hat\beta_j}\)</span> that consists of two factors:</p>
<ul>
<li><p>factor 1: the variance of <span class="math inline">\(\hat\beta_j\)</span> as if there were no multicollinearity</p></li>
<li><p>factor 2: the variance inflation factor (VIF)</p></li>
</ul>
<p>To do so, we will need an auxiliary regression model that has <span class="math inline">\(x_j\)</span> acting as the outcome variable, and the other <span class="math inline">\(p-2\)</span> regressors acting as regressors. In particular,</p>
<span class="math display" id="eq:tmp72765267">\[\begin{equation}
  \tag{3.4}
  X_{ij} = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_{j-1}x_{ij-1} + \beta_{j+1}x_{ij+1} + \cdots + \beta_{p-1}x_{ip-1} +\eps_i^*
\end{equation}\]</span>
<p>with <span class="math inline">\(\E{\eps_i^*}=0\)</span> and <span class="math inline">\(\var{\eps_i^*}\)</span> constant, and with <span class="math inline">\(X_{ij}\)</span> the notation for <span class="math inline">\(x_{ij}\)</span> considered as outcome. For this model, the coefficient of determination is denoted by <span class="math inline">\(R_j^2\)</span>, which can be understood by considering two extreme situations:</p>
<ul>
<li><p><span class="math inline">\(R_j^2=0\)</span>: the variability of regressor <span class="math inline">\(j\)</span> cannot be explained by a linear model with the other regressors acting as the regressors. This extreme situation happens if regressor <span class="math inline">\(j\)</span> is linearly independent of the other regressors.</p></li>
<li><p><span class="math inline">\(R_j^2=1\)</span>: the variability of regressor <span class="math inline">\(j\)</span> can be completely explained by a linear model of the other regressors. This extreme situation happens if regressor <span class="math inline">\(j\)</span> is a linear combination of the other regressors (for all <span class="math inline">\(n\)</span> sample observations).</p></li>
</ul>
<p>Hence, <span class="math inline">\(R_j^2\)</span> measures the to what extent the multicollinearity affects regressor <span class="math inline">\(j\)</span>.</p>
<p>Before presenting the alternative expression for <span class="math inline">\(\var{\hat\beta_j}\)</span>, we recall its expression when <span class="math inline">\(x_j\)</span> is the only regressor in a simple linear regression model (see Equation <a href="#eq:SigmaBetaLSE">(2.6)</a>): <span class="math display">\[
  \var{\hat\beta_j} = \frac{\sigma_j^2}{\sum_{i=1}^n (x_{ij}-\bar{x}_j)^2}
\]</span> in which <span class="math inline">\(\sigma_j^2\)</span> is the residual variance in the simple linear regression model with only <span class="math inline">\(x_j\)</span> as regressor.</p>
<p>We are now ready to give an alternative expression of <span class="math inline">\(\var{\hat\beta_j}\)</span> in the multiple linear regression model <a href="#eq:Mod5">(3.1)</a>:</p>
<span class="math display" id="eq:tmp76198716981">\[\begin{equation}
  \tag{3.5}
 \var{\hat\beta_j} = \frac{\sigma^2}{\sum_{i=1}^n (x_{ij}-\bar{x}_j)^2} \frac{1}{1-R_j^2} ,
\end{equation}\]</span>
<p>in which <span class="math inline">\(\sigma^2\)</span> is the residual variance of model <a href="#eq:Mod5">(3.1)</a> (i.e. the model that includes all <span class="math inline">\(p-1\)</span> regressors).</p>
<p>Equation <a href="#eq:tmp76198716981">(3.5)</a> tells us that <span class="math inline">\(\var{\hat\beta_j}\)</span> is the product of</p>
<ul>
<li><p>the variance of <span class="math inline">\(\hat\beta_j\)</span> in a model without collinearity (<span class="math inline">\(R_j^2=0\)</span>) for regressor <span class="math inline">\(j\)</span></p></li>
<li><p>the variance inflation factor (VIF)</p></li>
</ul>
<p>When all regressors are uncorrelated, we say we have <strong>orthogonal regressors</strong>. In this case all <span class="math inline">\(R_j^2=0\)</span> and the variances of the parameter estimators obtain their minimal values (most precise estimators), for given sample size, given error term variance and for given observed values of the regressors.</p>
<p>What can we do to remediate a large VIF? You may want to remove one or more regressors from the model (if this is allowed and makes sense keeping the research question in mind).</p>
<p>Finally, what is a large, problematic value for the VIF? Some guidelines give 10 as a threshold, others give 5. As always in statistics, don't ever apply a strict threshold!</p>
</div>
</div>
<div id="example-lead-concentration-5" class="section level2 unnumbered">
<h2>Example (Lead concentration)</h2>
<p>We consider the model with the three regressors (<em>Ld73</em>, <em>Totyrs</em> and <em>Age</em>), and check the multicollinearity. One reason for doing so, is that we noted that the significance of <em>Totyrs</em> disappeared when <em>Age</em> was added to the model. Although we had a reasonable explanation, we also saw a moderately large correlation between <em>Totyrs</em> and <em>Age</em>. This correlation could have caused multicollinearity, which in turn may have caused the insignificance of the effect of <em>Totyrs</em>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">R2.Ld73&lt;-<span class="kw">summary</span>(<span class="kw">lm</span>(Ld73<span class="op">~</span>Totyrs<span class="op">+</span>Age, <span class="dt">data=</span>lead,
                    <span class="dt">subset=</span><span class="op">!</span><span class="kw">is.na</span>(MAXFWT)))<span class="op">$</span>r.squared
<span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>R2.Ld73)</code></pre></div>
<pre><code>## [1] 1.072777</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">R2.Totyrs&lt;-<span class="kw">summary</span>(<span class="kw">lm</span>(Totyrs<span class="op">~</span>Ld73<span class="op">+</span>Age, <span class="dt">data=</span>lead,
                   <span class="dt">subset=</span><span class="op">!</span><span class="kw">is.na</span>(MAXFWT)))<span class="op">$</span>r.squared
<span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>R2.Totyrs)</code></pre></div>
<pre><code>## [1] 1.541473</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">R2.Age&lt;-<span class="kw">summary</span>(<span class="kw">lm</span>(Age<span class="op">~</span>Ld73<span class="op">+</span>Totyrs, <span class="dt">data=</span>lead,
                   <span class="dt">subset=</span><span class="op">!</span><span class="kw">is.na</span>(MAXFWT)))<span class="op">$</span>r.squared
<span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>R2.Age)</code></pre></div>
<pre><code>## [1] 1.592838</code></pre>
<p>The VIF of <em>Ld73</em> is very close to 1, and hence is not problematic at all. The VIFs of <em>Totyrs</em> and <em>Age</em> are slightly larger (<span class="math inline">\(1.5\)</span> to <span class="math inline">\(1.6\)</span>), but this is still not considered to be problematic. If the VIF<span class="math inline">\(\approx 2\)</span>, it means that the variance of the parameter estimator is 2 times larger than if there were no multicollinearity, and hence the standard error is <span class="math inline">\(\sqrt{2}=1.4\)</span> larger as compared to no multicollinearity.</p>
<p>Here is a faster way for calculating the VIFs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vif</span>(<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Totyrs<span class="op">+</span>Age<span class="op">+</span>Ld73, <span class="dt">data=</span>lead))</code></pre></div>
<pre><code>##   Totyrs      Age     Ld73 
## 1.541473 1.592838 1.072777</code></pre>
</div>
<div id="leverage" class="section level2">
<h2><span class="header-section-number">3.5</span> Leverage</h2>
<p>Residual plots are also useful for identifying outliers. Recall that it is not adviced for the statistician to remove observations only because they were identified as outliers. On the other hand, it is a task of the statistician to identify outliers and to report them so that the scientists who are closer to the data and the study can check whether perhaps something went wrong that may explain the outlying behavior of this data point. Sometimes an outlier does not stronly affect the parameter estimates and the conlcusions. Such outliers are usually not problematic. Other times an outlier may be very <strong>influential</strong> in the sense that this outlying observation has a strong effect on the numerical values of the parameter estimates. Such outliers are problematic and worrisome.</p>
<p>In this section we introduce the <strong>leverage</strong> of an observation, as a measure of the observation's influence on the regression fit.</p>
<p>In matrix notation, the vector of predictions can be written as <span class="math display">\[
  \hat{\mb{Y}} = \mb{X}\hat{\mb\beta} = \mb{X}(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y} = \mb{HY},
\]</span> where the <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\mb{H}\)</span> is generally known as the <strong>hat-matrix</strong>.</p>
<p>Note that the hat matrix is <em>idempotent</em>, i.e. <span class="math display">\[
  \mb{H}\mb{H}^t=\mb{H}^t\mb{H} = \mb{HH} =\mb{H}.
\]</span></p>
<p>The <span class="math inline">\(i\)</span>th element of <span class="math inline">\(\hat{\mb{Y}}\)</span>, i.e. <span class="math inline">\(\hat{Y}_i\)</span>, can be written as. <span class="math display">\[
  \sum_{j=1}^n h_{ij} Y_j
\]</span> with <span class="math inline">\(h_{ij}\)</span> the element on position <span class="math inline">\((i,j)\)</span> of the matrix matrix <span class="math inline">\(\mb{H}\)</span>. This equation demonstrates that the predictions are linear functions of the outcomes (it is an example of a <em>linear predictor</em>).</p>
<p>Without proof we give here the following property: <span class="math display">\[
  \sum_{j=1}^n h_{ij}=1 \;\;\text{ for all } \;\; i=1,\ldots, n.
\]</span></p>
<p>For the prediction of observation <span class="math inline">\(i\)</span> we can write</p>
<span class="math display" id="eq:tmp7167979207">\[\begin{equation}
  \hat{Y}_i = \mb{h}_i^t\mb{Y}
  \tag{3.6}
\end{equation}\]</span>
<p>with <span class="math inline">\(\mb{h}_i^t\)</span> the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mb{H}\)</span>. Sicne the sum of the elements of <span class="math inline">\(\mb{h}_i\)</span> always equals 1, Equation <a href="#eq:tmp7167979207">(3.6)</a> shows that the prediction <span class="math inline">\(\hat{Y}_i\)</span> is a weighted mean of the sample outcomes <span class="math inline">\(Y_1,\ldots, Y_n\)</span>.</p>
<p>This interpretation allows us to evaluate the elements of the vector <span class="math inline">\(\mb{h}_i\)</span>:</p>
<ul>
<li><p>if <span class="math inline">\(h_{ij}\)</span> is large (relative to the other elements), then outcome <span class="math inline">\(Y_j\)</span> strongly affects the prediction <span class="math inline">\(\hat{Y}_i\)</span>.</p></li>
<li><p>A global measure for the influence of observation <span class="math inline">\(Y_i\)</span> on the predictions <span class="math inline">\(\hat{Y}_1,\ldots, \hat{Y}_n\)</span> is given by <span class="math display">\[
   \sum_{j=1}^n h_{ij}^2 = \mb{h}_i^t\mb{h}_i = h_{ii} 
 \]</span> (the final equality follows from <span class="math inline">\(\mb{HH}=\mb{H}\)</span>). The square (<span class="math inline">\(h_{ij}^2\)</span>) is used because both large positive and large negative <span class="math inline">\(h_{ij}\)</span> imply that <span class="math inline">\(Y_i\)</span> is influential.</p></li>
</ul>
<p>The <strong>leverage</strong> of observation <span class="math inline">\(i\)</span> is defined as <span class="math inline">\(h_{ii}\)</span> and it is thus a global measure of the influence of observation <span class="math inline">\(i\)</span> on the predictions.</p>
<p>It can also be shown that <span class="math inline">\(\sum_{i=1}^n h_{ii} =p\)</span>. This may help in thresholding the individual <span class="math inline">\(h_{ii}\)</span> leverage values, i.e. the avere of the <span class="math inline">\(h_{ii}\)</span> is thus given by <span class="math inline">\(p/n\)</span>. Leverages much larger than <span class="math inline">\(p/n\)</span> may be called influential.</p>
<p>If an observation <span class="math inline">\(i\)</span> is identified as an outlier, and if its leverage <span class="math inline">\(h_{ii}\)</span> is large, then we call observation <span class="math inline">\(i\)</span> an <strong>influential outlier</strong>.</p>
</div>
<div id="example-lead-concentration-6" class="section level2 unnumbered">
<h2>Example (Lead concentration)</h2>
<p>We will look for influential outliers in the Lead Concentration example.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Ld73<span class="op">+</span>Totyrs,<span class="dt">data=</span>lead,<span class="dt">x=</span>T)
X&lt;-m<span class="op">$</span>x
H&lt;-X<span class="op">%*%</span><span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X)<span class="op">%*%</span><span class="kw">t</span>(X)
h&lt;-<span class="kw">diag</span>(H)
<span class="kw">sum</span>(h)</code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(h,<span class="dt">xlab=</span><span class="st">&quot;i&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;leverage&quot;</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">sum</span>(h)<span class="op">/</span><span class="kw">nrow</span>(lead[indNA,]),<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-101-1.png" width="672" /></p>
<p>Another and simpler way of computing the leverages:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">h&lt;-<span class="kw">influence</span>(m)<span class="op">$</span>h</code></pre></div>
</div>
<div id="assessment-of-the-model-assumptions-and-remedial-measures" class="section level2">
<h2><span class="header-section-number">3.6</span> Assessment of the Model Assumptions and Remedial Measures</h2>
<p>In Section <a href="#S:AssessAssumptions">2.8</a> we illustrated methods for assessing the model assumptions for simple linear regression. In this section we extent these methods to multiple linear regression models. We will also briefly explain what can be done in case of model violations.</p>
<div id="residual-analysis" class="section level3 unnumbered">
<h3>Residual analysis</h3>
<p>In principle all methods of Section <a href="#S:AssessAssumptions">2.8</a> are still applicable:</p>
<ul>
<li><p><strong>normal QQ-plots</strong> of the residuals for checking the assumption of normality of the error terms</p></li>
<li><p><strong>residual plots</strong> constructed as residuals versus a regressor. For multiple linear regression, such a plot can be plotted for each regressor separately. These plots can be used for checking the correctness of the model <span class="math inline">\(m(\mb{x})\)</span> as a function of <span class="math inline">\(\mb{x}\)</span> and for checking the homoscedasticity assumption. For the latter purpose also squared residuals or absolute values of the residuals may be plotted.</p></li>
</ul>
<p>Regarding the residual plots: although preference is given to making a residual plot for each regressor separately, some people construct only one single residual plot, but with the predicted outcomes <span class="math inline">\(\hat{Y}_i\)</span> on the horizontal axis. This is explained in the next paragraph.</p>
<p>Consider the model <span class="math display">\[
  Y_i = m(\mb{x}) + \eps_i = \mb\beta^t\mb{x} + \eps_i,
\]</span> with <span class="math inline">\(\E{\eps_i \mid \mb{x}}=0\)</span> and <span class="math inline">\(\var{\eps_i \mid \mb{x}}=\sigma^2\)</span>, <span class="math inline">\(i=1,\ldots, n\)</span>.</p>
<p>The expression <span class="math inline">\(\E{\eps_i \mid \mb{x}}=0\)</span> implies</p>
<ul>
<li><p><span class="math inline">\(\E{\eps_i \mid x_j}=0\)</span> for all <span class="math inline">\(j=1,\ldots, p-1\)</span> (this is why individual residual plots are meaningful)</p></li>
<li><p><span class="math inline">\(\E{\eps_i \mid \mb\beta^t\mb{x}}=0\)</span> for all <span class="math inline">\(\mb\beta\)</span> and hence als for <span class="math inline">\(\mb\beta=\hat{\mb\beta}\)</span>. This explains why residual plots of residuals versus <span class="math inline">\(\hat{Y}_i=\hat{\mb\beta}^t\mb{x}_i\)</span> may be plotted.</p></li>
</ul>
<p>Similar arguments apply to <span class="math inline">\(\var{\eps_i \mid \mb{x}}=\sigma^2\)</span>.</p>
<p>Finally, we note that <strong>(influential) outlier detection</strong> and <strong>multicolinearity</strong> assessment may also be part of the assessment.</p>
</div>
</div>
<div id="example-lead-concentration-7" class="section level2 unnumbered">
<h2>Example (Lead Concentration)</h2>
<p>We will now assess the model assumptions for the additive model with the following regressors: <em>Ld73</em>, <em>Totyrs</em> and <em>Age</em>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Ld73<span class="op">+</span>Totyrs<span class="op">+</span>Age, <span class="dt">data=</span>lead)

e&lt;-<span class="kw">residuals</span>(m)
YHat&lt;-<span class="kw">predict</span>(m)

<span class="kw">qqnorm</span>(e)
<span class="kw">qqline</span>(e)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-103-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">indNA&lt;-<span class="op">!</span><span class="kw">is.na</span>(lead<span class="op">$</span>MAXFWT)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(lead<span class="op">$</span>Ld73[indNA],e, <span class="dt">xlab=</span><span class="st">&quot;Blood lead concentration (microgram / 100ml)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">plot</span>(lead<span class="op">$</span>Totyrs[indNA],e, <span class="dt">xlab=</span><span class="st">&quot;Number of years living near smelter&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">plot</span>(lead<span class="op">$</span>Age[indNA],e, <span class="dt">xlab=</span><span class="st">&quot;Age (years)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">plot</span>(YHat[indNA],e, <span class="dt">xlab=</span><span class="st">&quot;Predicted MAXFWT (taps per 10 seconds)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-103-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p>The normal QQ plot shows an asymmetric deviation from normality in the left tail. However, since the sample size is not very small (<span class="math inline">\(n=83\)</span>), we do not consider this a serious problem and we believe that all inference will still be approximately correct.</p>
<p>None of the residual plots show any severe deviation from the model assumptions. The plots do not indicate large outliers, but we will also assess the leverage of the observations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">h&lt;-<span class="kw">influence</span>(m)<span class="op">$</span>h
<span class="kw">plot</span>(h,<span class="dt">xlab=</span><span class="st">&quot;i&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;leverage&quot;</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">sum</span>(h)<span class="op">/</span><span class="kw">nrow</span>(lead[indNA,]),<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-104-1.png" width="672" /></p>
<p>This gives not indication of strong influential outliers.</p>
<p>Finally, we assess multicollinearity (the results have been discussed before).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vif</span>(m)</code></pre></div>
<pre><code>##     Ld73   Totyrs      Age 
## 1.072777 1.541473 1.592838</code></pre>
</div>
<div id="exercise-lead-concentration-2" class="section level2 unnumbered">
<h2>Exercise: Lead concentration</h2>
<p>Consider now the lead concentration data, and assess the assumptions of the model with <em>Ld73</em>, <em>Totyrs</em> and their interaction effect.</p>
<p><details> <summary markdown="span">Try to first make the exercise yourself. You can expand this page to see a solution. </summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Ld73<span class="op">*</span>Totyrs, <span class="dt">data=</span>lead)

e&lt;-<span class="kw">residuals</span>(m)
YHat&lt;-<span class="kw">predict</span>(m)

<span class="kw">qqnorm</span>(e)
<span class="kw">qqline</span>(e)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-106-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">indNA&lt;-<span class="op">!</span><span class="kw">is.na</span>(lead<span class="op">$</span>MAXFWT)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(lead<span class="op">$</span>Ld73[indNA],e, <span class="dt">xlab=</span><span class="st">&quot;Blood lead concentration (microgram / 100ml)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">plot</span>(lead<span class="op">$</span>Totyrs[indNA],e, <span class="dt">xlab=</span><span class="st">&quot;Number of years living near smelter&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">plot</span>(lead<span class="op">$</span>Totyrs[indNA]<span class="op">*</span>lead<span class="op">$</span>Ld73[indNA],e, <span class="dt">xlab=</span><span class="st">&quot;Interaction&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">plot</span>(YHat[indNA],e, <span class="dt">xlab=</span><span class="st">&quot;Predicted MAXFWT (taps per 10 seconds)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-106-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p>None of the graphs show a severe deviation from the model assumptions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">h&lt;-<span class="kw">influence</span>(m)<span class="op">$</span>h
<span class="kw">plot</span>(h,<span class="dt">xlab=</span><span class="st">&quot;i&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;leverage&quot;</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">sum</span>(h)<span class="op">/</span><span class="kw">nrow</span>(lead[indNA,]),<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-107-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vif</span>(m)</code></pre></div>
<pre><code>##        Ld73      Totyrs Ld73:Totyrs 
##    9.043449   11.256362   20.105660</code></pre>
<p>The leverage plot shows one observation with a clearly larger leverage than the others, but as there were no outliers detected, the leverage is not very important here.</p>
<p>The VIFs, on the other hand, are quite large: 10 to 20. This happens often in models that include interaction terms. Since the interaction is basically the product of regressors in the model, it is not uncommon that these interactions show a large correlation with the regressors they are based on.</p>
<p></details></p>
</div>
<div id="exercise-blood-pressure-1" class="section level2 unnumbered">
<h2>Exercise: Blood pressure</h2>
<p>Consider now the blood pressure example and assess the model assumption of the model with dose, gender and their interaction term.</p>
<p><details> <summary markdown="span">Try to first make the exercise yourself. You can expand this page to see a solution. </summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(bp.reduction<span class="op">~</span>dose<span class="op">*</span>gender,<span class="dt">data=</span>BloodPressure)

e&lt;-<span class="kw">residuals</span>(m)
YHat&lt;-<span class="kw">predict</span>(m)

<span class="kw">qqnorm</span>(e)
<span class="kw">qqline</span>(e)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-108-1.png" width="672" /></p>
<p>The residuals appear to be nicely normally distributed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(BloodPressure<span class="op">$</span>dose,e, <span class="dt">xlab=</span><span class="st">&quot;dose (mg/day)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">plot</span>(BloodPressure<span class="op">$</span>dose,<span class="kw">abs</span>(e), <span class="dt">xlab=</span><span class="st">&quot;dose (mg/day)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;abs(residual)&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">boxplot</span>(e<span class="op">~</span>BloodPressure<span class="op">$</span>gender, <span class="dt">xlab=</span><span class="st">&quot;gender (0: man; 1: woman)&quot;</span>, 
        <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">plot</span>(BloodPressure<span class="op">$</span>dose[BloodPressure<span class="op">$</span>gender<span class="op">==</span><span class="dv">0</span>],e[BloodPressure<span class="op">$</span>gender<span class="op">==</span><span class="dv">0</span>], 
                                                   <span class="dt">xlab=</span><span class="st">&quot;dose (mg/day)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>,
     <span class="dt">main=</span><span class="st">&quot;man&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">plot</span>(BloodPressure<span class="op">$</span>dose[BloodPressure<span class="op">$</span>gender<span class="op">==</span><span class="dv">1</span>],e[BloodPressure<span class="op">$</span>gender<span class="op">==</span><span class="dv">1</span>], 
     <span class="dt">xlab=</span><span class="st">&quot;dose (mg/day)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>,
     <span class="dt">main=</span><span class="st">&quot;woman&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">plot</span>(YHat,e, <span class="dt">xlab=</span><span class="st">&quot;Predicted blood pressure reduction (mmHg)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-109-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p>The residual plots with dose on the horizontal axis may suggest a small increase of the variance with the dose, which would be a violation of the constant-variance assumption. The same could also be seen from the residual plot with the predicted outcomes on the horizontal axis. However, the increase is only very small and is therefore probably not problematic.</p>
<p>The boxlot shows now deviations from the model assumptions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">h&lt;-<span class="kw">influence</span>(m)<span class="op">$</span>h
<span class="kw">plot</span>(h,<span class="dt">xlab=</span><span class="st">&quot;i&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;leverage&quot;</span>,<span class="dt">cex.axis=</span><span class="fl">1.5</span>,<span class="dt">cex.lab=</span><span class="fl">1.5</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">sum</span>(h)<span class="op">/</span><span class="kw">nrow</span>(BloodPressure),<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-110-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vif</span>(m)</code></pre></div>
<pre><code>##        dose      gender dose:gender 
##    1.766162    2.263726    2.933834</code></pre>
<p>No influential outliers are detected and no multicollinearity is indicated.</p>
<p></details></p>
</div>
<div id="example-bacterial-count" class="section level2 unnumbered">
<h2>Example (Bacterial count)</h2>
<p>We have data from an experiment in which the antibacterial effect of metalic nanoparticles is investigated. For each of 5 doses, 7 experiments are replicated. For each experiment, the number of bacterial colonies on a plate are counted; this is known as the <em>colony forming units</em> (CFU). The smaller this number, the better the antibacterial activity of the nanoparticles.</p>
<p>We will read the data, make a plot and fit a linear regression model for which we will assess the model assumptions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;Data/Nano.RData&quot;</span>)
<span class="kw">skim</span>(Nano)</code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-111">Table 3.2: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">Nano</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">35</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">2</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">concentration</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.33</td>
<td align="right">0.27</td>
<td align="right">0.05</td>
<td align="right">0.1</td>
<td align="right">0.25</td>
<td align="right">0.5</td>
<td align="right">0.75</td>
<td align="left">▇▃▁▃▃</td>
</tr>
<tr class="even">
<td align="left">CFU</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">31.86</td>
<td align="right">14.47</td>
<td align="right">12.00</td>
<td align="right">19.0</td>
<td align="right">33.00</td>
<td align="right">45.5</td>
<td align="right">52.00</td>
<td align="left">▇▂▅▃▇</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(Nano<span class="op">$</span>concentration,Nano<span class="op">$</span>CFU, <span class="dt">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;CFU&quot;</span>,
     <span class="dt">cex.axis=</span><span class="fl">1.5</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>)

m&lt;-<span class="kw">lm</span>(CFU<span class="op">~</span>concentration, <span class="dt">data=</span>Nano)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = CFU ~ concentration, data = Nano)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.1054 -2.4675 -0.0708  3.1016  5.2741 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     49.381      0.880   56.12   &lt;2e-16 ***
## concentration  -53.103      2.089  -25.42   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.236 on 33 degrees of freedom
## Multiple R-squared:  0.9514, Adjusted R-squared:   0.95 
## F-statistic: 646.4 on 1 and 33 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(Nano<span class="op">$</span>concentration,Nano<span class="op">$</span>CFU, <span class="dt">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;CFU&quot;</span>,
     <span class="dt">cex.axis=</span><span class="fl">1.5</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>)
<span class="kw">abline</span>(<span class="kw">coef</span>(m), <span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-111-1.png" width="672" /></p>
<p>The plot with the fitted regression line already shows a deviation from the linearity assumption. This becomes also clear from the residual plot.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">e&lt;-<span class="kw">residuals</span>(m)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(Nano<span class="op">$</span>concentration, e, <span class="dt">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">boxplot</span>(e<span class="op">~</span>Nano<span class="op">$</span>concentration, <span class="dt">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-112-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p>The next plot looks at the constancy of variance assumption.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(<span class="kw">abs</span>(e)<span class="op">~</span>Nano<span class="op">$</span>concentration,  <span class="dt">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;abs(residual)&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">mean</span>(<span class="kw">abs</span>(e)), <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="dv">4</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-113-1.png" width="672" /></p>
<p>This plot does not give a clear picture. It seems to suggest that the variance for the concentration of <span class="math inline">\(0.1\)</span> w/v% is smaller than for the other concentrations. However, we should be careful in interpreting this graph, because we have already discovered a lack-of-fit to the linear regression line. Thus we know that we have larger residuals (in absolute value) for concentrations for which the residuals are not zero on average. The small absolute values of the residuals for the concentration of <span class="math inline">\(0.1\)</span> w/v% is here a consequence of the good fit of the corresponding observations to the regression line (as compared to the poor fit for many other concentrations). Hence, we should first try to improve the model fit, and then assess again the constant-variance assumption.</p>
<p>Next, we look at the normality of the residuals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qqnorm</span>(e)
<span class="kw">qqline</span>(e)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-114-1.png" width="672" /></p>
<p>This seems to suggest some (symmetric) deviation from the normality assumption, but just like before, we should be careful with the interpretatation of this normal QQ plot, because the model does not fit well.</p>
<div id="remedial-measures" class="section level3 unnumbered">
<h3>Remedial measures</h3>
<p>In the previous example (bacterial counts) we discovered a <em>devation from the linearity assumption</em>. Here are a few possible solutions:</p>
<ul>
<li><p><strong>transformation of the regressor</strong> <span class="math inline">\(x\)</span> to e.g. <span class="math inline">\(x^2\)</span>, <span class="math inline">\(\sqrt{x}\)</span>, <span class="math inline">\(\log(x)\)</span>, <span class="math inline">\(\exp(x)\)</span>, <span class="math inline">\(\exp(-x)\)</span>, <span class="math inline">\(1/x\)</span>, <span class="math inline">\(\ldots\)</span>. Note that even nonlinear transformations still result in linear regression models (the model is still linear in the <span class="math inline">\(\beta\)</span>-parameters).</p></li>
<li><p><strong>addition of other regressors</strong>. In our example, however, we only have the concentration. Still we can add a regressor. For example, we could add <span class="math inline">\(x^2\)</span> to the model, resulting in <span class="math inline">\(m(x)=\beta_0+\beta_1 x + \beta_2 x^2\)</span>. Even more terms can be added.</p></li>
<li><p><strong>transformation of the outcome</strong> <span class="math inline">\(Y\)</span>. Although this is very common, it comes with an important issue. Suppose we perform a log-transformation on the outcome, i.e. we formulate the linear regression model as <span class="math display">\[
   \log Y_i = \beta_0 + \beta_1 x_i + \eps_i
\]</span> with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>. This model thus implies <span class="math display">\[
  \E{\log(Y) \mid x} = \beta_0 + \beta_1 x.
\]</span> What is now the interpretation of <span class="math inline">\(\beta_1\)</span>? Let's see: <span class="math display">\[
   \beta_1 = \E{\log(Y) \mid x+1} - \E{\log(Y) \mid x} .
\]</span> Although this interpretation is correct, it is not a simple and convenient interpretation. Some people make the following <strong>error</strong>: <span class="math display">\[
   \beta_1 = \log\E{Y \mid x+1} - \log\E{Y \mid x} = \log \frac{\E{Y \mid x+1}}{\E{Y \mid x}},
\]</span> and thus <span class="math display">\[
   \exp\beta_1 = \frac{\E{Y \mid x+1}}{\E{Y \mid x}}.
\]</span> This would have been a conventient interpretation, but unfortunately this is wrong. The reason is of course: <span class="math display">\[
   \E{\log(Y)} \neq \log\E{Y}.
\]</span> More generally, for a nonlinear transformation <span class="math inline">\(g(\cdot)\)</span>, <span class="math display">\[
   \E{g(Y)} \neq g\left(\E{Y}\right).
\]</span> In the GLM course, you will formulate models for <span class="math inline">\(g\left(\E{Y}\right)\)</span> and then the paramaters will get again convenient interpretations.</p></li>
</ul>
</div>
</div>
<div id="example-bacterial-count-1" class="section level2 unnumbered">
<h2>Example (Bacterial count)</h2>
<p>We now try a few transformations, and each time we look at the residuals plots.</p>
<p>First we try adding extra terms (quadratic and cubic effects of the concentration).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1&lt;-<span class="kw">lm</span>(CFU<span class="op">~</span>concentration<span class="op">+</span><span class="kw">I</span>(concentration<span class="op">^</span><span class="dv">2</span>), <span class="dt">data=</span>Nano)
<span class="kw">summary</span>(m1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = CFU ~ concentration + I(concentration^2), data = Nano)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3561 -0.8739  0.1261  0.7771  3.1261 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)         54.1241     0.6245   86.67  &lt; 2e-16 ***
## concentration      -98.2164     4.4529  -22.06  &lt; 2e-16 ***
## I(concentration^2)  57.1521     5.4932   10.40 8.52e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.57 on 32 degrees of freedom
## Multiple R-squared:  0.9889, Adjusted R-squared:  0.9882 
## F-statistic:  1428 on 2 and 32 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">e1&lt;-m1<span class="op">$</span>residuals
<span class="kw">boxplot</span>(e1<span class="op">~</span>Nano<span class="op">$</span>concentration, <span class="dt">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-115-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m2&lt;-<span class="kw">lm</span>(CFU<span class="op">~</span>concentration<span class="op">+</span><span class="kw">I</span>(concentration<span class="op">^</span><span class="dv">2</span>)<span class="op">+</span><span class="kw">I</span>(concentration<span class="op">^</span><span class="dv">3</span>), <span class="dt">data=</span>Nano)
<span class="kw">summary</span>(m2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = CFU ~ concentration + I(concentration^2) + I(concentration^3), 
##     data = Nano)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7207 -0.6404  0.2793  0.8579  3.2961 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          55.2872     0.9648  57.305  &lt; 2e-16 ***
## concentration      -117.0779    12.8673  -9.099 2.91e-10 ***
## I(concentration^2)  117.4626    39.0848   3.005  0.00522 ** 
## I(concentration^3)  -50.0683    32.1390  -1.558  0.12942    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.536 on 31 degrees of freedom
## Multiple R-squared:  0.9897, Adjusted R-squared:  0.9887 
## F-statistic:   995 on 3 and 31 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">e2&lt;-m2<span class="op">$</span>residuals
<span class="kw">boxplot</span>(e2<span class="op">~</span>Nano<span class="op">$</span>concentration, <span class="dt">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-115-2.png" width="672" /></p>
<p>These extra terms seem to improve the fit when looking at the residual plots. Let us now also look at the fitted regression lines.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">concentrations&lt;-<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dt">by=</span><span class="fl">0.05</span>)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(Nano<span class="op">$</span>concentration,Nano<span class="op">$</span>CFU, <span class="dt">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;CFU&quot;</span>,
     <span class="dt">cex.axis=</span><span class="fl">1.5</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>, 
     <span class="dt">main=</span><span class="st">&quot;model with linear and quadratic terms&quot;</span>)
<span class="kw">lines</span>(concentrations,<span class="kw">predict</span>(m1,<span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">concentration=</span>concentrations)), <span class="dt">col=</span><span class="dv">2</span>)
<span class="kw">plot</span>(Nano<span class="op">$</span>concentration,Nano<span class="op">$</span>CFU, <span class="dt">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;CFU&quot;</span>,
     <span class="dt">cex.axis=</span><span class="fl">1.5</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>,
     <span class="dt">main=</span><span class="st">&quot;model with linear, quadratic and cubic terms&quot;</span>)
<span class="kw">lines</span>(concentrations,<span class="kw">predict</span>(m2,<span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">concentration=</span>concentrations)), <span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-116-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p>This seems indeed to improve the fit.</p>
<p>Let us try to apply a non-linear transformation to the concentration, e.g. <span class="math inline">\(\exp(-x)\)</span> because we see an exponentially decreasing trend in the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m3&lt;-<span class="kw">lm</span>(CFU<span class="op">~</span><span class="kw">I</span>(<span class="kw">exp</span>(<span class="op">-</span>concentration)), <span class="dt">data=</span>Nano)
<span class="kw">summary</span>(m3)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = CFU ~ I(exp(-concentration)), data = Nano)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.6755 -1.6025 -0.2068  1.8434  3.8434 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)             -26.214      1.550  -16.91   &lt;2e-16 ***
## I(exp(-concentration))   78.184      2.028   38.55   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.164 on 33 degrees of freedom
## Multiple R-squared:  0.9783, Adjusted R-squared:  0.9776 
## F-statistic:  1486 on 1 and 33 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">e3&lt;-m3<span class="op">$</span>residuals
<span class="kw">boxplot</span>(e3<span class="op">~</span>Nano<span class="op">$</span>concentration, <span class="dt">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-117-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(Nano<span class="op">$</span>concentration,Nano<span class="op">$</span>CFU, <span class="dt">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;CFU&quot;</span>,
     <span class="dt">cex.axis=</span><span class="fl">1.5</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>,
     <span class="dt">main=</span><span class="st">&quot;model with exp(concentration)&quot;</span>)
<span class="kw">lines</span>(concentrations,<span class="kw">predict</span>(m3,<span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">concentration=</span>concentrations)), <span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-117-2.png" width="672" /></p>
<p>This improves the fit slightly but not as good as before. Next we try a logarithmic transformation to the outcome.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m4&lt;-<span class="kw">lm</span>(<span class="kw">I</span>(<span class="kw">log</span>(CFU))<span class="op">~</span>concentration, <span class="dt">data=</span>Nano)
<span class="kw">summary</span>(m4)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = I(log(CFU)) ~ concentration, data = Nano)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.111146 -0.020796 -0.000825  0.040423  0.076379 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    3.99315    0.01331  299.92   &lt;2e-16 ***
## concentration -1.98326    0.03160  -62.76   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.04897 on 33 degrees of freedom
## Multiple R-squared:  0.9917, Adjusted R-squared:  0.9914 
## F-statistic:  3939 on 1 and 33 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">e4&lt;-m4<span class="op">$</span>residuals
<span class="kw">boxplot</span>(e4<span class="op">~</span>Nano<span class="op">$</span>concentration, <span class="dt">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">col=</span><span class="dv">4</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-118-1.png" width="672" /></p>
<p>This also seems to improve the fit, or at least it improves the fit on the scale of the log-transformed outcome. Let us now plot the data and the fitted regression line. However, the fit was on the scale of the log-transformed outcome and so we will need to backtransform to the original scale.</p>
<p>Based on the fitted model we have <span class="math display">\[
  \hat{m}^\prime(x) = \hat\beta_0 + \hat\beta_1 x
\]</span> with <span class="math inline">\(\hat{m}^\prime\)</span> an estimate of <span class="math inline">\(\E{\log(Y) \mid x}\)</span>. It would be tempting to consider <span class="math inline">\(\exp(\hat\beta_0 + \hat\beta_1 x)\)</span> as an estimate of <span class="math inline">\(\E{Y \mid x}\)</span>, but as explained earlier, the nonlinearity of the logarithmic function does not allow us to do so. Let's do it anyway (which I really <em>do not</em> advise to do -- I actually advice never to apply a nonlinear transformation to the outcome).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(Nano<span class="op">$</span>concentration,Nano<span class="op">$</span>CFU, <span class="dt">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;CFU&quot;</span>,
     <span class="dt">cex.axis=</span><span class="fl">1.5</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>,
     <span class="dt">main=</span><span class="st">&quot;model with log-transformed outcome&quot;</span>)
<span class="kw">lines</span>(concentrations,<span class="kw">exp</span>(<span class="kw">predict</span>(m4,<span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">concentration=</span>concentrations))), <span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-119-1.png" width="672" /></p>
<p>We can also look at the residuals on the original scale (after the poor backtransformation).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(<span class="kw">predict</span>(m4)<span class="op">-</span>Nano<span class="op">$</span>CFU<span class="op">~</span>Nano<span class="op">$</span>concentration, <span class="dt">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;residual&quot;</span>,
        <span class="dt">main=</span><span class="st">&quot;residuals after backtransformation&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="dv">4</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-120-1.png" width="672" /></p>
<p>We will continue will models <em>m2</em> and <em>m4</em> and check the normallity and constant-variance assumptions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">e2&lt;-m2<span class="op">$</span>residuals
e4&lt;-m4<span class="op">$</span>residuals

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">qqnorm</span>(e2, <span class="dt">main=</span><span class="st">&quot;linear, quadratic and cubic concentration effects&quot;</span>)
<span class="kw">qqline</span>(e2)
<span class="kw">qqnorm</span>(e4, <span class="dt">main=</span><span class="st">&quot;log-transformed CFU count&quot;</span>)
<span class="kw">qqline</span>(<span class="kw">as.numeric</span>(e4))</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-121-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">boxplot</span>(<span class="kw">abs</span>(e2)<span class="op">~</span>Nano<span class="op">$</span>concentration,  <span class="dt">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;abs(residual)&quot;</span>,
        <span class="dt">main=</span><span class="st">&quot;linear, quadratic and cubic concentration effects&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">mean</span>(<span class="kw">abs</span>(e2)),<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="dv">4</span>)
<span class="kw">boxplot</span>(<span class="kw">abs</span>(e4)<span class="op">~</span>Nano<span class="op">$</span>concentration,  <span class="dt">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;abs(residual)&quot;</span>,
        <span class="dt">main=</span><span class="st">&quot;log-transformed CFU count&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">mean</span>(<span class="kw">abs</span>(e4)),<span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="dv">4</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-122-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p>None of the two models give a graph that shows a clear systematic pattern. However, the graph for model <em>m2</em> suggests a much smaller variance of the residuals for the largest concentration. This phenomenon cannot be seen for model <em>m4</em>. This may be explained as follows. Count data (as for our bacterial count data example) typically show an increasing variance with the mean outcome (as for a Poisson distribution). This may explain that for model <em>m2</em> (no transformed outcome) that we still see the smaller variances for the larger concentrations (that correspond to the smaller mean CFUs). In model <em>m4</em> the outcome is log-transformed and this typically <em>squeezes</em> the larger values more than the smaller values, and hence it has a variance-stabilising effect on e.g. count data.</p>
<p>Outside the scope of this course: it is possible to not only model the conditinal mean as a function of the regressors, but also the conditional variance <span class="math inline">\(\var{Y \mid x}\)</span> can be modelled. This could allow for variance-heteroscedasticity.</p>
</div>
<div id="model-selection" class="section level2">
<h2><span class="header-section-number">3.7</span> Model Selection</h2>
<p>In this section we will briefly look into the problem of model selection. In general terms, model selection is the process of selecting a final set of regressors based on the sample data. Based on this selected set of regressors, the model fit can be used for answering the original research question.</p>
<p>We will not present a single best method, but we will rather briefly touch upon two methods:</p>
<ul>
<li><p>selection methods based on hypothesis testing. This method is still very popular, but we will argue that it should no longer be appied.</p></li>
<li><p>selection methods for building prediction models.</p></li>
</ul>
<div id="selection-methods-based-on-hypothesis-testing" class="section level3 unnumbered">
<h3>Selection methods based on hypothesis testing</h3>
<p>We limit the discussion to additive models.</p>
<p>There are three different strategies:</p>
<ul>
<li><p><strong>forward selection</strong>: The methods starts with a model with only an intercept. In each step one extra regressor can be added. When no extra regressor is selected by the algorithm, the procedure stops.</p></li>
<li><p><strong>backward elimination</strong>: The methods starts with all regressors included as main effects in the model. In each step a regressor can be removed from the model. When no additional regressor is to eliminated by the algorithm, the procedure stops.</p></li>
<li><p><strong>stepwise selection</strong>. The method starts as with forward selection, but after each added regressor the algorithm checks whether one of the previously selected regressors can be removed from the model. The procedure stops when no regressor is to be added or removed.</p></li>
</ul>
<p>All three strategies make use of hypothesis testing in each step:</p>
<ul>
<li><p>a regressor is added to the model it its parameter is significantly different from zero at the <span class="math inline">\(\alpha_\text{in}\)</span> level of significance. When more than one regressor is significant, the regressor with the smallest <span class="math inline">\(p\)</span>-value is selected.</p></li>
<li><p>a regressor is removed from the model if its parameter not significantly different from zero at the <span class="math inline">\(\alpha_\text{out}\)</span> level of significance. When more than on regressor is not significant, the regressor with the largest <span class="math inline">\(p\)</span>-value is removed.</p></li>
</ul>
<p>For the stepwise procedure, <span class="math inline">\(\alpha_\text{in}&lt;\alpha_\text{out}\)</span>.</p>
</div>
</div>
<div id="example-lead-concentration-8" class="section level2 unnumbered">
<h2>Example (Lead concentration)</h2>
<p>We illustrate the stepwise model selection procedure with the lead concentration example. The set of pontential regressors is: <em>Age</em>, <em>Sex</em>, <em>Iqf</em>, <em>Ld72</em>, <em>Ld73</em> and <em>Totyrs</em>.</p>
<p>The stepwise method is now illustrated, with <span class="math inline">\(\alpha_\text{in}=0.05\)</span> and <span class="math inline">\(\alpha_\text{out}=0.10\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">scope&lt;-(<span class="op">~</span>Age<span class="op">+</span>Sex<span class="op">+</span>Iqf<span class="op">+</span>Ld72<span class="op">+</span>Ld73<span class="op">+</span>Totyrs)

m0&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span><span class="dv">1</span>, <span class="dt">data=</span>lead)
<span class="kw">add1</span>(m0,<span class="dt">scope=</span>scope, <span class="dt">test=</span><span class="st">&quot;F&quot;</span>)</code></pre></div>
<pre><code>## Single term additions
## 
## Model:
## MAXFWT ~ 1
##        Df Sum of Sq     RSS    AIC F value    Pr(&gt;F)    
## &lt;none&gt;              13635.8 425.43                      
## Age     1    5808.5  7827.3 381.36 60.1089 2.299e-11 ***
## Sex     1     235.4 13400.4 425.99  1.4230   0.23640    
## Iqf     1     353.8 13282.0 425.25  2.1579   0.14571    
## Ld72    1     961.7 12674.1 421.36  6.1464   0.01524 *  
## Ld73    1    1586.8 12049.0 417.17 10.6672   0.00160 ** 
## Totyrs  1    1476.0 12159.8 417.93  9.8318   0.00239 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Several regressors show significant effects at the <span class="math inline">\(\alpha_\text{in}=0.05\)</span> level of significance and <em>Age</em> has the smallest p-value and hence this regressor is added to the model.</p>
<p>So will will add <em>Age</em> to the model. Since this is the first regressor added, it makes no sense to test whether it can be removed from the model. So we continue with testing for an extra regressor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1&lt;-<span class="kw">update</span>(m0,.<span class="op">~</span>.<span class="op">+</span>Age)
<span class="kw">add1</span>(m1,<span class="dt">scope=</span>scope,<span class="dt">test=</span><span class="st">&quot;F&quot;</span>)</code></pre></div>
<pre><code>## Single term additions
## 
## Model:
## MAXFWT ~ Age
##        Df Sum of Sq    RSS    AIC F value   Pr(&gt;F)   
## &lt;none&gt;              7827.3 381.36                    
## Sex     1      8.18 7819.1 383.28  0.0837 0.773081   
## Iqf     1    516.54 7310.7 377.70  5.6524 0.019819 * 
## Ld72    1    433.93 7393.4 378.63  4.6954 0.033219 * 
## Ld73    1    686.30 7141.0 375.75  7.6885 0.006912 **
## Totyrs  1     40.53 7786.8 382.93  0.4164 0.520583   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Now <em>Ld73</em> can be added to the model. The new model has now two regressors and so it now makes sense to check whether a regressor can be removed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m2&lt;-<span class="kw">update</span>(m1, .<span class="op">~</span>.<span class="op">+</span>Ld73)
<span class="kw">drop1</span>(m2,<span class="dt">test=</span><span class="st">&quot;F&quot;</span>)</code></pre></div>
<pre><code>## Single term deletions
## 
## Model:
## MAXFWT ~ Age + Ld73
##        Df Sum of Sq     RSS    AIC F value    Pr(&gt;F)    
## &lt;none&gt;               7141.0 375.75                      
## Age     1    4908.0 12049.0 417.17 54.9844 1.119e-10 ***
## Ld73    1     686.3  7827.3 381.36  7.6885  0.006912 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Both regressors are significant at the <span class="math inline">\(\alpha_\text{out}=0.10\)</span> level of significance and will thus remain in the model. We continu with checking whether an extra regressor can be added.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">add1</span>(m2,<span class="dt">scope=</span>scope,<span class="dt">test=</span><span class="st">&quot;F&quot;</span>)</code></pre></div>
<pre><code>## Single term additions
## 
## Model:
## MAXFWT ~ Age + Ld73
##        Df Sum of Sq    RSS    AIC F value  Pr(&gt;F)  
## &lt;none&gt;              7141.0 375.75                  
## Sex     1      7.95 7133.0 377.65  0.0880 0.76749  
## Iqf     1    414.95 6726.0 372.78  4.8738 0.03017 *
## Ld72    1      4.45 7136.5 377.69  0.0492 0.82502  
## Totyrs  1      2.25 7138.7 377.72  0.0250 0.87489  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The regressor <em>Iqf</em> can be added.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m3&lt;-<span class="kw">update</span>(m2, .<span class="op">~</span>.<span class="op">+</span>Iqf)
<span class="kw">drop1</span>(m3,<span class="dt">test=</span><span class="st">&quot;F&quot;</span>)</code></pre></div>
<pre><code>## Single term deletions
## 
## Model:
## MAXFWT ~ Age + Ld73 + Iqf
##        Df Sum of Sq     RSS    AIC F value    Pr(&gt;F)    
## &lt;none&gt;               6726.0 372.78                      
## Age     1    5080.0 11806.0 417.47 59.6665 2.995e-11 ***
## Ld73    1     584.7  7310.7 377.70  6.8677   0.01052 *  
## Iqf     1     415.0  7141.0 375.75  4.8738   0.03017 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>None of the regressors can be removed. So we will continue with trying to add an other regressor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">add1</span>(m3,<span class="dt">scope=</span>scope,<span class="dt">test=</span><span class="st">&quot;F&quot;</span>)</code></pre></div>
<pre><code>## Single term additions
## 
## Model:
## MAXFWT ~ Age + Ld73 + Iqf
##        Df Sum of Sq    RSS    AIC F value Pr(&gt;F)
## &lt;none&gt;              6726.0 372.78               
## Sex     1    0.7841 6725.3 374.77  0.0091 0.9243
## Ld72    1   18.0038 6708.0 374.55  0.2093 0.6486
## Totyrs  1    0.8200 6725.2 374.77  0.0095 0.9226</code></pre>
<p>Now no further effects are significant at the <span class="math inline">\(\alpha_\text{in}\)</span> level of significance and hence the procedure stops. The final model is shown next.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m3)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Age + Ld73 + Iqf, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -31.725  -3.208   0.347   6.271  17.630 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 18.59992    8.58465   2.167   0.0333 *  
## Age          2.65538    0.34376   7.724    3e-11 ***
## Ld73        -0.24433    0.09323  -2.621   0.0105 *  
## Iqf          0.15339    0.06948   2.208   0.0302 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.227 on 79 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.5067, Adjusted R-squared:  0.488 
## F-statistic: 27.05 on 3 and 79 DF,  p-value: 3.881e-12</code></pre>
<p>These hypothesis testing based methods show serious issues and raises questions:</p>
<ul>
<li><p>Why would we do it? If there is a clear research question (e.g. assessing whether there is an effect of the blood lead concentration on the average MAXFWT), there is no reason to do model selection. It makes much more sense to think about the presence of confounders that could be added the model (whether significant of not).</p></li>
<li><p>If the purpose is to build a model for predicting outcomes (e.g. predicting the MAXFWT for a child living near the smelter), then it makes sense to select regressors (<em>predictors</em>) that together make up a good prediction model. However, as we have argued before, significance (<span class="math inline">\(p\)</span>-value) of a regression coefficient has not very much to do with the regressor being important for improving the prediction capabilities of the model. To name two arguments:</p>
<ul>
<li><p>the <span class="math inline">\(\beta\)</span> parameters describe the relation between the regressor and the <em>mean</em> outcome (not individual outcomes)</p></li>
<li><p><span class="math inline">\(p\)</span>-values are sensitive to the sample size.</p></li>
</ul></li>
<li><p>A non-significant result is not a proof that the parameter equals zero.</p></li>
<li><p>The final model is the result of multiple hypothesis tests (later we will discuss the problem of multiple hypothesis testing or multiplicity).</p></li>
<li><p>In case the research goal is to assess the effect of a single regressor (e.g. the blood lead concentration), then we want to analyse the data with a model that gives a relevant interperation to the parameter of that regressor (= <em>target parameter</em>). Recall that the interpretation of an effect parameter is conditional on the other regressors in the model. Thus in each step of the model selection procedure, other regressors are in the model and hence the final interpretation of the target parameter depends on the finally selected model. It would be much better to assure that the target parameter has an interpretation that directly serves the research question.</p></li>
<li><p>At some stage, multicollinearity may exist and affect the <span class="math inline">\(p\)</span>-values and hence the model selection procedure.</p></li>
<li><p>None of the three strategies guarantees that the finally selected model is the best model in whatever sense.</p></li>
</ul>
<div id="model-selection-for-building-a-prediction-model" class="section level3 unnumbered">
<h3>Model selection for building a prediction model</h3>
<p>Suppose now that the goal is to build a model for predicting the MAXFWT score of a child. We limit the search to additive models with the following potential regressors (<strong>predictors</strong> in this context): <em>Age</em>, <em>Sex</em>, <em>Iqf</em>, <em>Ld72</em>, <em>Ld73</em> and <em>Totyrs</em>.</p>
<p>Recall from Section <a href="#S:PI">2.12</a> that a good prediction model should result in a small <strong>mean squared error</strong> or <strong>expected conditional test error</strong>, for a given <span class="math inline">\(\mb{x}\)</span>, <span class="math display">\[
  \text{Err}(x) = \Ef{\mb{Y} Y^*}{(\hat{Y}(x)-Y^*)^2} .
\]</span> This can be further averaged over all <span class="math inline">\(\mb{x}\)</span>, resulting in the <strong>expected test error</strong>, <span class="math display">\[
  \text{Err} = \Ef{\mb{Y} \mb{X} Y^* }{(\hat{Y}(X)-Y^*)^2} 
\]</span> in which the expectation is over the sample data <span class="math inline">\((\mb{Y},\mb{X})\)</span> and over the to-be-predicted outcomes <span class="math inline">\(Y^*\)</span>. In the context of building prediction models, the sample data is referred to as the <strong>training data</strong>.</p>
<p>A popular method for estimating the expected test error is via the <strong>cross validation</strong> (CV) method. Here we only present the <strong>Leave one out cross validation</strong> (LOOCV) method:</p>
<p>For each sample observation <span class="math inline">\((\mb{x}_i, Y_i)\)</span>:</p>
<ul>
<li><p>fit model based an all sample data except for observation <span class="math inline">\(i\)</span>. Let <span class="math inline">\(\hat{\mb\beta}_{-i}\)</span> denote the parameter estimate</p></li>
<li><p>predict the outcome for the observation <span class="math inline">\(i\)</span> that was left out: <span class="math inline">\(\hat{Y}^*_i=\hat{\mb\beta}_{-i}^t\mb{x}_i\)</span></p></li>
<li><p>compute the prediction error <span class="math inline">\(e^*_i=\hat{Y}^*_i-Y_i\)</span></p></li>
</ul>
<p>An estimate of the expected test error is then given by <span class="math display">\[
 \text{CV} = \frac{1}{n} \sum_{i=1}^n e^{*2}_i.
\]</span></p>
<p>It can be shown that CV can be computed without the need for refitting <span class="math inline">\(n\)</span> linear models: <span class="math display">\[
  \text{CV} = \frac{1}{n} \sum_{i=1}^n \left(\frac{\hat{Y}_i - Y_i}{1-h_{ii}}\right)^2. 
\]</span></p>
<p>With this criterion we can build a prediction model with similar strategies as before: forward selection, backward elimination or stepwise selection. In each step a predictor is added or removed based on the CV criterion.</p>
<p>Yet another strategy is <strong>best subset</strong> model selection. This method evaluates all models in a predefined set of models. For example, limitting the scope to additive models, and considering <span class="math inline">\(p-1\)</span> potential predictors, we will have to evaluate <span class="math inline">\(2^{p-1}\)</span> models (i.e. each predictor can be either in or out of the model). For each model the CV criterion is computed and the model with the smallest CV is selected as the final model.</p>
</div>
</div>
<div id="exercise-lead-concentration-3" class="section level2 unnumbered">
<h2>Exercise: Lead concentration</h2>
<p>Apply the forward selection strategy with CV as a selection criterion. What is your selected model, and how well does it perform for predicting new outcomes?</p>
<p><details> <summary markdown="span">Try to first make the exercise yourself. You can expand this page to see a solution. </summary></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.age&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Age, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.age<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.age)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 99.15088</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Ld73&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Ld73, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.Ld73<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.Ld73)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 153.2846</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Ld72&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Ld72, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.Ld72<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.Ld72)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 159.1378</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Totyrs&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Totyrs, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.Totyrs<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.Totyrs)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 152.7551</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Iqf&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Iqf, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.Iqf<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.Iqf)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 169.8563</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.sex&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Sex, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.sex<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.sex)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 168.8579</code></pre>
<p>The predictor <em>Age</em> gives the smalles CV criterion and is thus added to the regression model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Ld73&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Age<span class="op">+</span>Ld73, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.Ld73<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.Ld73)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 93.97038</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Ld72&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Age<span class="op">+</span>Ld72, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.Ld72<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.Ld72)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 95.38186</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Totyrs&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Age<span class="op">+</span>Totyrs, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.Totyrs<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.Totyrs)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 99.99147</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Iqf&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Age<span class="op">+</span>Iqf, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.Iqf<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.Iqf)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 94.07439</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.sex&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Age<span class="op">+</span>Sex, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.sex<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.sex)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 101.1353</code></pre>
<p>Now the predictor <em>Ld73</em> gives the smallest CV and is added to the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Ld72&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Age<span class="op">+</span>Ld73<span class="op">+</span>Ld72, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.Ld72<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.Ld72)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 96.07107</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Totyrs&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Age<span class="op">+</span>Ld73<span class="op">+</span>Totyrs, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.Totyrs<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.Totyrs)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 95.55408</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Iqf&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Age<span class="op">+</span>Ld73<span class="op">+</span>Iqf, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.Iqf<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.Iqf)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 90.57014</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.sex&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Age<span class="op">+</span>Ld73<span class="op">+</span>Sex, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.sex<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.sex)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 95.95992</code></pre>
<p>We now add <em>Iqf</em> to the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Ld72&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Age<span class="op">+</span>Ld73<span class="op">+</span>Iqf<span class="op">+</span>Ld72, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.Ld72<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.Ld72)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 92.72711</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Totyrs&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Age<span class="op">+</span>Ld73<span class="op">+</span>Iqf<span class="op">+</span>Totyrs, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.Totyrs<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.Totyrs)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 91.86483</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.sex&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Age<span class="op">+</span>Ld73<span class="op">+</span>Iqf<span class="op">+</span>Sex, <span class="dt">data=</span>lead)
<span class="kw">mean</span>((m.sex<span class="op">$</span>residuals<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">influence</span>(m.sex)<span class="op">$</span>h))<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 92.5748</code></pre>
<p>None of these CVs is smaller than the previous CV (<span class="math inline">\(90.57\)</span>) and hence the model selection procedure stops here.</p>
<p>The fact that adding more terms to the model would increase the CV, means that adding more terms would result in <em>overfitting</em>.</p>
<p>Based on the finally selected model, we estimate that the <em>expected test error</em> equals <span class="math inline">\(90.57\)</span>, and its square root (in the same units as the MAXFWT outcome) equals <span class="math inline">\(9.5\)</span> taps per 10 seconds. More elaborate methods for model building are outside the scope of this course. A final note: the reported estimate of the expected test error is actually still over-optimistic, because it is the result of a minimisation procedure. It would be good to have an independent test dataset for computing a final (and unbiased) estimate of the expected test error.</p>
<p></details></p>

</div>
</div>
<div id="Ch:DesignCausal" class="section level1">
<h1><span class="header-section-number">Hoofdstuk 4</span> Design-related Topics and Causal Inference</h1>
<div id="confounding" class="section level2">
<h2><span class="header-section-number">4.1</span> Confounding</h2>
<div id="causal-diagrams-and-controling-for-confounders" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Causal diagrams and controling for confounders</h3>
</div>
</div>
<div id="example-lead-concentration" class="section level2 unnumbered">
<h2>Example (Lead concentration)</h2>
<p>Recall the lead concentration example. We came to the weird conclusion that the number of years living near the lead smelter has a positive effect on the mean MAXFWT score (whether corrected for blood lead levels or not). When adding age to the additive model we saw that the effect of <em>Totyrs</em> became negative (as intuitively expected) and nearly neglectable. This is an example of <strong>confounding</strong>. Age plays here the role of a <strong>confounder</strong>. To keep things simple, we will ignore <em>Ld73</em>. Here is the regression analysis.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(MAXFWT<span class="op">~</span>Totyrs<span class="op">+</span>Age, <span class="dt">data=</span>lead)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Totyrs + Age, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -33.908  -3.906   1.649   6.760  17.981 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  24.1249     3.8136   6.326 1.35e-08 ***
## Totyrs       -0.2512     0.3893  -0.645    0.521    
## Age           2.9465     0.4396   6.703 2.63e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.866 on 80 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.4289, Adjusted R-squared:  0.4147 
## F-statistic: 30.05 on 2 and 80 DF,  p-value: 1.849e-10</code></pre>
<p>The confounding effect of age can be explained as follows:</p>
<ul>
<li><p>the outcome variable (MAXFWT) is affected by age (the older the child the better its motor functions)</p></li>
<li><p>the regressor <em>Totyrs</em> is also affected by age (the older the child, the larger the probability that it has lived for a longer time near the lead smelter)</p></li>
</ul>
<p>The possitive correlations between MAXFWT and age, and between <em>Totyrs</em> and age can be seen from the next analysis.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(lead[,<span class="kw">c</span>(<span class="st">&quot;MAXFWT&quot;</span>,<span class="st">&quot;Age&quot;</span>,<span class="st">&quot;Totyrs&quot;</span>)], <span class="dt">use=</span><span class="st">&quot;complete.obs&quot;</span>)</code></pre></div>
<pre><code>##           MAXFWT       Age    Totyrs
## MAXFWT 1.0000000 0.6526679 0.3290015
## Age    0.6526679 1.0000000 0.5725720
## Totyrs 0.3290015 0.5725720 1.0000000</code></pre>
<p>These conceptual relationships (i.e. derived from reasoning and prior knowledge and <em>not</em> based on the data) can be visualised in a <strong>causal graph</strong>; see Figure <a href="#fig:CausalLead">4.1</a>. The causal graph shows the directions of the potential causal relationships. Note that these directions are not inferred from the data, but they are based on prior knowledge. It is up to the data analyses to estimate the strength of the effects (effect sizes).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">g1 &lt;-<span class="st"> </span><span class="kw">dagitty</span>(<span class="st">&#39;dag {</span>
<span class="st">    Totyrs [pos=&quot;0,1&quot;]</span>
<span class="st">    MAXFWT [pos=&quot;2,1&quot;]</span>
<span class="st">    Age [pos=&quot;1,0&quot;]</span>
<span class="st">    </span>
<span class="st">    Totyrs &lt;- Age -&gt; MAXFWT</span>
<span class="st">}&#39;</span>)
g2 &lt;-<span class="st"> </span><span class="kw">dagitty</span>(<span class="st">&#39;dag {</span>
<span class="st">    Totyrs [pos=&quot;0,1&quot;]</span>
<span class="st">    MAXFWT [pos=&quot;2,1&quot;]</span>
<span class="st">    Age [pos=&quot;1,0&quot;]</span>
<span class="st">    </span>
<span class="st">    Totyrs -&gt; MAXFWT </span>
<span class="st">    Totyrs &lt;- Age -&gt; MAXFWT</span>
<span class="st">}&#39;</span>)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(g1)
<span class="kw">plot</span>(g2)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:CausalLead"></span>
<img src="DASM2_files/figure-html/CausalLead-1.png" alt="Two causal graphs or diagrams of the Lead concentration example. Age is a confounder. Left: Totyrs has no direct causal effect on MAXFWT. Right: Totyrs has a direct causal effect on MAXFWT." width="672" />
<p class="caption">
Figure 4.1: Two causal graphs or diagrams of the Lead concentration example. Age is a confounder. Left: Totyrs has no direct causal effect on MAXFWT. Right: Totyrs has a direct causal effect on MAXFWT.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p>An edge (arrow) can be interpreted as follows (here in terms of the example):</p>
<ul>
<li><p>if age is increased, the variable <em>Totyrs</em> tends to increase (or decrease). Note that this is (not necessarily) a deterministic effect; there is room for random variation.</p></li>
<li><p>similarly, if age is increased, the MAXFWT score tends to increase (or decrease)</p></li>
<li><p>the interpretation of the edge between <em>Totyrs</em> and MAXFWT needs to account for age, because there is an edge from age arriving into <em>Totyrs</em> and into MAXFWT:</p>
<ul>
<li><p>when there is no edge between <em>Totyrs</em> and MAXFWT, then the causal graph tells us that <em>Totyrs</em> and MAXFWT are <strong>conditionally indepdent</strong>, i.e. given age, there is no dependence between <em>Totyrs</em> and MAXFWT.</p></li>
<li><p>when there is an edge between <em>Totyrs</em> and MAXFWT, then, even when conditioning on age, there still is a direct effect from <em>Totyrs</em> on MAXFWT</p></li>
</ul></li>
</ul>
<p>The next chunck of R code illustrates the conditional (in)dependence for the lead concentration example. Since age is a continuous regressor, it is hard to illustrate the conditoning exactly, because for every different value of age we have at most a few observations. Therefore, we will group the subjects into 4 equally large age categories, and we will then condition on the age category.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(lead<span class="op">$</span>Age)</code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   3.750   6.375   8.667   9.054  12.062  15.917</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">age.quartiles&lt;-<span class="kw">quantile</span>(lead<span class="op">$</span>Age[<span class="op">!</span><span class="kw">is.na</span>(lead<span class="op">$</span>MAXFWT)])
Ind1&lt;-<span class="kw">between</span>(lead<span class="op">$</span>Age,age.quartiles[<span class="dv">1</span>],age.quartiles[<span class="dv">2</span>])
<span class="kw">cor</span>(lead<span class="op">$</span>Totyrs[Ind1],lead<span class="op">$</span>MAXFWT[Ind1],<span class="dt">use=</span><span class="st">&quot;complete.obs&quot;</span>)</code></pre></div>
<pre><code>## [1] -0.04566625</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Ind2&lt;-<span class="kw">between</span>(lead<span class="op">$</span>Age,age.quartiles[<span class="dv">2</span>],age.quartiles[<span class="dv">3</span>])
<span class="kw">cor</span>(lead<span class="op">$</span>Totyrs[Ind2],lead<span class="op">$</span>MAXFWT[Ind2],<span class="dt">use=</span><span class="st">&quot;complete.obs&quot;</span>)</code></pre></div>
<pre><code>## [1] 0.06312945</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Ind3&lt;-<span class="kw">between</span>(lead<span class="op">$</span>Age,age.quartiles[<span class="dv">3</span>],age.quartiles[<span class="dv">4</span>])
<span class="kw">cor</span>(lead<span class="op">$</span>Totyrs[Ind3],lead<span class="op">$</span>MAXFWT[Ind3],<span class="dt">use=</span><span class="st">&quot;complete.obs&quot;</span>)</code></pre></div>
<pre><code>## [1] -0.09796866</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Ind4&lt;-<span class="kw">between</span>(lead<span class="op">$</span>Age,age.quartiles[<span class="dv">4</span>],age.quartiles[<span class="dv">5</span>])
<span class="kw">cor</span>(lead<span class="op">$</span>Totyrs[Ind4],lead<span class="op">$</span>MAXFWT[Ind4],<span class="dt">use=</span><span class="st">&quot;complete.obs&quot;</span>)</code></pre></div>
<pre><code>## [1] -0.06886756</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(lead<span class="op">$</span>Totyrs[Ind1],lead<span class="op">$</span>MAXFWT[Ind1],
     <span class="dt">main=</span><span class="st">&quot;age in first quartile&quot;</span>, 
     <span class="dt">xlab=</span><span class="st">&quot;number of year near smelter&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;MAXFWT (taps per 10 second)&quot;</span>)
<span class="kw">plot</span>(lead<span class="op">$</span>Totyrs[Ind2],lead<span class="op">$</span>MAXFWT[Ind2],
     <span class="dt">main=</span><span class="st">&quot;age in second quartile&quot;</span>, 
     <span class="dt">xlab=</span><span class="st">&quot;number of year near smelter&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;MAXFWT (taps per 10 second)&quot;</span>)
<span class="kw">plot</span>(lead<span class="op">$</span>Totyrs[Ind3],lead<span class="op">$</span>MAXFWT[Ind3],
     <span class="dt">main=</span><span class="st">&quot;age in third quartile&quot;</span>, 
     <span class="dt">xlab=</span><span class="st">&quot;number of year near smelter&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;MAXFWT (taps per 10 second)&quot;</span>)
<span class="kw">plot</span>(lead<span class="op">$</span>Totyrs[Ind4],lead<span class="op">$</span>MAXFWT[Ind4],
     <span class="dt">main=</span><span class="st">&quot;age in fourth quartile&quot;</span>, 
     <span class="dt">xlab=</span><span class="st">&quot;number of year near smelter&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;MAXFWT (taps per 10 second)&quot;</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-136-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p>This analysis illustrates that within age group (i.e. conditional on age), the correlation between <em>Totyrs</em> and MAXFWT is nearly zero.</p>
<p>Let us now look at the marginal correlation between <em>Totyrs</em> and MAXFWT.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(lead<span class="op">$</span>Totyrs,lead<span class="op">$</span>MAXFWT,<span class="dt">use=</span><span class="st">&quot;complete.obs&quot;</span>)</code></pre></div>
<pre><code>## [1] 0.3290015</code></pre>
<p>Thus marginally there is a (weak) possitive correlation.</p>
<p>If we consider age again as a continuous regressor, we also have a solution for looking at the conditional (in)dependence between <em>Totyrs</em> and MAXFWT: the additive linear regression model with MAXFWT as outcome, and <em>Totyrs</em> and age as regressors. In this model, the regression parameter for the main effect of <em>Totyrs</em> is interpreted as the effect of <em>Totyrs</em> on the mean MAXFWT score, conditional on age (i.e. given age).</p>
</div>
<div id="example-diabetes" class="section level2 unnumbered">
<h2>Example (Diabetes)</h2>
<p>This is an example of the causal graph in the left panel of Figure <a href="#fig:CausalLead">4.1</a>.</p>
<p>Consider a placebo-controlled clinical study that aims to test a new treatment for diabetes 2 patients. The drug aims to reduce the blood glucose levels. It is, however, not a completely randomised study. The clinician decided which patient to give the new drug and which patient to give placebo. In particular,</p>
<ul>
<li><p>patients with a poor general condition were more likely to get the new drug</p></li>
<li><p>patients with a good general condition were more likely to get the placebo</p></li>
</ul>
<p>The outcome of interest is the blood glucose level reduction after one week of treatment.</p>
<p>The next chunck of R code shows a data exploration and the data analysis with regression models. Here is the meaning of the variable names in the dataset:</p>
<ul>
<li><p><em>condition</em>: poor (1) or good (0) general condition</p></li>
<li><p><em>treatment</em>: placebo (0) or new drug (1)</p></li>
<li><p><em>glucose</em>: blood glucose level reduction (mmol / 100 ml)</p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;Data/Diabetes.RData&quot;</span>)
<span class="kw">table</span>(Diabetes<span class="op">$</span>condition,Diabetes<span class="op">$</span>treatment)</code></pre></div>
<pre><code>##    
##      0  1
##   0 45 12
##   1  3 26</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">boxplot</span>(Diabetes<span class="op">$</span>glucose<span class="op">~</span>Diabetes<span class="op">$</span>treatment, <span class="dt">main=</span><span class="st">&quot;all data&quot;</span>,
        <span class="dt">xlab=</span><span class="st">&quot;treatment&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;reduction of blood glucode (mmol / 100ml)&quot;</span>, 
        <span class="dt">cex.axis=</span><span class="fl">1.3</span>,<span class="dt">cex.lab=</span><span class="fl">1.3</span>)
<span class="kw">boxplot</span>(Diabetes<span class="op">$</span>glucose<span class="op">~</span>Diabetes<span class="op">$</span>condition, <span class="dt">main=</span><span class="st">&quot;all data&quot;</span>,
        <span class="dt">xlab=</span><span class="st">&quot;condition&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;reduction of blood glucode (mmol / 100ml)&quot;</span>, 
        <span class="dt">cex.axis=</span><span class="fl">1.3</span>,<span class="dt">cex.lab=</span><span class="fl">1.3</span>)
<span class="kw">boxplot</span>(Diabetes<span class="op">$</span>glucose[Diabetes<span class="op">$</span>condition<span class="op">==</span><span class="dv">0</span>]<span class="op">~</span>Diabetes<span class="op">$</span>treatment[Diabetes<span class="op">$</span>condition<span class="op">==</span><span class="dv">0</span>],
        <span class="dt">main=</span><span class="st">&quot;condition = 0&quot;</span>,
        <span class="dt">xlab=</span><span class="st">&quot;treatment&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;reduction of blood glucode (mmol / 100ml)&quot;</span>, 
        <span class="dt">cex.axis=</span><span class="fl">1.3</span>,<span class="dt">cex.lab=</span><span class="fl">1.3</span>)
<span class="kw">boxplot</span>(Diabetes<span class="op">$</span>glucose[Diabetes<span class="op">$</span>condition<span class="op">==</span><span class="dv">1</span>]<span class="op">~</span>Diabetes<span class="op">$</span>treatment[Diabetes<span class="op">$</span>condition<span class="op">==</span><span class="dv">1</span>],
        <span class="dt">main=</span><span class="st">&quot;condition = 1&quot;</span>,
        <span class="dt">xlab=</span><span class="st">&quot;treatment&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;reduction of blood glucode (mmol / 100ml)&quot;</span>, 
        <span class="dt">cex.axis=</span><span class="fl">1.3</span>,<span class="dt">cex.lab=</span><span class="fl">1.3</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-138-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))

m1&lt;-<span class="kw">lm</span>(glucose<span class="op">~</span>treatment,<span class="dt">data=</span>Diabetes)
<span class="kw">summary</span>(m1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = glucose ~ treatment, data = Diabetes)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.0105 -0.6105 -0.1917  0.7895  2.6083 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1.1917     0.1309   9.102 3.70e-14 ***
## treatment     0.9189     0.1970   4.665 1.15e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9071 on 84 degrees of freedom
## Multiple R-squared:  0.2058, Adjusted R-squared:  0.1963 
## F-statistic: 21.76 on 1 and 84 DF,  p-value: 1.152e-05</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m2&lt;-<span class="kw">lm</span>(glucose<span class="op">~</span>treatment<span class="op">+</span>condition,<span class="dt">data=</span>Diabetes)
<span class="kw">summary</span>(m2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = glucose ~ treatment + condition, data = Diabetes)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.09902 -0.66999 -0.09499  0.60501  1.30501 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.09499    0.10445  10.484  &lt; 2e-16 ***
## treatment   -0.04285    0.20575  -0.208    0.836    
## condition    1.54688    0.21613   7.157 3.01e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7176 on 83 degrees of freedom
## Multiple R-squared:  0.5089, Adjusted R-squared:  0.4971 
## F-statistic:    43 on 2 and 83 DF,  p-value: 1.527e-13</code></pre>
<p>If we ignore the effect of <em>condition</em>, then we look at the first boxplot and the results of model <em>m1</em>. These results tell us that we have a strong positive effect of the new drug. It is estimated that the blood glucose levels reduce with <span class="math inline">\(0.92\)</span> mmol / 100ml (SE <span class="math inline">\(0.20\)</span> mmol / 100 ml) as compared to placebo. This effect is significant at the 5% level of significance (<span class="math inline">\(p&lt;0.0001\)</span>).</p>
<p>However, when we account for <em>condition</em> (i.e. condition on condition) then the effect of treatment vanishes. It is now estimated as <span class="math inline">\(-0.04\)</span> mmol / 100ml (SE <span class="math inline">\(0.21\)</span> mmol / 100ml) and this is no longer significant at the 5% level of significance (<span class="math inline">\(p=0.863\)</span>). So there is no evidence at all for a treatment effect, within the condition groups. This can also be seen from the two boxplots for condition=0 and condition=1.</p>
<p>The variable <em>condition</em> acts as a confounder. The condition of the patient had an effect on the probability of receiving treatment, and the condition also affected the outcome of the patient, because the poor condition was a consequence of the severity of the diabetes (i.e. condition=1 patients were mostly patients with a large blood glucose level before the start of the treatment) and patients starting off with high blood glucose levels had more chance for showing a large reduction in the blood levels. Thus, <em>condition</em> is a confounder. However, within the <em>condition</em> groups there was no effect of treatment. There is thus no direct arrow from <em>treatment</em> to <em>glucose</em> in the causal diagram.</p>
<p>Suppose now that a similar study is set up, but this the patients are properly randomised (thus without first looking at their general condition). In this case the condition does not affect the treatment allocation, and hence in the causal diagram there is no arrow from <em>condition</em> into <em>treatment</em>. If this edge is missing, <em>condition</em> is no longer a confounder and there is no need for including <em>condition</em> into the model. Would you still be allowed to include it in the model? The answer to this question is given later in this chapter.</p>
<div id="causality-and-confounders" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Causality and confounders</h3>
<p>In the previous two examples, we identified a confounder and we argued that the model should include the confounder as a regressor. Let <span class="math inline">\(Y\)</span> denote the outcome (as usual), <span class="math inline">\(X\)</span> the target regressor (i.e. we aim at estimating the causal effect of <span class="math inline">\(X\)</span>), and let <span class="math inline">\(Z\)</span> denote the confounder variable (it may also be a vector). Without going into the more techical aspects of causality, as we did in Section <a href="#S:Causality">2.10</a>, we note here that a <strong>causal effect</strong> can be unbiasedly estimated if</p>
<ul>
<li><p>the confounder is included in the model</p></li>
<li><p>the linear model <span class="math inline">\(m(\mb{x})\)</span> is correctly specified and all other required model assumptions hold true</p></li>
<li><p><strong>consistency</strong> and <strong>mean exchangeability</strong>, as in Section <a href="#S:Causality">2.10</a>, still hold, but the latter must be replaced by <strong>conditional mean exchangeability</strong>, <span class="math display">\[
  \E{Y(a) \mid  X=0, Z=z} = \E{Y(a) \mid  X=1, Z=z} = \E{Y(a) \mid Z=z}
\]</span> for <span class="math inline">\(a=0,1\)</span> and for all confounder outcomes <span class="math inline">\(z\)</span>. (this property is sometimes also referred to as <strong>ignorability</strong>)</p></li>
</ul>
<p>This all sounds very promissing, but there is a very important implication: what if not all confounders are observed or known? Confounders that are not in the dataset are known as <strong>unmeasured confounders</strong>. So we may only be sure that we can estimate a causal effect, if we are very confident that we have observed all confounders!</p>
</div>
<div id="colliders" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Colliders</h3>
</div>
</div>
<div id="example-study-time" class="section level2 unnumbered">
<h2>Example (Study time)</h2>
<p>Researchers suspect that the weekly number of hours a 15 year old child spends on working for school at home, is related to the level of the highest educational degree of its mother. They have a random sample of 56 fifteen year old school kids in England. The dataset has the following variables:</p>
<ul>
<li><p><em>study.time</em>: weekly number of hours of working for school at home</p></li>
<li><p><em>mother</em>: highest educational degree of the mother (1: secondary school; 2: bachelor; 3: master; 4: PhD)</p></li>
<li><p><em>school.result</em>: latest school result (as %)</p></li>
</ul>
<p>How should we analyse this dataset? Do we want to account for the school result? Let us explore the dataset and try some regression analyses.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;Data/School.RData&quot;</span>)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">boxplot</span>(School<span class="op">$</span>study.time<span class="op">~</span>School<span class="op">$</span>mother, <span class="dt">main=</span><span class="st">&quot;all data&quot;</span>,
        <span class="dt">xlab=</span><span class="st">&quot;highest educational degree of mother&quot;</span>, 
        <span class="dt">ylab=</span><span class="st">&quot;weekly study time (hours)&quot;</span>, 
        <span class="dt">cex.axis=</span><span class="fl">1.3</span>,<span class="dt">cex.lab=</span><span class="fl">1.3</span>)
<span class="kw">plot</span>(School<span class="op">$</span>study.time,School<span class="op">$</span>school.result, <span class="dt">main=</span><span class="st">&quot;all data&quot;</span>,
        <span class="dt">ylab=</span><span class="st">&quot;School result&quot;</span>, 
        <span class="dt">xlab=</span><span class="st">&quot;weekly study time (hours)&quot;</span>, 
        <span class="dt">cex.axis=</span><span class="fl">1.3</span>,<span class="dt">cex.lab=</span><span class="fl">1.3</span>)
<span class="kw">boxplot</span>(School<span class="op">$</span>school.result<span class="op">~</span>School<span class="op">$</span>mother, <span class="dt">main=</span><span class="st">&quot;all data&quot;</span>,
        <span class="dt">ylab=</span><span class="st">&quot;School result&quot;</span>, 
        <span class="dt">xlab=</span><span class="st">&quot;highest educational degree of mother&quot;</span>, 
        <span class="dt">cex.axis=</span><span class="fl">1.3</span>,<span class="dt">cex.lab=</span><span class="fl">1.3</span>)
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-139-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1&lt;-<span class="kw">lm</span>(study.time<span class="op">~</span>mother,<span class="dt">data=</span>School)
<span class="kw">summary</span>(m1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = study.time ~ mother, data = School)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3436 -1.2936  0.0247  1.3089  3.5930 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 10.18872    0.69998  14.556   &lt;2e-16 ***
## mother      -0.08172    0.27569  -0.296    0.768    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.852 on 54 degrees of freedom
## Multiple R-squared:  0.001624,   Adjusted R-squared:  -0.01686 
## F-statistic: 0.08786 on 1 and 54 DF,  p-value: 0.7681</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m2&lt;-<span class="kw">lm</span>(study.time<span class="op">~</span>mother<span class="op">+</span>school.result,<span class="dt">data=</span>School)
<span class="kw">summary</span>(m2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = study.time ~ mother + school.result, data = School)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.8066 -0.9524  0.1709  0.9350  2.8406 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    0.16846    1.64454   0.102  0.91880    
## mother        -0.72922    0.23150  -3.150  0.00268 ** 
## school.result  0.17597    0.02734   6.436 3.64e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.401 on 53 degrees of freedom
## Multiple R-squared:  0.4396, Adjusted R-squared:  0.4184 
## F-statistic: 20.79 on 2 and 53 DF,  p-value: 2.167e-07</code></pre>
<p>When not controlling for the school results, there seems to be no evidence of an effect of the educational level of the mother on the average weekly study time of the children. However, when controlling for school results, the effect of the educational level of the mother becomes important and statistically significant at the 5% level of significance (<span class="math inline">\(p=0.003\)</span>): among children with the same school results, we estimate that for each one step increase in the educational level of the mother, the children spend on average about <span class="math inline">\(0.73*60 \approx\)</span> 45 minutes per week less time working for school.</p>
<p>Which of the two regression models gives us the best estimate for the caual effect of the educational level of the mother? This time the answer is that we <em>may not</em> control for the school result, because it is a <em>collider</em> instead of a <em>confounder</em>. It is not the school result that affects the study time or the mother's educational level, it is rather the opposite. This is also illustrated in the causal diagram in Figure <a href="#fig:CausalCollider">4.2</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">g3 &lt;-<span class="st"> </span><span class="kw">dagitty</span>(<span class="st">&#39;dag {</span>
<span class="st">    mother [pos=&quot;0,1&quot;]</span>
<span class="st">    study_time [pos=&quot;2,1&quot;]</span>
<span class="st">    school_result [pos=&quot;1,0&quot;]</span>
<span class="st">    </span>
<span class="st">    mother -&gt; study_time</span>
<span class="st">    mother -&gt; school_result &lt;- study_time</span>
<span class="st">}&#39;</span>)

<span class="kw">plot</span>(g3)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:CausalCollider"></span>
<img src="DASM2_files/figure-html/CausalCollider-1.png" alt="A causal diagram for the Study Time example. The school result acts a a collider." width="672" />
<p class="caption">
Figure 4.2: A causal diagram for the Study Time example. The school result acts a a collider.
</p>
</div>
<p>Controlling for a collider may introduce correlation between the outcome and the other regressor, which may thus result in the false conclusion that there is a causal effect of the mother's educational level on the expected study time.</p>
<p>This is illustrated in the following simulation study. We start from the observed data on the mother's eductional level, and for the parameters of the data generating model we make use of the following fitted regression model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m&lt;-<span class="kw">lm</span>(school.result<span class="op">~</span>mother<span class="op">+</span>study.time, <span class="dt">data=</span>School)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = school.result ~ mother + study.time, data = School)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -11.099  -3.809   0.264   2.822  11.296 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  31.5438     4.4209   7.135 2.73e-09 ***
## mother        3.8833     0.7854   4.945 8.07e-06 ***
## study.time    2.4928     0.3873   6.436 3.64e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.271 on 53 degrees of freedom
## Multiple R-squared:  0.5447, Adjusted R-squared:  0.5275 
## F-statistic:  31.7 on 2 and 53 DF,  p-value: 8.814e-10</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">db&lt;-<span class="kw">data.frame</span>(<span class="dt">mother=</span>School<span class="op">$</span>mother)

betas&lt;-<span class="kw">c</span>()
pvalues&lt;-<span class="kw">c</span>()

<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N1000) {
  <span class="co"># For the study time we permute the observed study times.</span>
  <span class="co">#   This will assure that mother and study time are independent</span>
  db<span class="op">$</span>study.time&lt;-<span class="kw">sample</span>(School<span class="op">$</span>study.time,<span class="dt">replace =</span> <span class="ot">FALSE</span>)
  
  <span class="co"># Next we construct a collider (parameters from fitted model on real data)</span>
  db<span class="op">$</span>school.result&lt;-<span class="kw">round</span>(<span class="dv">32</span><span class="op">+</span><span class="dv">4</span><span class="op">*</span>db<span class="op">$</span>mother<span class="op">+</span><span class="fl">2.5</span><span class="op">*</span>db<span class="op">$</span>study.time,<span class="dv">0</span>)<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">56</span>,<span class="dt">mean=</span><span class="dv">0</span>,<span class="dt">sd=</span><span class="dv">5</span>)
  
  <span class="co"># now we fit the model that includes the collider</span>
  m&lt;-<span class="kw">lm</span>(study.time<span class="op">~</span>mother<span class="op">+</span>school.result, <span class="dt">data=</span>db)
  betas&lt;-<span class="kw">c</span>(betas,<span class="kw">coef</span>(m)[<span class="dv">2</span>]) <span class="co"># parameter estimate for effect of mother</span>
  pvalues&lt;-<span class="kw">c</span>(pvalues,<span class="kw">summary</span>(m)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">4</span>])
}

<span class="kw">hist</span>(betas, <span class="dt">main=</span><span class="st">&quot;parameter estimates of effect of mother controlled for school result&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">mean</span>(betas), <span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-141-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(pvalues, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),
     <span class="dt">main=</span><span class="st">&quot;Two-sided p-values of effect of mother controlled for school result&quot;</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-141-2.png" width="672" /></p>
<p>These results very clearly illustrate that the causal effect cannot be estimated unbiasedly when the model controls for a collider. This results in spurious correlation between the outcome and the regressor of interest.</p>
</div>
<div id="collapsibility" class="section level2">
<h2><span class="header-section-number">4.2</span> Collapsibility</h2>
<p>Another related, but different concept is <strong>collapsibility</strong>. We will see that linear models are often <strong>collapsible</strong> for the target effect (i.e. the effect of interest). In later courses (e.g. GLM) you will come accross models that do not have this collapsibility proporty and that will cause some interpration issues.</p>
</div>
<div id="example-blood-pressure" class="section level2 unnumbered">
<h2>Example (Blood pressure)</h2>
<p>Recall the blood pressure example, but this time we will simplify the problem by looking only at two doses: dose=0 (placebo) and the 2mg/day dose (active treatment group). These groups are coded in the dataset as <em>treatment=0</em> and <em>treatment=1</em>, respectively. The research question is to test whether the treatment has an effect on the average blood pressure reduction. We also have the age of the patients. Recall that this is a randomised study: 20 patients are randomised over the two treatment groups.</p>
<p>Let us think about a causal diagram for this study. Since it is a randomised study, the age cannot affect the treatment, and obviously the treatment cannot affect the age. However, it may be possible that the age affects how a patient responds to treatment, i.e. age may affect the blood pressure reduction (not the other way around). This is visualised in Figure <a href="#fig:CausalBP2">4.3</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">g4 &lt;-<span class="st"> </span><span class="kw">dagitty</span>(<span class="st">&#39;dag {</span>
<span class="st">    treatment [pos=&quot;0,1&quot;]</span>
<span class="st">    bp.reduction [pos=&quot;2,1&quot;]</span>
<span class="st">    age [pos=&quot;1,0&quot;]</span>
<span class="st">    </span>
<span class="st">    treatment -&gt; bp.reduction &lt;- age</span>
<span class="st">}&#39;</span>)

<span class="kw">plot</span>(g4)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:CausalBP2"></span>
<img src="DASM2_files/figure-html/CausalBP2-1.png" alt="The causal diagram for the blood pressure example." width="672" />
<p class="caption">
Figure 4.3: The causal diagram for the blood pressure example.
</p>
</div>
<p>With what model should the data be analysed for answering the research question? From the causal diagram we can already see that <em>age</em> is not a confounder, nor a collider. Since it is not a confounder, we are not obliged to include <em>age</em> in the model, and since it is not a collider this is no reason to not include <em>age</em> in the model.</p>
<p>The concept of <strong>collapsibility</strong> may help us to make a decision. We will introduce this concept with the following notation:</p>
<ul>
<li><p><span class="math inline">\(Y\)</span>: outcome</p></li>
<li><p><span class="math inline">\(X\)</span>: target regressor</p></li>
<li><p><span class="math inline">\(Z\)</span>: covariate. Thus <span class="math inline">\(Z\)</span> is a potential regressor, which we refer to as a <em>covariate</em> (i.e. it co-varies with the outcome).</p></li>
<li><p><span class="math inline">\(f_{xy\mid z}\)</span>: density function of the distribution of <span class="math inline">\((Y,X) \mid Z\)</span></p></li>
<li><p><span class="math inline">\(f_{xy}\)</span>: density function of the distribution of <span class="math inline">\((Y,X)\)</span>.</p></li>
<li><p><span class="math inline">\(g(\cdot)\)</span> is a <strong>functional</strong>, i.e. a mapping from a density function to the real line <span class="math inline">\(\mathbb{R}\)</span>. The (conditional) expectation is an example of such a functional.</p></li>
</ul>
<p>We say that <span class="math inline">\(g(\cdot)\)</span> is collapsible on <span class="math inline">\(Z\)</span> if <span class="math display">\[
  \Ef{Z}{g(f_{xy\mid z}(Y,X \mid Z))} = g(f_{xy}(Y,X)).
\]</span></p>
<p>If <span class="math inline">\(g(\cdot)\)</span> is the indentity function, the collapsibility condition is trivial: <span class="math display">\[
  \Ef{Z}{f_{xy\mid z}(Y,X \mid Z)} = \int_{-\infty}^{+\infty} f_{xy\mid z}(Y,X \mid z) f_z(z) dz = \int_{-\infty}^{+\infty} f_{xy\mid z}(Y,Z,z)  dz = f_{xy}(Y,X) = g(f_{xy}(Y,X)).
\]</span></p>
<p>We now apply the collapsibility definition to <span class="math inline">\(g(.)\)</span> being the target effect according to the two following statistical models <span class="math display">\[
  {\cal{M}}_{xz}: \E{Y \mid X, Z} = \beta_0 + \beta_x X + \beta_z Z
\]</span> and <span class="math display">\[
  {\cal{M}}_{x}: \E{Y \mid X} = \alpha_0 + \alpha_x X.
\]</span></p>
<p>Consider, for some density function <span class="math inline">\(f_{xy}\)</span> of <span class="math inline">\((Y,X)\)</span>, the following functional: <span class="math display">\[
  g(f_{xy}) = \E{Y \mid X=x+1} -  \E{Y \mid X=x}.
\]</span> When Model <span class="math inline">\({\cal{M}}_{x}\)</span> is conisdered, this becomes <span class="math display">\[
   g(f_{xy}) = \E{Y \mid X=x+1} -  \E{Y \mid X=x} = \alpha_x
\]</span> which is the target effect size of <span class="math inline">\(X\)</span> in model <span class="math inline">\({\cal{M}}_{x}\)</span>.</p>
<p>When we then apply the same <span class="math inline">\(g(\cdot)\)</span> to the density function of <span class="math inline">\((Y,X)\)</span>, conditional on <span class="math inline">\(Z\)</span>, we get <span class="math display">\[
  g(f_{xy\mid z}) = \E{Y \mid X=x+1, Z=z} -  \E{Y \mid X=x, Z=z}.
\]</span> When applied to Model <span class="math inline">\({\cal{M}}_{xz}\)</span>, we find <span class="math display">\[
  g(f_{xy\mid z}) = \E{Y \mid X=x+1, Z=z} -  \E{Y \mid X=x, Z=z} = \beta_x.
\]</span></p>
<p>Hence, the effect size in Model <span class="math inline">\({\cal{M}}_{xz}\)</span> is collapsible on variable <span class="math inline">\(Z\)</span> if <span class="math display">\[
  \Ef{Z}{g(f_{xy\mid z}(Y,X \mid Z))} = g(f_{xy}(Y,X))
\]</span> or, equivalently, <span class="math display">\[
  \beta_x=\Ef{Z}{\E{Y \mid X=x+1, Z} -  \E{Y \mid X=x, Z}} = \E{Y \mid X=x+1} -  \E{Y \mid X=x} = \alpha_x.
\]</span> This holds if, <span class="math display">\[
  \Ef{Z}{\E{Y \mid X, Z}} = \E{Y \mid X}.
\]</span> This equality will always hold if any of the following independences hold:</p>
<ul>
<li><p><span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> are conditionally independent, given <span class="math inline">\(X\)</span>, i.e, <span class="math display">\[
   Y \ind Z \mid X,
\]</span></p></li>
<li><p><span class="math inline">\(Z\)</span> is independent of <span class="math inline">\(X\)</span>, i.e. <span class="math display">\[
  Z \ind X.
\]</span></p></li>
</ul>
<p>This can be seen as follows:</p>
<ul>
<li><p>The condition <span class="math inline">\(Y \ind Z \mid X\)</span> says that if <span class="math inline">\(X\)</span> is known, <span class="math inline">\(Z\)</span> provides no additional information on the distribution of <span class="math inline">\(Z\)</span>. Hence <span class="math inline">\(\E{Y\mid X,Z} = \E{Y\mid X}\)</span></p></li>
<li>If <span class="math inline">\(Z \ind Z\)</span>, we can make the following calculations (the independence property is used when going from line one to line two):
<span class="math display">\[\begin{eqnarray*}
 \Ef{Z}{\E{Y \mid X,Z}}
   &amp;=&amp; \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} y f(y\mid x,z) dy f(z) dz \\
   &amp;=&amp; \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} y \frac{f(y, x,z)}{f(x) f(z)} dy f(z) dz \\
   &amp;=&amp; \int_{-\infty}^{+\infty} \frac{y}{f(x)} \int_{-\infty}^{+\infty} f(y, x,z) dz dy \\
   &amp;=&amp; \int_{-\infty}^{+\infty} \frac{y}{f(x)}  f(y, x) dy \\
   &amp;=&amp; \int_{-\infty}^{+\infty}y  f(y\mid x) dy \\
   &amp;=&amp; \E{Y\mid X}
\end{eqnarray*}\]</span></li>
</ul>
<p>When applied to the blood pressure example with <span class="math inline">\(Z\)</span> the age, <span class="math inline">\(X\)</span> the treatment and <span class="math inline">\(Y\)</span> the blood pressure reduction, we can immediately see that <span class="math inline">\(Z \ind X\)</span> (age is independent of treatment), which is guaranteed by the randomisation of the treatment. This can also be read from the causal diagram in Figure <a href="#fig:CausalBP2">4.3</a>.</p>
<p>What is collapsibility telling us for the blood pressure example?</p>
<ul>
<li><p>the research questions has nothing to do with the age, and the causal effect of interest is given by the parameter <span class="math inline">\(\alpha_x\)</span> in Model <span class="math inline">\({\cal{M}}_x\)</span>.</p></li>
<li><p>if we add age to the model, the (conditional) treatment effect is given by the parameter <span class="math inline">\(\beta_x\)</span> in Model <span class="math inline">\({\cal{M}}_{xz}\)</span>.</p></li>
<li><p>collapsibility tells us that <span class="math inline">\(\beta_x=\alpha_x\)</span>, and hence we can choose to work with either Model <span class="math inline">\({\cal{M}}_x\)</span> or Model <span class="math inline">\({\cal{M}}_{xz}\)</span>.</p></li>
</ul>
<p>On the one hand we have a convenient answer: both models are good and will provide us with unbiased estimators of the target causal effect. Can we find arguments to prefer one model over the other? We will come back to this question, but first we show the results of the two model fits.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;Data/Bloodpressure2.RData&quot;</span>)

m.x&lt;-<span class="kw">lm</span>(bp.reduction<span class="op">~</span>treatment, <span class="dt">data=</span>BloodPressure2)
<span class="kw">summary</span>(m.x)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = bp.reduction ~ treatment, data = BloodPressure2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -7.10  -1.75   0.20   1.50   5.90 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)   -0.900      1.168  -0.771  0.45083   
## treatment      5.400      1.651   3.270  0.00425 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.692 on 18 degrees of freedom
## Multiple R-squared:  0.3727, Adjusted R-squared:  0.3378 
## F-statistic: 10.69 on 1 and 18 DF,  p-value: 0.004252</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.xz&lt;-<span class="kw">lm</span>(bp.reduction<span class="op">~</span>treatment<span class="op">+</span>Age, <span class="dt">data=</span>BloodPressure2)
<span class="kw">summary</span>(m.xz)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = bp.reduction ~ treatment + Age, data = BloodPressure2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0220 -1.3936  0.3129  1.3029  2.8220 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 33.69677    4.92470   6.842 2.86e-06 ***
## treatment    5.78441    0.85692   6.750 3.40e-06 ***
## Age         -0.64068    0.09051  -7.079 1.85e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.912 on 17 degrees of freedom
## Multiple R-squared:  0.8411, Adjusted R-squared:  0.8224 
## F-statistic: 44.99 on 2 and 17 DF,  p-value: 1.621e-07</code></pre>
<p>From model <em>m.x</em> we conclude that average blood pressure reduction in the treatment group is estimated to be <span class="math inline">\(5.4\)</span> mmHg (SE <span class="math inline">\(1.65\)</span> mmHg) larger than in the placebo group. This is significant at the 5% level of significance (<span class="math inline">\(p=0.0043\)</span>).</p>
<p>From model <em>m.xz</em> we conclude that among people of the same age, the average blood pressure reduction in the treatment group is estimated to be <span class="math inline">\(5.8\)</span> mmHg (SE <span class="math inline">\(0.86\)</span> mmHg) larger than in the placebo group. This is significant at the 5% level of significance (<span class="math inline">\(p&lt;0.0001\)</span>).</p>
<p>Discussion:</p>
<ul>
<li><p>Although in both models the effect size of the treatment is the same (<span class="math inline">\(\alpha_x = \beta_x\)</span>), the estimates are numerically different. Note that this is not a problem from a theoretical perspective. Both estimates result from unbiased estimators of the same causal effect.</p></li>
<li><p>The standard error (SE) of the causal effect estimate in the model with the age effect (<em>m.xz</em>) is smaller than in the model without the age effect. This is not true in general, but it a beneficial effect of adding the covariate.</p></li>
<li><p>The <span class="math inline">\(p\)</span>-value for the causal effect in the model with the age effect (<em>m.xz</em>) is smaller than in the model without the age effect. This is here a consequence of the smaller SE in the former model.</p></li>
<li><p>In model <em>m.xz</em> we have given the default interpretation of the effect of the treatment, i.e. the effect of the treatment controlled for age. However, now that we know that collapsibility gives <span class="math inline">\(\alpha_x=\beta_x\)</span>, we were not required to give the conditional interpretation The marginal interpretation, as in model <em>m.x</em>, is also correct.</p></li>
</ul>
<p>For this particular case study, the inclusion of age seems to be advantegous: it gives a smaller SE and hence a more narrow confidence interval for the causal effect size. A smaller SE also results in a smaller <span class="math inline">\(p\)</span>-value and hence in larger statistical power. Adding a covariate to the model, however, does not always result in a reduction of the SE. It only happens if the covariate is correlated with the outcome.</p>
<p>In this section, we have fitted two models to the dataset. This was for illustration purposes only. In a real setting you may not first fit the two models, and then select the model that gives you the <em>best</em> or desired results!</p>
<p>Finally, we show some exploratory graphs for the <em>BloodPressure2</em> dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(BloodPressure2<span class="op">$</span>Age<span class="op">~</span>BloodPressure2<span class="op">$</span>treatment, 
        <span class="dt">xlab=</span><span class="st">&quot;treatment group (0: placebo -- 1: active treatment)&quot;</span>,
        <span class="dt">ylab=</span><span class="st">&quot;age (years)&quot;</span>,
        <span class="dt">cex.axis=</span><span class="fl">1.5</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-143-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(BloodPressure2<span class="op">$</span>Age[BloodPressure2<span class="op">$</span>treatment<span class="op">==</span><span class="dv">0</span>])</code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   48.00   49.50   52.50   54.00   59.25   61.00</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(BloodPressure2<span class="op">$</span>Age[BloodPressure2<span class="op">$</span>treatment<span class="op">==</span><span class="dv">1</span>])</code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   48.00   51.75   55.00   54.60   57.00   62.00</code></pre>
<p>These boxplots need to be interpreted with care, because each is based on only 10 observations. The results show that the age distributions in the two treatment groups are similar. This is of course a consequence of the randomisation which assures age <span class="math inline">\(\ind\)</span> treatment.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(BloodPressure2<span class="op">$</span>Age,BloodPressure2<span class="op">$</span>bp.reduction,
     <span class="dt">xlab=</span><span class="st">&quot;age (years)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;Blood pressure reduction (mmHg)&quot;</span>,
     <span class="dt">cex.axis=</span><span class="fl">1.5</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-144-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(BloodPressure2<span class="op">$</span>Age,BloodPressure2<span class="op">$</span>bp.reduction)</code></pre></div>
<pre><code>## [1] -0.6443274</code></pre>
<p>These results show a negative correlation between the covariate <em>age</em> and the outcome variable.</p>
</div>
<div id="randomisation-restriction" class="section level2">
<h2><span class="header-section-number">4.3</span> Randomisation restriction</h2>
<p>Consider the following (artificial) example. The objective is to assess the effect of treatment (<em>treatment</em>=1) versus placebo (<em>treatment</em>=0) on the mean outcome. The researchers think that gender may have an effect on the outcome. For this reason they want to guarantee a balance between men and women in the study. For this reason they setup the following randomisation scheme: for each gender 10 subjects are randomly assigned to treatment and 10 subjects are assigned to placebo.</p>
<p>This design strongly resembles a randomised design, except that randomisation is performed within groups (gender). How should this dataset be analysed? In particular, should gender be included as a regressor in the model, or not?</p>
<p>In our example gender imposes a <strong>randomisation restriction</strong>. If it were a completely randomised design that does not make use of the gender, then with a total sample size of <span class="math inline">\(n\)</span>, the number of possible assignments of <span class="math inline">\(n\)</span> subjects in two groups of <span class="math inline">\(n_0=n_1\)</span> subjects, is given by <span class="math inline">\({n \choose n_1}\)</span>. For example, with <span class="math inline">\(n=40\)</span> and <span class="math inline">\(n_0=n_1=20\)</span>, <span class="math inline">\({n \choose n_1}={40 \choose 20}\)</span>=137846528820. On the other hand, when we randomise subjects to treatment within the two gender groups, with the restriction that for each gender we randomise 20 men or women to 10 active treatments and 10 placebos, the number of possible treatment assignments is given by <span class="math inline">\({20 choose 10}\times {20 choose 10}\)</span>=34134779536, which is approximately a factor 4 smaller as compared to the unrestricted randomisation.</p>
<p>In this context, the gender is referred to as a <strong>stratification factor</strong> or a <strong>blocking factor</strong>. Each gender (man or woman) is then referred to as a <strong>stratum</strong> or a <strong>block</strong>.</p>
<p>Can we think of <em>gender</em> as a confounder, collider or covariate?</p>
<ul>
<li><p>gender may affect the outcome (as for a confounder and a covariate, but not as for a collider)</p></li>
<li><p>despite the randomisation restriction implied by gender, gender is still stochastically independent of treatment (knowing the gender, does not change the probability of being treated or not). Hence, (1) gender is not a confounder, and (2) the effect size for treatment is collapsible on gender.</p></li>
</ul>
<p>So these arguments seem to suggest that we are free to either include <em>gender</em> as a covariate in the model, or not. This is, however, not entirely true. Since gender imposes a randomisation restriction, gender must be included in the model, otherwise no valid inference for the effect size of treatment can be guaranteed. The focus is not not on bias, but on the validity of the statistical inference (standard error, confidence intervals and hypothesis testing).</p>
<p>We will not prove that for <em>stratified randomised study</em> the <em>stratification factor</em> (here: <em>gender</em>) must be included in the model. Rather we will illustrate it in a simulation study.</p>
<p>In the next simulation study, we will randomise 20 men over two treatment groups in a balanced way, and, similarly, we will also randomise 20 women over two treatment groups in a balanced way. For each repeated experiment, we will analyse the data with two models: one with only the treatment as a 0/1 dummy regressor, and one with treatment and gender as 0/1 dummies. The outcomes are simulated under the null hypothesis of no treatment effect, but gender does have an effect on the mean outcome. Since data are simulated under the null hypothesis of no treatment effect, we expect the p-values to be uniformly distributed and we expect the empirical type I error rate to be close to the nominal level of <span class="math inline">\(\alpha=5\%\)</span>. Over the repeated experiments, we keep track of the estimates of the treatment effect, the standard errors and the p-values for testing for no treatment effect.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1786</span>)

n.men&lt;-<span class="dv">20</span>
n.women&lt;-<span class="dv">20</span>
results1&lt;-<span class="kw">matrix</span>(<span class="dt">nrow=</span>N10000,<span class="dt">ncol=</span><span class="dv">3</span>)
results2&lt;-<span class="kw">matrix</span>(<span class="dt">nrow=</span>N10000,<span class="dt">ncol=</span><span class="dv">3</span>)

db&lt;-<span class="kw">data.frame</span>(<span class="dt">treatment=</span><span class="kw">rep</span>(<span class="ot">NA</span>,n.men<span class="op">+</span>n.women),<span class="dt">gender=</span><span class="ot">NA</span>,<span class="dt">outcome=</span><span class="ot">NA</span>)

<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N10000) {
  <span class="co"># stratified random treatment assignment</span>
  treatment.men&lt;-<span class="kw">sample</span>(<span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="kw">c</span>(n.men<span class="op">/</span><span class="dv">2</span>,n.men<span class="op">/</span><span class="dv">2</span>)), <span class="dt">replace =</span> <span class="ot">FALSE</span>)
  treatment.women&lt;-<span class="kw">sample</span>(<span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="kw">c</span>(n.women<span class="op">/</span><span class="dv">2</span>,n.women<span class="op">/</span><span class="dv">2</span>)), <span class="dt">replace =</span> <span class="ot">FALSE</span>)
 
  db<span class="op">$</span>treatment&lt;-<span class="kw">c</span>(treatment.men,treatment.women)
  db<span class="op">$</span>gender&lt;-<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>,n.men),<span class="kw">rep</span>(<span class="dv">1</span>,n.women))
  
  <span class="co"># simulation of outcomes</span>
  db<span class="op">$</span>outcome&lt;-<span class="kw">rnorm</span>(n.men<span class="op">+</span>n.women,<span class="dt">mean=</span><span class="dv">10</span>,<span class="dt">sd=</span><span class="dv">2</span>) <span class="co"># no treatment effect</span>
  db<span class="op">$</span>outcome[db<span class="op">$</span>gender<span class="op">==</span><span class="dv">0</span>]&lt;-db<span class="op">$</span>outcome[db<span class="op">$</span>gender<span class="op">==</span><span class="dv">0</span>]<span class="op">+</span><span class="dv">2</span> <span class="co"># gender effect</span>
  
  <span class="co"># analysis with the model with only treatment</span>
  m1&lt;-<span class="kw">lm</span>(outcome<span class="op">~</span>treatment, <span class="dt">data=</span>db)
  results1[i,]&lt;-<span class="kw">summary</span>(m1)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>)]
  
  <span class="co"># analysis with the model with treatment and gender </span>
  m2&lt;-<span class="kw">lm</span>(outcome<span class="op">~</span>treatment<span class="op">+</span>gender, <span class="dt">data=</span>db)
  results2[i,]&lt;-<span class="kw">summary</span>(m2)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>)]
}</code></pre></div>
<p>First we look at the results of the analysis of the model with the treatment and gender effects included.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># approximation of the expectation of the target effect size</span>
<span class="kw">mean</span>(results2[,<span class="dv">1</span>])</code></pre></div>
<pre><code>## [1] -0.01035173</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># approximation of the variance of the estimator of the target effect size</span>
<span class="kw">var</span>(results2[,<span class="dv">1</span>])</code></pre></div>
<pre><code>## [1] 0.3999589</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># approximation of the expectation of the estimators of the variance </span>
<span class="co">#    of the estimated target effect size</span>
<span class="kw">mean</span>(results2[,<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.400905</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># empirical type I eror rate for testing for no treatment effect size</span>
<span class="kw">mean</span>(results2[,<span class="dv">3</span>]<span class="op">&lt;</span><span class="fl">0.05</span>)</code></pre></div>
<pre><code>## [1] 0.0498</code></pre>
<p>These simulation results demonstrate:</p>
<ul>
<li><p>the estimator of the treatment effect is unbiased</p></li>
<li><p>the estimator of the variance of the treatment effect estimator is unbiased</p></li>
<li><p>the type I error rate is controlled at the nominal significance level</p></li>
</ul>
<p>Next we look at the results of the analysis of the model with only the treatment effect (thus ignoring the randomisation restriction imposed by <em>gender</em>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># approximation of the expectation of the target effect size</span>
<span class="kw">mean</span>(results1[,<span class="dv">1</span>])</code></pre></div>
<pre><code>## [1] -0.01035173</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># approximation of the variance of the estimator of the target effect size</span>
<span class="kw">var</span>(results1[,<span class="dv">1</span>])</code></pre></div>
<pre><code>## [1] 0.3999589</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># approximation of the expectation of the estimators of the variance </span>
<span class="co">#    of the estimated target effect size</span>
<span class="kw">mean</span>(results1[,<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.5067563</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># empirical type I eror rate for testing for no treatment effect size</span>
<span class="kw">mean</span>(results1[,<span class="dv">3</span>]<span class="op">&lt;</span><span class="fl">0.05</span>)</code></pre></div>
<pre><code>## [1] 0.0254</code></pre>
<p>These simulation results demonstrate:</p>
<ul>
<li><p>the estimator of the treatment effect is unbiased</p></li>
<li><p>the estimator of the variance of the treatment effect estimator is biased</p></li>
<li><p>the type I error rate is not controlled at the nominal significance level</p></li>
</ul>
<p>Hence, we conclude that</p>
<ul>
<li><p>the factor causing the randomisation restriction should be included in the model (otherwise no valid inference can be guaranteed). Note that this requirement stands even if the effect of the stratification factor is non-signifcant in the data analysis.</p></li>
<li><p>the treatment effect size estimate from the correct model (including <em>gender</em>) also has the marginal treatment effect size interpretation (thanks to the collapsibility)</p></li>
</ul>
</div>
<div id="sample-size-and-power" class="section level2">
<h2><span class="header-section-number">4.4</span> Sample size and power</h2>
<p>Consider the normal multiple linear regression model <a href="#eq:Mod5">(3.1)</a>, <span class="math display">\[
 Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{p-1} x_{ip-1} + \eps_ i
\]</span> with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>.</p>
<p>Without loss of generallity, suppose we are interested in the parameter <span class="math inline">\(\beta_1\)</span>. We will focus on the hypotheses <span class="math display">\[
  H_0: \beta_1 =0 \;\;\text{ versus }\;\; H_1: \beta_1\neq 0
\]</span> and we will be interested in the relationship between the sample size <span class="math inline">\(n\)</span> and the statistical power of the test. The power of a test is only defined for a very specific element of the alternative hypothesis, which is denoted by, for some <span class="math inline">\(\delta\neq 0\)</span>, <span class="math display">\[
  H_1(\delta): \beta_1 = \delta.
\]</span></p>
<p>If <span class="math inline">\(T\)</span> is the test statistic and if <span class="math inline">\(R_\alpha\)</span> is the rejection region of the test at the <span class="math inline">\(\alpha\)</span> level of significance, then the statistical power is defined as <span class="math display">\[
  \pi_\alpha(\delta) =\prob{T \in R_\alpha \mid \beta_1=\delta}.
\]</span> Note that <span class="math inline">\(\pi_\alpha(0)=\alpha\)</span> by construction of the rejection region.</p>
<p>If all model assumptions hold true, then the sampling distribution of the LSE <span class="math inline">\(\hat\beta_1\)</span> is given by (see Theorem <a href="#thm:DistrMod4">2.4</a>) <span class="math display">\[
   \hat\beta_1 \sim N(\beta_1, \sigma^2_{\beta_1})
\]</span> with <span class="math display">\[
  \sigma^2_{\beta_1} = \left[ (\mb{X}^t\mb{X})^{-1}\right]_{2,2} \sigma^2.
\]</span></p>
<p>This sampling distribution forms the basis of hypothesis testing. For simplicity we first assume that the residual variance <span class="math inline">\(\sigma^2\)</span> is known, then for testing <span class="math display">\[
  H_0: \beta_1 =0 \;\;\text{ versus }\;\; H_1: \beta_1\neq 0
\]</span> we have the following result for the test statistic <span class="math display">\[
  T = \frac{\hat\beta_1}{\sigma_{\beta_1}} \stackrel{H_0}{\sim} N(0,1).
\]</span></p>
<p>For studying the power of the test, we also need to know the distrbution of the test statistic under the alternative <span class="math inline">\(H_1(\delta): \beta_1=\delta\)</span>. Under the given assumptions, this is very easy. First we write the test statistic as <span class="math display">\[
  T=\frac{\hat\beta_1}{\sigma_{\beta_1}}=\frac{\hat\beta_1-\delta+\delta}{\sigma_{\beta_1}} = 
  \frac{\hat\beta_1-\delta}{\sigma_{\beta_1}}+\frac{\delta}{\sigma_{\beta_1}}.
\]</span> Under the alternative <span class="math inline">\(H_1(\delta)\)</span>, <span class="math display">\[
   \frac{\hat\beta_1-\delta}{\sigma_{\beta_1}} \sim N(0,1).
\]</span> Hence, under <span class="math inline">\(H_1(\delta)\)</span>, <span class="math display">\[
  T=\frac{\hat\beta_1-\delta}{\sigma_{\beta_1}}+\frac{\delta}{\sigma_{\beta_1}}
  \sim N\left(\frac{\delta}{\sigma_{\beta_1}}, 1\right) .
\]</span></p>
We will now use this expression to find the power for the two sided t-test. For the two-sided test, we can use <span class="math inline">\(\mid T \mid\)</span> as a test statistic, which has rejection region <span class="math display">\[
  R_\alpha=[z_{1-\alpha/2},+\infty[.
\]</span> The power of this two-sided test can thus be written as
<span class="math display">\[\begin{eqnarray*}
  \pi_\alpha(\delta) 
    &amp;=&amp; \prob{\mid T \mid &gt;z_{1-\alpha/2} \mid \beta_1=\delta} \\
    &amp;=&amp; \prob{T &gt;z_{1-\alpha/2} \mid \beta_1=\delta} + \prob{T &lt; -z_{1-\alpha/2} \mid \beta_1=\delta} \\
    &amp;=&amp; \prob{T -\frac{\delta}{\sigma_{\beta_1}}&gt;z_{1-\alpha/2} -\frac{\delta}{\sigma_{\beta_1}}\mid \beta_1=\delta} + \prob{T -\frac{\delta}{\sigma_{\beta_1}}&lt; -z_{1-\alpha/2}-\frac{\delta}{\sigma_{\beta_1}} \mid \beta_1=\delta} \\
    &amp;=&amp; 1-\Phi\left(z_{1-\alpha/2}-\frac{\delta}{\sigma_{\beta_1}}\right) +  \Phi\left(-z_{1-\alpha/2}-\frac{\delta}{\sigma_{\beta_1}}\right).
\end{eqnarray*}\]</span>
<p>In this context, <span class="math inline">\(\frac{\delta}{\sigma_{\beta_1}}\)</span> is referred to as the <strong>noncentrality parameter</strong> (ncp). Note that if the ncp equals zero, the power reduces to the nominal significance level <span class="math inline">\(\alpha\)</span>.</p>
<p>This expression for the power <span class="math inline">\(\pi_\alpha(\delta)\)</span> can be used for sample size calculation, or, more generally, for designing studies so as to guarantee a preset power, say <span class="math inline">\(\pi_0\)</span>.</p>
<p>A very simple example: consider the intercept-only model, <span class="math inline">\(\E{Y\mid x}=\beta_0\)</span> and suppose the target parameter is the population mean <span class="math inline">\(\beta_0\)</span> (thus replace <span class="math inline">\(\beta_1\)</span> in the previous paragraphs with <span class="math inline">\(\beta_0\)</span>). The design matrix <span class="math inline">\(\mb{X}\)</span> for this model is the <span class="math inline">\(n\times 1\)</span> matrix with all elements equal to 1. Hence, <span class="math inline">\(\sigma_{\beta_0}^2=\sigma^2/n\)</span> and the ncp becomes <span class="math inline">\(\sqrt{n}\frac{\delta}{\sigma}\)</span>.</p>
<p>More generally, the ncp (and hence the power) is determined by the full design matrix <span class="math inline">\(\mb{X}\)</span>.</p>
<p>In the previous sections we assumed that the residual variance <span class="math inline">\(\sigma^2\)</span> was known. In realistic settings, this is unknown and typically estimated by MSE. The appropriate test statistic and null distribution is then <span class="math display">\[
  T = \frac{\hat\beta_1}{\hat\sigma_{\beta_1}} \stackrel{H_0}{\sim} t_{n-p}.
\]</span> The rejection region for the two-sided test based on <span class="math inline">\(\mid T \mid\)</span> at the <span class="math inline">\(\alpha\)</span> level of significance is thus <span class="math inline">\([t_{n-p; 1-\alpha/2}, +\infty[\)</span>. It can be shown (here without proof) that under the alternative <span class="math inline">\(H_1(\delta)\)</span>, <span class="math display">\[
   T \sim t_{n-p,\nu},
\]</span> i.e. <span class="math inline">\(T\)</span> is distributed as a <strong>noncentral <span class="math inline">\(t\)</span>-distribution</strong> with <span class="math inline">\(n-p\)</span> degrees of freedom and noncentrality parameter <span class="math inline">\(\nu=\frac{\delta}{\sigma_{\beta_1}}\)</span>. The power is now given by <span class="math display">\[
  \pi_\alpha(\delta) = \prob{\mid T \mid &gt;t_{n-p;1-\alpha/2} \mid \beta_1=\delta} =
  1-F_t\left(t_{n-p;1-\alpha/2}; n-p,\nu\right) +  F_t\left(-t_{n-p;1-\alpha/2}; n-p,\nu\right).
\]</span> with <span class="math inline">\(F_t(\cdot;n-p,\nu)\)</span> the CDF of a noncentral <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-p\)</span> degrees of freedom and ncp <span class="math inline">\(\nu\)</span>.</p>
</div>
<div id="example-two-01-dummies" class="section level2 unnumbered">
<h2>Example (two 0/1 dummies)</h2>
<p>Suppose we need to design a study with two 0/1 dummy regressors. These two 0/1 dummies thus define <span class="math inline">\(2\times 2=4\)</span> groups. Suppose that one dummy codes for two treatment groups, and the other codes for two hospitals.</p>
<p>The study must be designed such that the power is <span class="math inline">\(\pi_0=80\%\)</span> for detecting a treatment effect of <span class="math inline">\(\beta_1=2\)</span> with a two-sided test at the <span class="math inline">\(\alpha=5\%\)</span> level of significance. This effect size of <span class="math inline">\(\beta_1=2\)</span> is considered to be the smallest clinically relevant effect size. If the true effect size is equal to 2 or larger, the study design should guarantee that it will be detected with a probability of <span class="math inline">\(80\%\)</span>.</p>
<p>From previous studies on the same outcome variable, we know that the variance of the outcome is typically not larger than <span class="math inline">\(\sigma^2=3\)</span>.</p>
<p>We don't know whether the hospital will have an effect on the mean outcome. The final data analysis will include the 0/1 dummy for hospital as a regressor.</p>
<p>We will solve the following problems: restrict the attention to balanced designs (i.e. equal number of observations in each of the 4 groups): what sample size is needed? Does the effect of the hospital have an effect on the power/sample size?</p>
<p>First we write a function to construct the design matrix for a given sample size <span class="math inline">\(n\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ConstructX&lt;-<span class="cf">function</span>(n) {
  <span class="co"># n: total sample size. It must be a multiple of 4</span>
  <span class="co">#</span>
  <span class="co"># This function constructs the design matrix for a given sample size and for </span>
  <span class="co">#   a balanced design, for a model with main effects of two 0/1 dummies</span>
  
  m&lt;-n<span class="op">/</span><span class="dv">4</span>
  X&lt;-<span class="kw">matrix</span>(<span class="dt">nrow=</span>n,<span class="dt">ncol=</span><span class="dv">3</span>)
  X[,<span class="dv">1</span>]&lt;-<span class="dv">1</span>
  X[,<span class="dv">2</span>]&lt;-<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">2</span><span class="op">*</span>m),<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">2</span><span class="op">*</span>m))
  X[,<span class="dv">3</span>]&lt;-<span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>),<span class="kw">rep</span>(m,<span class="dv">4</span>))
  
  <span class="kw">return</span>(X)
}</code></pre></div>
<p>Next we write a function for the power.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">powerX&lt;-<span class="cf">function</span>(n,delta,sigma2,<span class="dt">alpha=</span><span class="fl">0.05</span>) {
  <span class="co"># n: total sample size. It must be a multiple of 4</span>
  <span class="co"># delta: effect size</span>
  <span class="co"># sigma2: varance of the error term</span>
  <span class="co"># alpha: significance level</span>
  <span class="co">#</span>
  <span class="co"># this functions computes the power for detecting the effect size at the alpha level </span>
  <span class="co">#   for a balanced design</span>
  
  X&lt;-<span class="kw">ConstructX</span>(n)
  sigma2Beta&lt;-<span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>X)[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">*</span>sigma2
  ncp&lt;-delta<span class="op">/</span><span class="kw">sqrt</span>(sigma2Beta)
  tcrit&lt;-<span class="kw">qt</span>(<span class="dv">1</span><span class="op">-</span>alpha<span class="op">/</span><span class="dv">2</span>,<span class="dt">df=</span>n<span class="op">-</span><span class="dv">3</span>)
  
  pwr&lt;-<span class="dv">1</span><span class="op">-</span><span class="kw">pt</span>(tcrit,<span class="dt">df=</span>n<span class="op">-</span><span class="dv">3</span>,<span class="dt">ncp =</span> ncp)<span class="op">+</span><span class="kw">pt</span>(<span class="op">-</span>tcrit,<span class="dt">df=</span>n<span class="op">-</span><span class="dv">3</span>,<span class="dt">ncp =</span> ncp)
  
  <span class="kw">return</span>(pwr)
}

<span class="co"># check: power for delta=0 (which agrees with H0) should be alpha</span>
<span class="kw">powerX</span>(<span class="dv">20</span>,<span class="dt">delta=</span><span class="dv">0</span>,<span class="dt">sigma2=</span><span class="dv">3</span>)</code></pre></div>
<pre><code>## [1] 0.05</code></pre>
<p>Next we apply the power function for a sequence of sample sizes and make a plot of power versus sample size.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nseq&lt;-<span class="kw">seq</span>(<span class="dv">4</span>,<span class="dv">40</span>,<span class="dt">by=</span><span class="dv">4</span>)
pwr&lt;-<span class="kw">c</span>()

<span class="cf">for</span>(n <span class="cf">in</span> nseq) {
  pwr&lt;-<span class="kw">c</span>(pwr,<span class="kw">powerX</span>(n,<span class="dt">delta=</span><span class="dv">2</span>,<span class="dt">sigma2=</span><span class="dv">3</span>))
}

<span class="kw">plot</span>(nseq,pwr,
     <span class="dt">xlab=</span><span class="st">&quot;sample size&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;power&quot;</span>,
     <span class="dt">cex.lab=</span><span class="fl">1.5</span>, <span class="dt">cex.axis=</span><span class="fl">1.5</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="fl">0.8</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="dv">4</span>)</code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-150-1.png" width="672" /></p>
<p>From this graph we read that we need a sample size of <span class="math inline">\(n=28\)</span> to guarantee a power of <span class="math inline">\(80\%\)</span>.</p>

</div>
</div>
<div id="reporting" class="section level1">
<h1><span class="header-section-number">Hoofdstuk 5</span> Reporting</h1>
<p>Here we provide some general guidelines for reporting the statistical results of a linear regression analysis. Many of these guidelines are more generally applicable to reporting other types of statistical analysis.</p>
<ul>
<li><p>Always report the study design. This is extremely important to determine what statistical methods can be applied in a valid way, and what kind of conclusions are allowed (e.g. causal conclusion or not).</p></li>
<li><p>Always report the research question.</p></li>
<li><p>It is strongly recommended to always start with a data exploration. If strange or unexpected observations are detected, then identify the observations are report them. It is not up to the statistician to decide to remove observations from the analysis! In no case the dataset must be adapted to the statistical method (by e.g. removing outliers), but in any case the statistical method must be chosen so that the method is valid and can produce an answer to the research question.</p></li>
<li><p>Always report results in their correct units. E.g. in the blood pressure example we have estimated <span class="math inline">\(\beta_1\)</span> as <span class="math inline">\(1.79\)</span>. Thus we say write <span class="math inline">\(\hat\beta_1=1.79\)</span> mmHg / mg/day, or in words: we estimate that the blood pressure reduces on average with <span class="math inline">\(1.79\)</span> mmHg per increase of of the daily dose with 1 mg.</p></li>
<li><p>Always report estimates with confidence intervals. If the confidence interval is centered at the estimate <span class="math inline">\(\hat\beta_1=1.79\)</span> and stretches <span class="math inline">\(0.35\)</span> to either side, then the best way to report to the confidence interval is <span class="math display">\[
  (1.44 \text{ to } 2.14) \text{ mmHg / mg/day}
\]</span> Some bad examples: <span class="math inline">\((1.44 - 2.14)\)</span> mmHg / mg/day, or (1.44, 2.14) mmHg/ mg/day, or <span class="math inline">\(1.79 \pm 0.35\)</span> mmHg / mg/day.</p></li>
<li><p>When reporting confidence intervals, the nominal coverage should also be mentioned (e.g. <span class="math inline">\(95\%\)</span> or <span class="math inline">\(90\%\)</span> confidence interval).</p></li>
<li><p>Standard errors of estimates may be reported as e.g. <span class="math inline">\(1.79\)</span> (SE=<span class="math inline">\(0.17\)</span>) mmHg / mg/day, or <span class="math inline">\(1.79\)</span> mmHg / mg/day (SE=<span class="math inline">\(0.17\)</span> mmHg / mg/day).</p></li>
<li><p>When reporting the results of a hypothesis test, always mention the null and alternative hypothesis, as well as the significance level and the <span class="math inline">\(p\)</span>-value. A hypothesis test is seldom to be performed without also estimating a population parameter (e.g. an effect size or a regression coefficient). The estimate (with confidence interval) should also be reported to complement the interpretation of the hypothesis test.</p></li>
<li><p>Make sure that you have formulated an answer to the original research question. If this is not possible (e.g. inconclusive results) then also report this.</p></li>
<li><p>The report must be written in neat English. The formulation of a conclusion from a statistical analysis must be very precise and with appropriate nuance.</p></li>
<li><p>Never copy-and-paste software output to the report, but instead extract the relevant information from the output and place it in the report in either the text or in a nicely formatted table. You may list the software output in the appendix. Exception: when the report is meant to be read by other statisticians or data scientists, you may show them the software output (as for example in these course notes).</p></li>
</ul>

</div>



<div id="app:VecDiff" class="section level1">
<h1><span class="header-section-number">A</span> Vector Differentiation</h1>
<p>For <span class="math inline">\(\mb\beta^t = (\beta_0, \ldots, \beta_{p-1})\)</span>,</p>
<span class="math display">\[\begin{eqnarray*}
  \frac{d}{d\beta} = \left(\begin{array}{c}
    \frac{\partial}{\partial \beta_0}\\
    \vdots\\
    \frac{\partial}{\partial \beta_{p-1}}
    \end{array}\right).
\end{eqnarray*}\]</span>
<p>Upon using this notation, the following differentiation rules hold for <span class="math inline">\(\mb{a}\)</span> (a <span class="math inline">\((p\times 1)\)</span> vector) and <span class="math inline">\(\mb{A}\)</span> (a symmetric <span class="math inline">\((p\times p)\)</span> matrix):</p>
<ul>
<li><p><span class="math inline">\(\frac{d(\mb\beta^t \mb{a})}{d\mb\beta} = \frac{d(\mb{a}^t\mb\beta)}{d\mb\beta} = \mb{a}\)</span></p></li>
<li><p><span class="math inline">\(\frac{d(\mb\beta^t\mb{A\beta})}{d\mb\beta} = 2\mb{A\beta}\)</span>.</p></li>
</ul>
</div>
<div id="app:LinTrans" class="section level1">
<h1><span class="header-section-number">B</span> Linear Transformations of MVN</h1>

<div class="lemma">
<span id="lem:LinTransNorm" class="lemma"><strong>Lemma B.1  (Linear transformation of MVN)  </strong></span>If <span class="math inline">\(\mb{Z}\sim \text{MVN}(\mb\mu,\mb\Sigma)\)</span> (<span class="math inline">\(n\)</span>-dimensional stochastic vector), and if <span class="math inline">\(\mb{A}\)</span> is a constant <span class="math inline">\(p\times n\)</span> matrix <span class="math inline">\(\mb{A}\)</span> of rank <span class="math inline">\(p\)</span> and <span class="math inline">\(\mb{a}\)</span> a constant <span class="math inline">\(p\times 1\)</span> vector <span class="math inline">\(\mb{a}\)</span>, then it holds that <span class="math display">\[ 
  \mb{AZ}+\mb{a} \sim \text{MVN}(\mb{A\mu}+\mb{a},\mb{A\Sigma}\mb{A}^t).
\]</span>
</div>

</div>
<div id="app:Slutsky" class="section level1">
<h1><span class="header-section-number">C</span> Slutsky's Theorem</h1>

<div class="theorem">
<span id="thm:Slutsky" class="theorem"><strong>Theorem C.1  (Slutsky's Theorem)  </strong></span>Consider two sequences of stochastic variables, <span class="math inline">\(X_1,\ldots, X_n\)</span> and <span class="math inline">\(Y_1,\ldots, Y_n\)</span>. Assume that, as <span class="math inline">\(n\rightarrow \infty\)</span>, <span class="math display">\[
   X_n \convDistr X \;\;\;\text{ and }\;\;\; Y_n \convProb c
 \]</span> with <span class="math inline">\(X\)</span> a stochastic variable and <span class="math inline">\(c\)</span> a constant. Then, it holds that
<span class="math display">\[\begin{eqnarray*}
  X_n + Y_n &amp;\convDistr&amp; X+c \\
  X_n Y_n &amp;\convDistr&amp; cX \\
  X_n / Y_n &amp;\convDistr&amp; X/c \;\;\;\text{(if }1/c\text{ exists)}.
 \end{eqnarray*}\]</span>
</div>

</div>
<div id="types-of-statistical-models" class="section level1">
<h1><span class="header-section-number">D</span> Types of Statistical Models</h1>
<p>We introduce the concept of a statistical model in the setting of a single outcome <span class="math inline">\(Y\)</span> and a single regressor <span class="math inline">\(x\)</span>.</p>
<p>Given the regressor <span class="math inline">\(x\)</span>, the outcome can be described by the conditional distribution. We use the notation <span class="math display">\[
  Y \mid x
\]</span> to refer to the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x\)</span>. The corresponding distribution function is denoted by <span class="math inline">\(F(y\mid x)\)</span> and the density function <span class="math inline">\(f(y\mid x)\)</span>.</p>
<p>Without any further restrictions on this conditional distribution, we say that it is a {}. However, often we imply further restrictions on the conditional distribution. Let us start with a simple example. Suppose that <span class="math inline">\(x\)</span> is a 0/1 indicator (0: placebo and 1: active treatment). Then we can write <span class="math inline">\(Y \mid x=0 \sim F(y\mid x=0)\)</span> and <span class="math inline">\(Y \mid x=1 \sim F(y\mid x=1)\)</span>. But again, without any further restrictions, this is a nonparametric model, because it only says that for <span class="math inline">\(x=0\)</span> and <span class="math inline">\(x=1\)</span> the distribution of <span class="math inline">\(Y\)</span> is given by two distinct distribution function, but there is no relationship between the distributions assumed. Even in this nonparametric model, we can identify parameters. For example, <span class="math display">\[
  \mu_0 = E(Y \mid x=0) = \int y f(y\mid x=0) dy  
\]</span> and <span class="math display">\[
  \mu_1 = E(Y \mid x=1) = \int y f(y\mid x=1) dy.
\]</span> So we have two meaningful parameters, but they do not impose any restrictions on <span class="math inline">\(f(y\mid x=0)\)</span> and <span class="math inline">\(f(y\mid x=1)\)</span>. These nonparametric models can be represented by the set of all proper conditional distribution functions. For the example with <span class="math inline">\(x=0/1\)</span> this can be written as <span class="math display">\[
  \left\{ F(\cdot \mid x): F(\cdot \mid x) \text{ is a distribution function}, x\in \{0,1\} \right\}.
\]</span></p>
<p>The model can be turned into a {}, by imposing strong distributional assumptions on <span class="math inline">\(f(y\mid x=0)\)</span> and <span class="math inline">\(f(y\mid x=1)\)</span>. For example, (<span class="math inline">\(x=0,1\)</span>) <span class="math display">\[
  Y \mid x \sim N(\mu_x, \sigma^2).
\]</span> This model says that when <span class="math inline">\(x=0\)</span>, the outcome has a normal distribution with mean <span class="math inline">\(\mu_0\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, and when <span class="math inline">\(x=1\)</span>,the outcome has a normal distribution with mean <span class="math inline">\(\mu_1\)</span> and also variance <span class="math inline">\(\sigma^2\)</span>. Thus the parameters <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\mu_1\)</span> have the same interpretation as in the nonparametric model, but now the distributions are explicitly specified and there is the additional restriction that the variances in the two <span class="math inline">\(x=0/1\)</span> groups are identical.</p>
<p>Another example of a parametric model is the simple linear regression model. Now <span class="math inline">\(x\)</span> is a continuous regressor. We write the models as <span class="math display">\[
   Y \mid x \sim N(\beta_0 +\beta_1 x, \sigma^2).
\]</span> Thus, for each <span class="math inline">\(x\)</span> the model completely specifies the distribution of the outcome. The model specification also included a description of how the mean of the outcome varies with <span class="math inline">\(x\)</span>.</p>
<p>Let us write this parametric model in a more generic way. Let <span class="math inline">\(\theta\)</span> denote the parameter vector with <span class="math inline">\(p\)</span> elements and let <span class="math inline">\(F_\theta(y \mid)\)</span> denote the distribution function of the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x\)</span>. This distribution is thus completely known, up to some parameter <span class="math inline">\(\theta\)</span> that has to be estimated from the data. If <span class="math inline">\(\theta\)</span> contains real-values parameter, we have <span class="math inline">\(\theta \in \mathbb{R}^p\)</span>. The statistical model can then be represented by the following set of conditional distribution functions, <span class="math display">\[
  \left\{ F(\cdot\mid x) :  F(\cdot\mid x)=F_\theta(\cdot\mid x), \theta\in\mathbb{R}^p, x\in \mathbb{R} \right\}.
\]</span></p>
<p>We can also define {} models. These are models that do not complete specify the conditional distribution, but still they imply restrictions. For example, we could impose the following linear model for the conditional mean, <span class="math display">\[
  E(Y \mid x) = \beta_0 + \beta_1 x
\]</span> without any further restrictions on the conditional distribution. This is not a nonparametric model, because there is a restriction on some aspects of the distribution (here: the conditional mean). It is neither a parametric model, because up to the parameter <span class="math inline">\(\theta^t=(\beta_0,\beta_1)\)</span>, the distribution is not completely specified. This is an example of a {}. It can be represented by the following set of conditional distribution functions, <span class="math display">\[
  \left\{ F(\cdot\mid x) :  E_F(Y\mid x)=\beta_0 + \beta_1 x, (\beta_0,\beta_1) \in\mathbb{R}^2, x\in \mathbb{R} \right\},
\]</span> in which the subscript <span class="math inline">\(F\)</span> in <span class="math inline">\(E_F(Y\mid x)\)</span> is used to stress that the expectation is defined w.r.t. the distribution function <span class="math inline">\(F\)</span>.</p>
<p>Semiparametric models are not limited to include restrictions on the conditional mean. It could for example also involve restrictions on the variance, <span class="math display">\[
  \left\{ F(\cdot\mid x) :  E_F(Y\mid x)=\beta_0 + \beta_1 x,  \text{ and } \text{Var}(Y\mid x)=\sigma^2, (\beta_0,\beta_1) \in\mathbb{R}^2, \sigma^2 \in \mathbb{R}^+, x\in \mathbb{R} \right\}.
\]</span></p>
<p>So far we have always looked at conditional distribution functions, i.e. <span class="math inline">\(x\)</span> is considered a fixed constant. Sometimes <span class="math inline">\(x\)</span> can be random as well. For example, reconsider the example with <span class="math inline">\(x\)</span> a binary 0/1 indicator for two treatments. In a randomised clinical trial, the treatment allocation happens completely at random. Thus <span class="math inline">\(x\)</span> is a random variable; it is thus better to write it as the capital letter <span class="math inline">\(X\)</span>. In this example, <span class="math display">\[
  X \sim \text{Binom}(1,1/2).
\]</span> This marginal distribution for <span class="math inline">\(X\)</span>, together with the model for the conditional distribution of <span class="math inline">\(Y\mid X\)</span>, specifies the joint statistical model of <span class="math inline">\((Y,X)\)</span>.</p>
</div>
<div id="r-session-info" class="section level1">
<h1><span class="header-section-number">E</span> R Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sessionInfo</span>()</code></pre></div>
<pre><code>## R version 3.6.3 (2020-02-29)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Catalina 10.15.7
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] car_3.0-10     carData_3.0-4  dagitty_0.3-0  GGally_2.0.0   plotly_4.9.2.1
##  [6] ggplot2_3.3.0  skimr_2.1.2    tidyr_1.0.2    dplyr_1.0.2    MASS_7.3-51.5 
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.4         assertthat_0.2.1   digest_0.6.25      utf8_1.1.4        
##  [5] V8_3.4.0           R6_2.4.1           cellranger_1.1.0   plyr_1.8.6        
##  [9] repr_1.1.0         evaluate_0.14      httr_1.4.1         highr_0.8         
## [13] pillar_1.4.3       rlang_0.4.7        lazyeval_0.2.2     curl_4.3          
## [17] readxl_1.3.1       rstudioapi_0.11    data.table_1.12.8  rmarkdown_2.3     
## [21] labeling_0.3       stringr_1.4.0      foreign_0.8-76     htmlwidgets_1.5.1 
## [25] munsell_0.5.0      compiler_3.6.3     xfun_0.13          pkgconfig_2.0.3   
## [29] base64enc_0.1-3    htmltools_0.4.0    tidyselect_1.1.0   tibble_3.0.0      
## [33] bookdown_0.20      rio_0.5.16         reshape_0.8.8      fansi_0.4.1       
## [37] viridisLite_0.3.0  crayon_1.3.4       withr_2.1.2        grid_3.6.3        
## [41] jsonlite_1.6.1     gtable_0.3.0       lifecycle_0.2.0    magrittr_1.5      
## [45] scales_1.1.0       zip_2.1.1          cli_2.0.2          stringi_1.4.6     
## [49] farver_2.0.3       ellipsis_0.3.0     generics_0.0.2     vctrs_0.3.4       
## [53] boot_1.3-24        openxlsx_4.2.3     RColorBrewer_1.1-2 tools_3.6.3       
## [57] forcats_0.5.0      glue_1.4.0         purrr_0.3.3        crosstalk_1.1.0.1 
## [61] hms_0.5.3          abind_1.4-5        yaml_2.2.1         colorspace_1.4-1  
## [65] knitr_1.28         haven_2.2.0</code></pre>

</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DASM2.pdf"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
